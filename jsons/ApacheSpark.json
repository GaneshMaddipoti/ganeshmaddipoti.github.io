{"id":null,"name":"ApacheSpark","data":{"id":"root","size":{"width":1728,"height":997},"pan":{"x":-695.4422670492002,"y":731.901051758586,"scale":3.303354313758801},"children":[{"id":"line-1768776347399","type":"Line","lineCoordinates":[{"x":1033.8216552734375,"y":1239.6767578125},{"x":1033.8216552734375,"y":1239.6767578125}]},{"id":"line-1768776355611","type":"Line","lineCoordinates":[{"x":1124.3660888671875,"y":1114.84912109375},{"x":1124.3660888671875,"y":1114.84912109375}]},{"id":"node-1768994213987","position":{"x":-834.3212826251984,"y":22.18943977355957},"size":{"width":2292.2955932617188,"height":1706.0202026367188},"type":"Container","title":"Apache Spark","description":"History:\nThe need for large-scale computing was initially driven by Google, whose proprietary systems like the \n1) Google File System (GFS), (distributed)\n2) MapReduce (MR), (parallel programming pardigm based on functional programming)\n3) Bigtable (structured data across GFS)\nwere designed to handle data at a scale traditional relational databases could not. \nThese ideas provided a blueprint for Apache Hadoop, including the Hadoop Distributed File System (HDFS)\nMost of the work Google did was proprietary, but the ideas expressed in the afore‐mentioned three papers spurred innovative ideas elsewhere in the open source community—especially at Yahoo!, which was dealing with similar big data challenges of scale for its search engine.\nHowever, Hadoop MapReduce had several shortcomings:\n• It was operationally complex and difficult to administer.\n• It was inefficient for iterative or interactive jobs because it required writing intermediate results to local disk between every stage, resulting in heavy I/O overhead.\n• It was not conducive to combining different workloads like machine learning, streaming, or interactive SQL.\nTo address these issues, researchers at UC Berkeley started the Spark project in 2009. Spark was designed to be highly fault-tolerant and parallel, but it enhanced the system by supporting in-memory storage for intermediate results. By 2013, the project was donated to the Apache Software Foundation, and its creators formed Databricks.\n\n\nWhat is :\nApache Spark is a unified analytics engine for large-scale data processing. (Cluster computing platform)\nAt its core, Spark is a “computational engine” that is responsible for scheduling, distributing, and monitoring applications consisting of many computational tasks across many worker machines, or a computing cluster. \nIf Hadoop MapReduce was the \"first generation\" of big data processing, Spark is the \"second generation\"—built for speed, ease of use, and versatility.\n\nFeatures\nEasy to use: we can develop applications, using a high-level API that let us focus on the content of our computation\nSpeed: It is up to 100x faster than Hadoop MapReduce because it processes data in-memory (RAM) rather than writing to the disk after every step.\nModularity: Spark SQL, Spark Structured Streaming, Spark MLlib, and GraphX, combining all the workloads running under one engine. \nExtensibility: Spark decouples storage and compute, and using connectors we can connect to multiple sources. \nLazy Evaluation: Spark doesn't execute your code immediately. It builds a plan (called a DAG or Directed Acyclic Graph) and only runs it when you finally ask for a result. This allows it to optimize the entire process first.\nAs general engine: letting you combine multiple types of computations (e.g., SQL queries, text process‐ing, and machine learning) that might previously have required different engines.\nPolyglot: You can write Spark code in Python (PySpark), Scala, Java, or R.\n\n\nThe Unified Stack\nIn 2016, Spark was recognized for replacing disparate processing engines with a unified stack of components. This stack includes:\n• Spark SQL: For working with structured data, supporting ANSI SQL:2003-compliant queries.\n• Spark MLlib: A library of common machine learning algorithms built on high-level DataFrame APIs.\n• Spark Structured Streaming: A model that treats real-time data streams as continuously appended tables.\n• GraphX: A library for manipulating graphs and performing graph-parallel computations.\n\nDistributed Execution Architecture\nSpark operates as a distributed engine where components work collaboratively on a cluster of machines.\n• Spark Driver: Responsible for instantiating a SparkSession, orchestrating operations, and transforming code into DAG computations to be scheduled as tasks.\n• SparkSession: Introduced in Spark 2.0, it serves as a single unified entry point for all Spark functionality.\n• Cluster Manager: Manages and allocates resources for the cluster. Spark supports Standalone, YARN, Mesos, and Kubernetes managers.\n• Executors: Run on worker nodes, communicate with the driver, and are responsible for executing tasks.\n• Partitions: Spark treats data distributed across a cluster as a logical data abstraction in memory. Partitioning allows for efficient parallelism, where each executor's core is assigned a single partition of data to work on.\n\nThe Developer Experience\nSpark is favored by data engineers, data scientists, and machine learning engineers because it allows them to build complex applications using a single engine. \nData scientists use it to cleanse data and build model pipelines, while data engineers use it to build scalable data pipelines and perform ETL tasks. \nCommon use cases include parallel processing of large datasets, ad hoc interactive queries, and analyzing social network graphs\n\n\n\n\n\n","pan":{"x":-4859.316721799682,"y":-2888.8025078881137,"scale":0.17222228012774893},"children":[{"id":"node-1768994224387-937691","position":{"x":-669.03759765625,"y":-2438.9286346435547},"size":{"width":4260.523254394531,"height":2464.3912506103516},"type":"Container","title":"Deployment","color":"transparent","pan":{"x":-1708.1477114999152,"y":-86.73973160524451,"scale":1.0947671940733648},"children":[{"id":"node-1768994224388-298441","position":{"x":119.09141540527344,"y":773.4568481445312},"size":{"width":384.52947998046875,"height":288.747314453125},"type":"Container","title":"Client","description":"\n","children":[{"id":"node-1769514065885","position":{"x":12.449310302734375,"y":15.751785278320312},"size":{"width":738.5188598632812,"height":523.5322875976562},"type":"Container","title":"Spark application","children":[{"id":"node-1769514073966-720199","position":{"x":32.9354248046875,"y":26.578487396240234},"size":{"width":1389.4005126953125,"height":941.2673950195312},"type":"Container","title":"Spark driver","description":"Driver Program: The \"brain\" of your application. It runs your main() function and converts your code into tasks.\nAt a high level, every Spark application consists of a driver program that launches various parallel operations on a cluster. \nThe driver program contains your application’s main function and defines distributed datasets on the cluster, then applies operations to them.\nDriver programs access Spark through a SparkContext object, which represents a connection to a computing cluster.\n\n\nResponsibilities\n1) Maintains the SparkContext/SparkSession: It is the entry point of the application.\n2) Analyzes Code: it converts your high-level code (SQL, Python, Scala) into a logical plan.\n3) Creates the DAG: It builds a Directed Acyclic Graph (DAG) of all the transformations you want to perform.\n4) Schedules Tasks: It breaks the DAG into \"Stages\" and then into \"Tasks,\" which it sends to the Executors (worker nodes) to be processed.\n","children":[{"id":"node-1769514073967-321965","position":{"x":107.41201782226562,"y":85.80938720703125},"size":{"width":2528.6052856445312,"height":1542.2193908691406},"type":"Container","title":"SparkSession","description":"The Script Starts: The SparkSession is instantiated.\nThe Negotiation: The session's internal SparkContext reaches out to the Cluster Manager (like YARN).\nThe AM Launch: YARN allocates a container for the ApplicationMaster.\nThe Command Center: The SparkSession (the Driver) then uses that AM to start Executors across the rest of the cluster.","children":[{"id":"node-1769514073967-964486","position":{"x":51.0009765625,"y":88.201904296875},"size":{"width":4832.708160400391,"height":2648.916748046875},"type":"CodeBox","content":"#Application \nfrom pyspark.sql import SparkSession\n\n# 1. The Driver starts and creates the SparkSession\nspark = SparkSession.builder.appName(\"LogAnalysis\").get_home()\n# 2. The Driver creates a plan to read the data (Lazy Evaluation)\nlogs = spark.read.text(\"s3://my-bucket/logs/*.txt\")\n# 3. The Driver plans the transformation (Filtering)\naws_logs = logs.filter(logs.value.contains(\"AWS\"))\n# 4. The Driver triggers an 'Action' (.count())\n# This is where the Driver converts the DAG into actual tasks\nresult = aws_logs.count()\n# 5. The Driver receives the final number from the executors and prints it\nprint(f\"Total AWS entries: {result}\")\n\nspark.stop()\n\n\n\n\n\n\n"}]}]}]}]},{"id":"node-1768994224388-583630","position":{"x":958.575439453125,"y":771.3358764648438},"size":{"width":450,"height":300},"type":"Container","title":"Cluster Manager","description":"The cluster manager is responsible for managing and allocating resources for the\ncluster of nodes on which your Spark application runs. ","pan":{"x":-559.715758741801,"y":-411.0674357336627,"scale":0.21095045626050013},"children":[{"id":"node-1768994224388-895464","position":{"x":15.268295288085938,"y":-87.52663993835449},"size":{"width":450,"height":300},"type":"Container","title":"Apache YARN"},{"id":"node-1768994224388-474429","position":{"x":-467.78299820423126,"y":-88.37072658538818},"size":{"width":450,"height":300},"type":"Container","title":"Standalone"},{"id":"node-1768994224388-830986","position":{"x":965.3104095458984,"y":-98.9773280620575},"size":{"width":450,"height":300},"type":"Container","title":"Mesos"},{"id":"node-1768994224388-433762","position":{"x":493.5273609161377,"y":-91.90109252929688},"size":{"width":450,"height":300},"type":"Container","title":"Kubernetes"}]},{"id":"node-1768994224388-889554","position":{"x":1420.81787109375,"y":269.3124084472656},"size":{"width":450,"height":300},"type":"Container","title":"Worker Node","children":[{"id":"node-1768994224388-5378","position":{"x":89.96749877929688,"y":62.73999786376953},"size":{"width":718.8135986328125,"height":423.2466735839844},"type":"Container","title":"Application Master","children":[{"id":"node-1768994224388-993284","position":{"x":145.12289428710938,"y":88.24881744384766},"size":{"width":1090.2388916015625,"height":657.4938354492188},"type":"Container","title":"Spark Driver","description":"Driver Program: The \"brain\" of your application. It runs your main() function and converts your code into tasks.\nAt a high level, every Spark application consists of a driver program that launches various parallel operations on a cluster. \nThe driver program contains your application’s main function and defines distributed datasets on the cluster, then applies operations to them.\nDriver programs access Spark through a SparkContext object, which represents a connection to a computing cluster.\n\n\nResponsibilities\n1) Maintains the SparkContext/SparkSession: It is the entry point of the application.\n2) Analyzes Code: it converts your high-level code (SQL, Python, Scala) into a logical plan.\n3) Creates the DAG: It builds a Directed Acyclic Graph (DAG) of all the transformations you want to perform.\n4) Schedules Tasks: It breaks the DAG into \"Stages\" and then into \"Tasks,\" which it sends to the Executors (worker nodes) to be processed.\n","children":[{"id":"node-1768994224388-815872","position":{"x":153.72389221191406,"y":68.72859191894531},"size":{"width":1840.5922546386719,"height":1064.607666015625},"type":"CodeBox","content":"#Application \nfrom pyspark.sql import SparkSession\n\n# 1. The Driver starts and creates the SparkSession\nspark = SparkSession.builder.appName(\"LogAnalysis\").get_home()\n# 2. The Driver creates a plan to read the data (Lazy Evaluation)\nlogs = spark.read.text(\"s3://my-bucket/logs/*.txt\")\n# 3. The Driver plans the transformation (Filtering)\naws_logs = logs.filter(logs.value.contains(\"AWS\"))\n# 4. The Driver triggers an 'Action' (.count())\n# This is where the Driver converts the DAG into actual tasks\nresult = aws_logs.count()\n# 5. The Driver receives the final number from the executors and prints it\nprint(f\"Total AWS entries: {result}\")\n\nspark.stop()\n\n\n\n\n\n"}]}]}]},{"id":"node-1768994224388-836652","position":{"x":1909.6837158203125,"y":657.9249267578125},"size":{"width":450,"height":300},"type":"Container","title":"Worker Node","children":[{"id":"node-1768994224388-201049","position":{"x":229.38766479492188,"y":158.36561584472656},"size":{"width":450,"height":300},"type":"Container","title":"Executor","children":[{"id":"node-1768994224388-518941","position":{"x":181.1865997314453,"y":118.54624938964844},"size":{"width":450,"height":300},"type":"Container","title":"JVM"}]}]},{"id":"node-1768994224388-291383","position":{"x":1871.712890625,"y":1227.8822021484375},"size":{"width":450,"height":300},"type":"Container","title":"Worker Node"},{"id":"node-1768994224388-33816","type":"Path","startElement":"node-1768994224388-298441","endElement":"node-1768994224388-583630"},{"id":"node-1768994224388-898071","type":"Path","startElement":"node-1768994224388-583630","endElement":"node-1768994224388-836652"},{"id":"node-1768994224388-592941","type":"Path","startElement":"node-1768994224388-583630","endElement":"node-1768994224388-291383"},{"id":"node-1768994224388-957355","type":"Path","startElement":"node-1768994224388-583630","endElement":"node-1768994224388-889554"},{"id":"node-1768994224388-720171","position":{"x":66.03778076171875,"y":6.603759765625},"type":"Line","lineCoordinates":[{"x":690.7655029296875,"y":292.893798828125},{"x":703.627197265625,"y":1635.557373046875}]},{"id":"node-1768994224388-939734","position":{"x":369.8416290283203,"y":291.53314208984375},"size":{"width":364.4654235839844,"height":100},"type":"TitleBox","content":"Client Mode"},{"id":"node-1768994224388-377469","position":{"x":780.3194580078125,"y":287.870849609375},"size":{"width":360.314697265625,"height":95.837158203125},"type":"TitleBox","content":"Cluster Mode"},{"id":"node-1768994224388-656731","position":{"x":-1867.9034881591797,"y":-18.300048828125},"size":{"width":1662.3623657226562,"height":1951.9658203125},"type":"NoteBox","content":"Modes:\nIn enterprise setups, we don't submit jobs from our own laptop. Instead, we log into a Gateway (Edge) Node.\nLocal Mode:\nThe entire Spark application (Driver and Executors) runs inside a single JVM on your local machine.\nClient Mode:\nThe driver will run in client machine(edge node), slim AM will run in cluster\nCluster:\nThe Cluster Manager starts the AM first, and then the AM launches the Spark Driver inside its own process"}]},{"id":"node-1769451521963-436559","position":{"x":-3522.133085851331,"y":5031.84905196486},"size":{"width":1431.349365234375,"height":1059.3499145507812},"type":"Container","title":"Input Source","pan":{"x":-204.43327712038308,"y":-579.5589389224399,"scale":0.6346709922521182},"children":[{"id":"node-1769451521963-220966","position":{"x":414.8279724121094,"y":-163.41685485839844},"size":{"width":450,"height":300},"type":"Container","title":"Files","description":"The sources detail several built-in data sources:\n• Parquet: This is the default and preferred data source for Spark. It is an open-source columnar format that offers I/O optimizations like compression, saving storage space and allowing quick access to columns.\n• JSON: A popular, easy-to-read format supported in both single-line and multiline modes.\n• CSV: A common text format where each field is delimited by a comma. It is popular among data and business analysts because it can be generated by spreadsheets.\n• Avro: Introduced as a built-in source in Spark 2.4, it is often used by Apache Kafka for message serialization.\n• ORC: An optimized columnar format that supports a vectorized reader, which reads blocks of rows at once to reduce CPU usage.\n• Images and Binary Files: Spark supports image files for machine learning applications. Spark 3.0 specifically added support for binary files, converting each file into a single DataFrame record containing raw content and metadata."},{"id":"node-1769451521963-24884","position":{"x":978.3477478027344,"y":-162.72982215881348},"size":{"width":450,"height":300},"type":"Container","title":"Streams"},{"id":"node-1769451521963-427374","position":{"x":1542.7810821533203,"y":-168.2375841140747},"size":{"width":450,"height":300},"type":"Container","title":"Warehouses"},{"id":"node-1769451521964-157703","position":{"x":-170.21849060058594,"y":-187.41349744796753},"size":{"width":450,"height":300},"type":"Container","title":"Collections"},{"id":"node-1769603578618","position":{"x":649.932373046875,"y":462.74169921875},"size":{"width":450,"height":300},"type":"Container","title":"Hive Metastore"}]},{"id":"node-1769451530018-624828","position":{"x":5495.882058496813,"y":4956.831695092302},"size":{"width":1523.8358764648438,"height":1197.90283203125},"type":"Container","title":"Output","pan":{"x":-879.7247105729607,"y":-511.83276988158303,"scale":0.43876051149948475},"children":[{"id":"node-1769451530018-10157","position":{"x":338.58861541748047,"y":-175.84991073608398},"size":{"width":450,"height":300},"type":"Container","title":"Data warehouse Lakehouse","color":"transparent"},{"id":"node-1769451530018-583375","position":{"x":-722.421875,"y":-168.98964881896973},"size":{"width":450,"height":300},"type":"Container","title":"Files","color":"transparent"},{"id":"node-1769451530018-837784","position":{"x":872.5235748291016,"y":-182.4047393798828},"size":{"width":450,"height":300},"type":"Container","title":"Streams","color":"transparent"},{"id":"node-1769451530018-569505","position":{"x":-202.26616668701172,"y":-170.52281951904297},"size":{"width":450,"height":300},"type":"Container","title":"Databases","color":"transparent"},{"id":"node-1769451530018-463867","position":{"x":-207.32096195220947,"y":323.7085876464844},"size":{"width":450,"height":300},"type":"Container","title":"Console","color":"transparent"},{"id":"node-1769451530018-492893","position":{"x":340.82635498046875,"y":319.0240478515625},"size":{"width":450,"height":300},"type":"Container","title":"Memory","color":"transparent"}]},{"id":"node-1769517247275","position":{"x":4006.528076171875,"y":-2452.5940856933594},"size":{"width":3542.773193359375,"height":2455.9905281066895},"type":"Container","title":"Spark UI","description":"Spark includes a graphical user interface that you can use to inspect or monitor Spark applications in their various stages of decomposition—that is jobs, stages, and tasks.\n\nDepending on how Spark is deployed, the driver launches a web UI, running by\ndefault on port 4040, where you can view metrics and details such as:\n• A list of scheduler stages and tasks\n• A summary of RDD sizes and memory usage\n• Information about the environment\n• Information about the running executors\n• All the Spark SQL queries"},{"id":"node-1769595074486","position":{"x":-4194.7662353515625,"y":-2402.170944213867},"size":{"width":3207.0591735839844,"height":2461.9686317443848},"type":"Container","title":"Installation","pan":{"x":67.12571164490961,"y":58.92552735196642,"scale":0.6520145459695635},"children":[{"id":"node-1769595223991","position":{"x":87.79092407226562,"y":105.61155700683594},"size":{"width":4848.811340332031,"height":3316.9931640625},"type":"CodeBox","content":"pip install pyspark\n# A critical prerequisite is having Java 8 or above installed with the JAVA_HOME environment variable properly set\n\n# bin: Contains interactive shells such as pyspark, spark-shell, spark-sql, and sparkR.\n# sbin: Contains administrative scripts to start and stop Spark components.\n# kubernetes: Provides Dockerfiles for creating images for Spark on Kubernetes clusters.\n# data: Populated with .txt files used as input for various Spark components.\n\n\n\n\n\n\n\n\n\n"}]},{"id":"node-1769603202874-870967","position":{"x":230.81401565512078,"y":4803.187265530088},"size":{"width":2451.3806762695312,"height":1633.7938232421875},"type":"Container","title":"Ecosystem","pan":{"x":-388.93795572345755,"y":-362.8326781494336,"scale":2.159244113039222},"children":[{"id":"node-1769603202874-895150","position":{"x":868.9764404296875,"y":918.2609252929688},"size":{"width":450,"height":300},"type":"Container","title":"Spark Core","description":"Spark Core contains the basic functionality of Spark, \nincluding components for task scheduling, memory management, fault recovery, interacting with storage systems, and more. \nSpark Core is also home to the API that defines resilient distributed datasets (RDDs), which are Spark’s main programming abstraction. \nRDDs represent a collection of items distributed across many compute nodes that can be manipulated in parallel. \n","pan":{"x":-490.8382115794601,"y":-327.2254743863067,"scale":0.23914845000000007},"children":[{"id":"node-1769603202874-292469","position":{"x":801.1938495635986,"y":-303.746826171875},"size":{"width":450,"height":300},"type":"Container","title":"Partitioning","description":"In the Spark world, the partition is the smallest unit of data processing. \nSpark cannot process a dataset \"as a whole.\" \nInstead, it breaks the dataset down into these manageable chunks so it can apply the \"Divide and Conquer\" strategy across a cluster of computers.\n\n1. The Relationship: Partition - Task - Core\nTo understand Spark processing, you have to look at how these three elements link together:\nPartition: A physical slice of your data (e.g., 10,000 rows of a CSV).\nTask: A single unit of work (e.g., \"convert these strings to uppercase\").\nCore (Slot): A CPU thread on a Worker node.\nThe Rule: One Task processes exactly one Partition on one Core at a time.\n\n2. Parallelism vs. Partitioning\nIf you have a 1GB file and only 1 partition, \nSpark can only use 1 CPU core to process it—even if your cluster has 100 computers! The other 99 computers will sit idle.\nUnder-partitioning: Too few partitions = Low parallelism (slow).\nOver-partitioning: Too many partitions = High overhead (Spark spends more time managing tasks than actually processing data).\n\n3. How Spark decides the number of Partitions?\nSpark sets the initial number of partitions based on where the data comes from:\nFrom HDFS/S3: Usually based on the file \"blocks\" (defaulting to 128MB per partition).\nFrom Local Collections: If you use sc.parallelize(data), it usually defaults to the total number of cores in your cluster.\nFrom Shuffles: When you do a join or groupBy, Spark defaults to 200 partitions (this is a configurable setting called spark.sql.shuffle.partitions).\n\n4. What happens during processing?\nWhen you write code like rdd.map(lambda x: x + 1), Spark doesn't send the data to the code; it sends the code to the data.\nThe Driver sends the \"Map\" function to every Executor.\nEach Executor applies that function to the partitions it has in its local memory.\nBecause each partition is independent, they can all be processed at the exact same time without talking to each other.\n"},{"id":"node-1769603202874-365908","position":{"x":803.9901885986328,"y":76.01147270202637},"size":{"width":450,"height":300},"type":"Container","title":"RDD","description":"The RDD is the most basic abstraction in Spark. There are three vital characteristics associated with an RDD:\n• Dependencies\n• Partitions (with some locality information)\n• Compute function: Partition => Iterator[T]\n\nAn RDD in Spark is simply an immutable distributed collection of objects. \nEach RDD is split into multiple partitions, which may be computed on different nodes of the cluster. \nRDDs can contain any type of Python, Java, or Scala objects, including user-defined classes.\nIn Spark all work is expressed as either creating new RDDs, transforming existing RDDs, or calling operations on RDDs to compute a result. \nUnder the hood, Spark automatically distributes the data contained in RDDs across your cluster and parallelizes the operations you perform on them.\n\nUsers create RDDs in two ways: \n1) by loading an external dataset, or \n2) by distributing a collection of objects (e.g., a list or set) in their driver program.\n\nOnce created, RDDs offer two types of operations: \n1) transformations and \n2) actions.\nTransformations construct a new RDD from a previous one.\nActions, on the other hand, compute a result based on an RDD, and either return it to the driver program or save it to an external storage system.\nReturn type: transformations return RDDs, whereas actions return some other data type.\n\nTo summarize, every Spark program and shell session will work as follows:\n1. Create some input RDDs from external data.\n2. Transform them to define new RDDs using transformations like filter().\n3. Ask Spark to persist() any intermediate RDDs that will need to be reused.\n4. Launch actions such as count() and first() to kick off a parallel computation, which is then optimized and executed by Spark.\n","pan":{"x":3.660232220305005,"y":4.6586927653168,"scale":0.5118894105964976},"children":[{"id":"node-1769603202874-959215","position":{"x":38.127410888671875,"y":24.419944763183594},"size":{"width":799.1734008789062,"height":507.9425354003906},"type":"CodeBox","content":"# READ\n#local collection\ndata = [1, 2, 3, 4, 5]\nrdd = sc.parallelize(data)\n\n# Read a single file, a directory, or a wildcard pattern\nrdd_text = sc.textFile(\"path/to/data.txt\")\n\n# You can also specify the minimum number of partitions\nrdd_partitioned = sc.textFile(\"path/to/data.txt\", minPartitions=4)\n\n# Useful for processing a directory of small logs\nrdd_whole = sc.wholeTextFiles(\"path/to/directory/\")\n# Result: [('file1.txt', 'content...'), ('file2.txt', 'content...')]\n\n#from json\nrdd_raw_json = sc.textFile(\"data.json\")\nrdd_parsed_json = rdd_raw_json.map(lambda line: json.loads(line))\n\n#from csv\nrdd_raw_csv = sc.textFile(\"data.csv\")\ndef parse_csv(line):\n    # StringIO allows the csv module to treat a string like a file\n    f = StringIO(line)\n    reader = csv.reader(f)\n    return next(reader)\nrdd_csv = rdd_raw_csv.map(parse_csv)\n\n#binary files\nrdd_binary = sc.binaryFiles(\"path/to/images/\")\n\n\n#TRANSFORMATIONS\n#map: Transforms each element into exactly one new element.\n#flatMap: Similar to map, but each input item can be mapped to 0 or more output items.\n\n# Example: Converting text to uppercase\nupper_rdd = rdd.map(lambda line: line.upper())\n\n# Example: Splitting lines into individual words\nwords_rdd = rdd.flatMap(lambda line: line.split(\" \"))\n\n# Keep only lines that contain the word 'Spark'\nfiltered_rdd = rdd.filter(lambda line: \"Spark\" in line)\n\n#ACTIONS\n#Action\t                Description\n#collect()\t        Returns the entire RDD as a list to the driver. (Warning: Only use for small datasets!)\n#count()\t        Returns the number of elements in the RDD.\n#take(n)\t        Returns the first $n$ elements of the RDD.\n#saveAsTextFile(path)\tWrites the RDD to a text file (or directory of files).\n\n# Triggering the execution\nprint(f\"Total words: {word_counts.count()}\")\n\n# Taking a sample of the results\nprint(word_counts.take(5))\n\n# Saving the results back to disk\nword_counts.saveAsTextFile(\"output/word_count_results\")\n\n\n"}]},{"id":"node-1769603202874-980504","position":{"x":816.8458251953125,"y":426.38631439208984},"size":{"width":450,"height":300},"type":"Container","title":"Key-value RDD","description":"Key-Value Pair RDDs (often called \"Pair RDDs\") are a specialized type of RDD where each element is a tuple consisting of two items: (Key, Value).\n","children":[{"id":"node-1769603202874-385260","position":{"x":67.78445434570312,"y":33.99518585205078},"size":{"width":749.0989379882812,"height":478.214599609375},"type":"CodeBox","content":"# Starting with a regular RDD of strings\nlines = sc.textFile(\"data.csv\")\n\n# Transforming into a Pair RDD: (User_ID, Transaction_Amount)\n# Input line example: \"user123,50.00\"\npair_rdd = lines.map(lambda x: (x.split(\",\")[0], float(x.split(\",\")[1])))\n\n# EFFICIENT: Sum of amounts per user\ntotal_per_user = pair_rdd.reduceByKey(lambda a, b: a + b)\n\n# LESS EFFICIENT: Group all transactions into a list per user\nall_transactions = pair_rdd.groupByKey()\n\n# RDD 1: (User_ID, Name)\nnames = sc.parallelize([(\"1\", \"Alice\"), (\"2\", \"Bob\")])\n\n# RDD 2: (User_ID, Purchase_Amount)\npurchases = sc.parallelize([(\"1\", 25.00), (\"1\", 15.00), (\"2\", 40.00)])\n\n# Join them together\n# Result: (\"1\", (\"Alice\", 25.00)), (\"1\", (\"Alice\", 15.00)), (\"2\", (\"Bob\", 40.00))\njoined_rdd = names.join(purchases)\n\n"}]},{"id":"node-1769603202874-891029","position":{"x":-439.1964416503906,"y":-241.69775009155273},"size":{"width":1174.3028259277344,"height":980.4383087158203},"type":"CodeBox","content":"from pyspark import SparkConf, SparkContext\n\ndef has_word(line, word):\n    return word.lower() in line.lower()\n\nconf = SparkConf().setAppName(\"SparkByExamples.com\").setMaster(\"local[*]\")\nsc = SparkContext(conf = conf)\n\n#creating RDD from collection\ntokens = sc.parallelize([\"hello world\", \"spark by example\"])\nprint(tokens.count())\n\n#creating RDD from file\nlines = sc.textFile(\"data/book.txt\")\nlines.cache()\nprint(lines.count())\nprint(lines.first())\n\n# print line count having sundog (case-insensitive)\nfiltered_lines = lines.filter(lambda line: has_word(line, \"sundog\"))\nprint(filtered_lines.count())\nsc.stop()\n\n\n\n\n\n"}]},{"id":"node-1769603202874-405322","position":{"x":865.4361114501953,"y":523.4250793457031},"size":{"width":450,"height":300},"type":"Container","title":"Spark SQL","description":"However, the original RDD model has limitations because the compute function is opaque to Spark. \nBecause Spark cannot inspect the computation or the specific data types, it has no way to optimize the expression or rearrange it into an efficient query plan\n\nStructuring Spark\nTo address these issues, Spark 2.x introduced schemes to express computations using common patterns found in data analysis, such as filtering, selecting, and aggregating. \nThese operators are available via a domain-specific language (DSL) in Spark's supported languages (Java, Python, Scala, R, and SQL). \nThis structure allows data to be arranged in a tabular format and yields benefits like better performance, space efficiency, and expressivity. \nFor example, a query to average ages that requires complex lambda functions in the RDD API becomes much more expressive and simpler when using the DataFrame API.\n\nSpark SQL and the Underlying Engine\nThe Spark SQL engine is the substrate upon which these Structured APIs are built, allowing for optimized query plans and compact code generation.\n• Catalyst Optimizer: This component converts a query into an execution plan through four phases: Analysis, Logical Optimization, Physical Planning, and Code Generation.\n• Project Tungsten: This focuses on whole-stage code generation, which collapses the whole query into a single function to improve CPU efficiency and performance.\nIn summary, the high-level DataFrame and Dataset APIs are more expressive and intuitive than the low-level RDD API, allowing you to tell Spark what to do rather than how to do it\n\n\nTables in Spark hold data along with relevant metadata, such as the schema, table name, and physical location. All of this is stored in a central metastore; by default, Spark uses the Apache Hive metastore. Spark allows you to create two types of tables:\n• Managed Tables: Spark manages both the metadata and the data in the file store. If you drop a managed table, both the metadata and the actual data are deleted.\n• Unmanaged Tables: Spark only manages the metadata, while you manage the data yourself in an external source. Dropping an unmanaged table only removes the metadata, not the data.\nViews can also be created on top of existing tables. These can be session-scoped temporary views, which are visible only to a single SparkSession, or global temporary views, which are visible across all SparkSessions within a Spark application. Unlike tables, views do not actually hold data and disappear after the Spark application or session terminates.\n\n\n","pan":{"x":-294.02587011379444,"y":-250.24761529369096,"scale":0.17580460327901115},"children":[{"id":"node-1769603202874-772675","position":{"x":-212.60116577148438,"y":-8.73614501953125},"size":{"width":1528.3236083984375,"height":1029.8053588867188},"type":"Container","title":"Catalyst Optimiser","description":"Spark Application Concepts\nUnderstanding how Spark transforms code into execution requires familiarity with several key terms:\n• Application: A user program built on Spark APIs, consisting of a driver and executors.\n• SparkSession: The unified entry point for programming Spark with its Structured APIs.\n• Job: A parallel computation spawned in response to a Spark action.\n• Stage: A division of a job based on operation boundaries; stages dictate data transfer among executors.\n• Task: The smallest unit of execution, which is sent to a Spark executor; each task maps to a single core and works on a single partition of data.\n\nTransformations, Actions, and Lazy Evaluation\nSpark operations are classified into two categories:\n• Transformations: These operations, such as select() or filter(), are immutable, meaning they return a new DataFrame without altering the original. They are evaluated lazily, meaning Spark records them as a lineage but does not execute them immediately.\n• Actions: Operations like show(), count(), or collect() trigger the actual execution of all recorded transformations.\nNarrow vs. Wide Dependencies\n• Narrow Transformations: Any transformation where a single output partition can be computed from a single input partition (e.g., filter()), requiring no exchange of data across the cluster.\n• Wide Transformations: Operations like groupBy() or orderBy() that require Spark to perform a shuffle, where data from multiple partitions is read, combined, and written to disk","pan":{"x":-461.7574012581497,"y":-21.89482879779689,"scale":0.4361542012791446},"children":[{"id":"node-1769603202874-364068","position":{"x":-226.55312728881836,"y":143.19412231445312},"size":{"width":458.93631744384766,"height":336.3800048828125},"type":"Container","title":"Unresolved logical plan","description":"Submission: You submit the application (via spark-submit or a notebook).\n\nDriver Initialization: The Spark Driver starts and initializes the SparkSession and SparkContext.\n\nResource Request: The Driver contacts the Cluster Manager (YARN/K8s).\n\nExecutor Launch: The Cluster Manager starts Executors on worker nodes. The Executors register themselves with the Driver so the Driver knows how much \"horsepower\" it has available."},{"id":"node-1769603202874-323678","position":{"x":596.9351806640625,"y":155.15977478027344},"size":{"width":402.72259521484375,"height":336.98834228515625},"type":"Container","title":"Logical Plan","description":"Lineage Building: Spark builds a logical graph of every operation you call.\n\nThe Action: The process is \"stuck\" in this phase until an Action (e.g., .save(), .collect()) is called.\n\nDAG Creation: Upon the Action, the DAG Scheduler takes the lineage and transforms it into a formal Directed Acyclic Graph."},{"id":"node-1769603202874-350604","position":{"x":2055.0413818359375,"y":177.41152954101562},"size":{"width":425.3331298828125,"height":314.805908203125},"type":"Container","title":"Physical plans","description":"The DAG Scheduler is the high-level orchestrator that breaks the graph into physical execution units.\n\nStage Splitting: The DAG Scheduler looks for Shuffle dependencies (Wide transformations).\n\nEverything that can be done in one go without moving data across the network is grouped into a Single Stage.\n\nEvery time data needs to be redistributed (e.g., a join or reduceBy), a New Stage is created.\n\nTask Set Creation: Within each stage, the scheduler looks at the number of data partitions. It creates a Task Set—one Task for every partition of data."},{"id":"node-1769603202875-163736","position":{"x":2062.9361572265625,"y":816.796142578125},"size":{"width":450,"height":300},"type":"Container","title":"Cost model","description":"Now, the Task Scheduler takes over from the DAG Scheduler.\n\nTask Submission: The Task Scheduler looks at the available Executors and sends the Tasks to them.\n\nData Locality: The Task Scheduler tries to send the code to the node where the data already lives (e.g., \"moving the computation to the data\").\n\nExecution: The Executor receives the Task (bytecode) and runs it inside its JVM.\n\nNote for Python: If it's a PySpark job, the Executor's JVM launches a Python Worker process to handle the Python logic.\n\nShuffle Map: If there are multiple stages, the first stage writes its output to local disk (Shuffle files), and the next stage fetches that data."},{"id":"node-1769603202875-324822","position":{"x":2082.2591552734375,"y":1300.3270874023438},"size":{"width":450,"height":300},"type":"Container","title":"Selected Physical plan","description":"Result Aggregation: Once the final stage is complete, the results are sent back to the Driver (if you called .collect()) or written to the destination (like S3 or HDFS).\n\nCleanup: Once the main() method finishes or spark.stop() is called, the Driver informs the Cluster Manager to shut down the Executors and release the resources."},{"id":"node-1769603202875-218275","type":"Path","startElement":"node-1769603202874-364068","endElement":"node-1769603202874-323678"},{"id":"node-1769603202875-775384","type":"Path","startElement":"node-1769603202874-350604","endElement":"node-1769603202875-163736"},{"id":"node-1769603202875-879846","type":"Path","startElement":"node-1769603202875-163736","endElement":"node-1769603202875-324822"},{"id":"node-1769603202875-2388","position":{"x":-127.23131561279297,"y":1661.3951416015625},"size":{"width":514.131591796875,"height":324.018310546875},"type":"Container","title":"DAG","children":[{"id":"node-1769603202875-228447","position":{"x":159.35037231445312,"y":350.3323059082031},"size":{"width":522.7493286132812,"height":102.443359375},"type":"TextBox","title":"Stage","content":"Each job gets divided into smaller sets of tasks called stages that depend on each other.","color":"#ffb30033"},{"id":"node-1769603202875-674028","position":{"x":901.9952392578125,"y":348.0083465576172},"size":{"width":449.4486083984375,"height":97.55667114257812},"type":"TextBox","title":"Stage","color":"#ffb30033"},{"id":"node-1769603202875-463171","position":{"x":1543.5596923828125,"y":345.4484405517578},"size":{"width":537.16748046875,"height":97.35511779785156},"type":"TextBox","title":"Stage","color":"#ffb30033"},{"id":"node-1769603202875-685170","position":{"x":294.0989685058594,"y":574.1750793457031},"size":{"width":285.7440185546875,"height":100},"type":"TextBox","title":"Task","content":"A single unit of work or execution that will be sent to a Spark executor.","color":"#8d6e6333"},{"id":"node-1769603202875-951096","position":{"x":316.3985290527344,"y":775.1614990234375},"size":{"width":271.0838623046875,"height":100},"type":"TextBox","title":"Task","color":"#8d6e6333"},{"id":"node-1769603202875-17544","position":{"x":331.2255859375,"y":1014.6758422851562},"size":{"width":268.6405029296875,"height":104.88665771484375},"type":"TextBox","title":"Task","color":"#8d6e6333"},{"id":"node-1769603202875-4471","position":{"x":1017.4268798828125,"y":627.0839538574219},"size":{"width":273.5272216796875,"height":97.556640625},"type":"TextBox","title":"Task","color":"#8d6e6333"},{"id":"node-1769603202875-271039","position":{"x":1020.260009765625,"y":931.5952758789062},"size":{"width":280.857177734375,"height":100},"type":"TextBox","title":"Task","color":"#8d6e6333"},{"id":"node-1769603202875-968589","position":{"x":1689.9285888671875,"y":684.327880859375},"size":{"width":210,"height":100},"type":"TextBox","title":"Task","color":"#8d6e6333"},{"id":"node-1769603202875-44105","type":"Path","startElement":"node-1769603202875-685170","endElement":"node-1769603202875-951096"},{"id":"node-1769603202875-610236","type":"Path","startElement":"node-1769603202875-951096","endElement":"node-1769603202875-17544"},{"id":"node-1769603202875-590045","type":"Path","startElement":"node-1769603202875-17544","endElement":"node-1769603202875-4471"},{"id":"node-1769603202875-553807","type":"Path","startElement":"node-1769603202875-4471","endElement":"node-1769603202875-271039"},{"id":"node-1769603202875-997808","type":"Path","startElement":"node-1769603202875-271039","endElement":"node-1769603202875-968589"},{"id":"node-1769603202875-76851","position":{"x":129.27359008789062,"y":98.64378356933594},"size":{"width":1971.6580200195312,"height":114.66012573242188},"type":"TextBox","title":"Job","content":"A parallel computation consisting of multiple tasks that gets spawned in response to a Spark action (e.g., save(), collect()).","color":"#66bb6a33"}]},{"id":"node-1769603202875-217121","position":{"x":490.92918395996094,"y":1651.42578125},"size":{"width":510.76739501953125,"height":329.626220703125},"type":"Container","title":"transformations","color":"transparent","children":[{"id":"node-1769603202875-972403","position":{"x":29.273513793945312,"y":57.10669708251953},"size":{"width":470.3021240234375,"height":485.1553649902344},"type":"Container","title":"Narrow","description":"Narrow Transformations on RDDs (Resilient Distributed Datasets) are operations where each partition of the parent RDD is used by at most one partition of the child RDD. \nSince there is no need to shuffle data across the network between executors, these operations are highly efficient and fast.","color":"transparent","children":[{"id":"node-1769603202875-461893","position":{"x":39.36027526855469,"y":589.0945892333984},"size":{"width":846.5283203125,"height":308.635009765625},"type":"CodeBox","content":"# 1. map(func): Applies a function to each element\nrdd_map = rdd.map(lambda x: (x, 1))\n\n# 2. filter(func): Returns elements that satisfy the condition\nrdd_filter = rdd.filter(lambda x: x > 10)\n\n# 3. flatMap(func): Similar to map, but flattens the result\nrdd_flatmap = rdd.flatMap(lambda line: line.split(\" \"))\n\n# 4. mapPartitions(func): Runs a function on each partition as a whole\ndef process_partition(iterator):\n    # Initialize a resource once here (e.g., a database connection)\n    return [x * 2 for x in iterator]\n\nrdd_partition = rdd.mapPartitions(process_partition)\n\n# 5. mapPartitionsWithIndex(func): Includes the partition index\nrdd_idx = rdd.mapPartitionsWithIndex(lambda idx, it: [f\"Part: {idx}, Val: {x}\" for x in it])\n\n# 6. mapValues(func): Applies function only to the value, keeping the key intact\nrdd_kv = sc.parallelize([(\"a\", 1), (\"b\", 2)])\nrdd_map_val = rdd_kv.mapValues(lambda v: v + 10)\n\n# 7. flatMapValues(func): Same as above, but allows returning multiple values\nrdd_flat_val = rdd_kv.flatMapValues(lambda v: range(1, v + 1))\n\n# 8. union(other): Combines two RDDs (does not remove duplicates)\n# intersection, subtract, cartesian also perform on 2 RDDs\nrdd_union = rdd1.union(rdd2)\n\n# 9. sample(withReplacement, fraction, seed): Takes a random subset\nrdd_sample = rdd.sample(False, 0.1, 42)\n\n# 10. glom(): Coalesces all elements within each partition into a list\n# Result: One list per partition\nrdd_glommed = rdd.glom()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","color":"transparent"},{"id":"node-1769603202875-895800","position":{"x":43.36832046508789,"y":66.76531219482422},"size":{"width":190.66616821289062,"height":89.02900695800781},"type":"NoteBox"},{"id":"node-1769603202875-118104","position":{"x":533.857177734375,"y":65.2459487915039},"size":{"width":180.12045288085938,"height":81.99851989746094},"type":"NoteBox"},{"id":"node-1769603202875-103970","position":{"x":49.16535186767578,"y":189.4945526123047},"size":{"width":183.6356964111328,"height":90.78662109375},"type":"NoteBox"},{"id":"node-1769603202875-999729","position":{"x":48.270965576171875,"y":316.4006042480469},"size":{"width":183.63568115234375,"height":87.27139282226562},"type":"NoteBox"},{"id":"node-1769603202875-918605","position":{"x":546.356689453125,"y":316.8954620361328},"size":{"width":178.36285400390625,"height":87.27139282226562},"type":"NoteBox"},{"id":"node-1769603202875-377259","position":{"x":540.28564453125,"y":190.0965805053711},"size":{"width":174.84759521484375,"height":89.02900695800781},"type":"NoteBox"},{"id":"node-1769603202875-392310","type":"Path","startElement":"node-1769603202875-895800","endElement":"node-1769603202875-118104"},{"id":"node-1769603202875-834862","type":"Path","startElement":"node-1769603202875-103970","endElement":"node-1769603202875-377259"},{"id":"node-1769603202875-679380","type":"Path","startElement":"node-1769603202875-999729","endElement":"node-1769603202875-918605"}]},{"id":"node-1769603202875-779354","position":{"x":520.8079223632812,"y":55.512855529785156},"size":{"width":467.8658447265625,"height":485.96746826171875},"type":"Container","title":"Wide","description":"Wide Transformations are operations that require data to be redistributed across the cluster. This process is known as a Shuffle. Unlike narrow transformations, a single partition in the \"parent\" RDD may contribute data to multiple partitions in the \"child\" RDD, necessitating network I/O and disk I/O, which makes them more \"expensive\" in terms of performance.","color":"transparent","children":[{"id":"node-1769603202875-776928","position":{"x":24.093727111816406,"y":583.6126861572266},"size":{"width":873.9658203125,"height":310.10980224609375},"type":"CodeBox","content":"# 1. repartition(numPartitions): Reshuffles data to increase or decrease partitions\n# This always triggers a full shuffle to ensure data is balanced.\nrdd_repartitioned = rdd.repartition(10)\n\n# 2. coalesce(numPartitions, shuffle=True): \n# While coalesce is usually narrow (shuffle=False), setting shuffle=True \n# forces a wide transformation to redistribute data.\nrdd_coalesced_wide = rdd.coalesce(2, shuffle=True)\n\n# 3. groupByKey(): Groups all values for each key into a single sequence\n# Warning: This can be memory-intensive as it pulls all values into a list.\nrdd_grouped = rdd_kv.groupByKey()\n\n# 4. reduceByKey(func): Merges values for each key using an associative function\n# More efficient than groupByKey because it performs local stays/combining before shuffling.\nrdd_reduced = rdd_kv.reduceByKey(lambda a, b: a + b)\n\n# 5. aggregateByKey(zeroValue, seqOp, combOp): \n# Highly flexible aggregation allowing different input and output types.\nrdd_agg = rdd_kv.aggregateByKey(0, lambda acc, v: acc + v, lambda acc1, acc2: acc1 + acc2)\n\n# 6. join(other): Performs an Inner Join between two RDDs\nrdd_joined = rdd1.join(rdd2)\n\n# 7. leftOuterJoin(other) / rightOuterJoin(other): \n# Performs outer joins; keys present in one RDD but not the other result in None.\nrdd_left_join = rdd1.leftOuterJoin(rdd2)\n\n# 8. cogroup(other): Groups data from both RDDs sharing the same key\n# Returns (Key, (Iterable_from_RDD1, Iterable_from_RDD2))\nrdd_cogrouped = rdd1.cogroup(rdd2)\n\n# 9. sortByKey(ascending=True): Sorts the RDD by the keys\nrdd_sorted = rdd_kv.sortByKey()\n\n# 10. sortBy(func): Sorts based on a custom function\nrdd_sorted_custom = rdd.sortBy(lambda x: x[1])\n\n# 11. distinct(): Removes duplicate records across the entire RDD\n# Requires a shuffle to compare records that might be on different partitions.\nrdd_distinct = rdd.distinct()\n\n# 12. intersection(other): Returns elements common to both RDDs\nrdd_intersect = rdd1.intersection(rdd2)\n\n# 13. subtract(other): Returns elements in rdd1 that are NOT in rdd2\nrdd_diff = rdd1.subtract(rdd2)\n\n\n\n\n\n","color":"transparent"},{"id":"node-1769603202875-995897","position":{"x":115.5076675415039,"y":63.126739501953125},"size":{"width":178.36727905273438,"height":87.2787857055664},"type":"NoteBox"},{"id":"node-1769603202875-777077","position":{"x":121.01758575439453,"y":193.6598663330078},"size":{"width":178.36727905273438,"height":87.27879333496094},"type":"NoteBox"},{"id":"node-1769603202875-733243","position":{"x":126.43240356445312,"y":320.99928283691406},"size":{"width":180.1246337890625,"height":89.03616333007812},"type":"NoteBox"},{"id":"node-1769603202875-145305","position":{"x":572.1084594726562,"y":65.0386734008789},"size":{"width":183.639404296875,"height":82.00666809082031},"type":"NoteBox"},{"id":"node-1769603202875-510517","position":{"x":575.9323120117188,"y":188.61366271972656},"size":{"width":181.8819580078125,"height":85.52142333984375},"type":"NoteBox"},{"id":"node-1769603202875-412560","position":{"x":576.2415161132812,"y":315.7033996582031},"size":{"width":180.1246337890625,"height":85.52142333984375},"type":"NoteBox"},{"id":"node-1769603202875-487421","type":"Path","startElement":"node-1769603202875-995897","endElement":"node-1769603202875-510517"},{"id":"node-1769603202875-242237","type":"Path","startElement":"node-1769603202875-777077","endElement":"node-1769603202875-145305"},{"id":"node-1769603202875-814741","type":"Path","startElement":"node-1769603202875-733243","endElement":"node-1769603202875-510517"},{"id":"node-1769603202875-514194","type":"Path","startElement":"node-1769603202875-777077","endElement":"node-1769603202875-412560"},{"id":"node-1769603202875-269980","type":"Path","startElement":"node-1769603202875-733243","endElement":"node-1769603202875-412560"},{"id":"node-1769603202875-707810","type":"Path","startElement":"node-1769603202875-733243","endElement":"node-1769603202875-145305"},{"id":"node-1769603202875-851980","type":"Path","startElement":"node-1769603202875-995897","endElement":"node-1769603202875-412560"}]}]},{"id":"node-1769603202875-89082","position":{"x":1142.6610107421875,"y":1655.3770751953125},"size":{"width":520.816650390625,"height":322.762451171875},"type":"Container","title":"actions","description":"Actions are the operations that trigger the execution of the transformations recorded in the logical plan (DAG). While transformations are lazy, actions are eager: they compute a result and either return it to the Driver program or write it to an external storage system.","color":"transparent","children":[{"id":"node-1769603202875-255482","position":{"x":26.761474609375,"y":20.09466552734375},"size":{"width":965.9417724609375,"height":556.2090454101562},"type":"CodeBox","content":"# 1. collect(): Returns the entire RDD as a list to the driver\n# Use only on small/filtered datasets.\ndata_list = rdd.collect()\n\n# 2. take(n): Returns the first n elements of the RDD\nfirst_five = rdd.take(5)\n\n# 3. first(): Returns the first element of the RDD (similar to take(1))\nfirst_row = rdd.first()\n\n# 4. top(n): Returns the top n elements based on default or custom ordering\ntop_three = rdd.top(3)\n\n# 5. takeSample(withReplacement, num, seed): Returns a fixed-size random sample\nsample_data = rdd.takeSample(False, 10, 123)\n\n# 6. count(): Returns the total number of elements in the RDD\ntotal_rows = rdd.count()\n\n# 7. countByKey(): Returns a dictionary of (key, count) pairs\n# Only available on RDDs of (Key, Value) pairs.\nkey_counts = rdd_kv.countByKey()\n\n# 8. countByValue(): Returns a dictionary of (value, count) pairs\nval_counts = rdd.countByValue()\n\n# 9. reduce(func): Reduces the elements of the RDD using a commutative function\n# Example: Summing all numbers\ntotal_sum = rdd.reduce(lambda a, b: a + b)\n\n# 10. fold(zeroValue, op): Similar to reduce but with a starting 'zero' value\n# Useful for providing an initial state for calculations.\nsum_with_base = rdd.fold(0, lambda acc, v: acc + v)\n\n# 11. aggregate(zeroValue, seqOp, combOp): \n# Complex reduction allowing different return types (e.g., calculating average).\n# (Sum, Count) = rdd.aggregate((0, 0), \n#                              lambda acc, v: (acc[0] + v, acc[1] + 1), \n#                              lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1]))\n\n# 12. saveAsTextFile(path): Saves the RDD as text files (one per partition)\nrdd.saveAsTextFile(\"hdfs:///data/output/results_txt\")\n\n# 13. saveAsPickleFile(path): Saves the RDD using Python's pickle serialization\nrdd.saveAsPickleFile(\"s3://my-bucket/output/data.pkl\")\n\n# 14. foreach(func): Runs a function (like a print or DB write) on each element\n# Note: Since this runs on Executors, print() won't show on the Driver console.\nrdd.foreach(lambda x: print(x))\n\n\n\n\n","color":"transparent"}]},{"id":"node-1769603994983","position":{"x":281.4161071777344,"y":442.6715393066406},"size":{"width":242.2058868408203,"height":131.324951171875},"type":"NoteBox","content":"Analysis"},{"id":"node-1769603997635","position":{"x":1002.779296875,"y":480.7586669921875},"size":{"width":356.42901611328125,"height":195.96441650390625},"type":"NoteBox","content":"Logical Optimization"},{"id":"node-1769603999496","position":{"x":1771.625244140625,"y":469.09661865234375},"size":{"width":317.4986572265625,"height":219.85614013671875},"type":"NoteBox","content":"Physical Planning"},{"id":"node-1769604084287","position":{"x":283.4914245605469,"y":71.7498664855957},"size":{"width":241.5773162841797,"height":131.142578125},"type":"NoteBox","content":"Catalog"},{"id":"node-1769604125675","position":{"x":1293.0606079101562,"y":155.6121826171875},"size":{"width":460.7640380859375,"height":336.6885070800781},"type":"Container","title":"Optimized logical plan"},{"id":"path-1769604154160","type":"Path","startElement":"node-1769603202874-323678","endElement":"node-1769604125675"},{"id":"path-1769604209144","type":"Path","startElement":"node-1769604125675","endElement":"node-1769603202874-350604"}]},{"id":"node-1769603202875-362691","position":{"x":1590.663330078125,"y":342.9428405761719},"size":{"width":450,"height":300},"type":"Container","title":"Project Tungsten","pan":{"x":-396.75439042151413,"y":-264.5029269476761,"scale":0.2657205000000001},"children":[{"id":"node-1769604529613","position":{"x":-175.27745819091797,"y":-30.165481567382812},"size":{"width":450,"height":300},"type":"Container","title":"Selected Physical plan"},{"id":"node-1769604530293","position":{"x":628.1767883300781,"y":-44.142906188964844},"size":{"width":450,"height":300},"type":"Container","title":"RDD"},{"id":"node-1769604538856","position":{"x":290.9449920654297,"y":278.4501037597656},"size":{"width":308.8310852050781,"height":194.19769287109375},"type":"NoteBox","content":"Code generation"},{"id":"path-1769604590391","type":"Path","startElement":"node-1769604529613","endElement":"node-1769604530293"}]},{"id":"node-1769603202875-874970","type":"Path","startElement":"node-1769603202874-772675","endElement":"node-1769603202875-362691"}]},{"id":"node-1769603202875-428713","position":{"x":1207.9105224609375,"y":-111.04102754592896},"size":{"width":450,"height":300},"type":"Container","title":"Spark Streaming"},{"id":"node-1769603202875-639500","position":{"x":1736.9188232421875,"y":-113.43353700637817},"size":{"width":450,"height":300},"type":"Container","title":"MLlib"},{"id":"node-1769603202875-196732","position":{"x":2247.131591796875,"y":-120.0151596069336},"size":{"width":450,"height":300},"type":"Container","title":"GraphX"},{"id":"node-1769603202875-267998","type":"Path","startElement":"node-1769603202874-405322","endElement":"node-1769603202874-895150"},{"id":"node-1769603202875-160287","type":"Path","startElement":"node-1769603202875-428713","endElement":"node-1769603202874-405322"},{"id":"node-1769603202875-795750","type":"Path","startElement":"node-1769603202875-639500","endElement":"node-1769603202874-405322"},{"id":"node-1769603202875-177908","type":"Path","startElement":"node-1769603202875-196732","endElement":"node-1769603202874-405322"},{"id":"node-1769603202875-180089","position":{"x":-394.8951480248005,"y":-104.49277696230112},"size":{"width":450,"height":300},"type":"Container","title":"SQL"},{"id":"node-1769603202875-893458","position":{"x":153.3223679206341,"y":-104.49277696230112},"size":{"width":450,"height":300},"type":"Container","title":"Dataframe","description":"The DataFrame API, \nInspired by pandas DataFrames in structure, format, and a few specific operations,\nSpark DataFrames are distributed in-memory tables with named columns and schemas. \nThey support basic data types like integers and strings, as well as complex types like maps, arrays, and structs. \nWhile Spark can infer schemas, the sources recommend defining your schema up front to relieve Spark of the expensive task of inferring types and to detect errors early.\n• Columns and Expressions: Named columns in DataFrames are objects with public methods that can be used in logical or mathematical expressions.\n• Rows: A record in a DataFrame is represented as a generic Row object, where fields can be accessed by an index.\n• Common Operations: Data is typically loaded using a DataFrameReader and written back using a DataFrameWriter. Common transformations include projections and filters (using select() and where()), as well as aggregations (using groupBy(), orderBy(), and count())."},{"id":"node-1769603202875-462091","position":{"x":677.0658219001263,"y":-99.59797986440955},"size":{"width":450,"height":300},"type":"Container","title":"Dataset","description":"The Dataset API\nThe Dataset API provides a unified interface where DataFrames are considered \"untyped\" while Datasets are \"typed\".\n• Typed vs. Untyped: In Scala and Java, Datasets are collections of strongly typed objects, whereas in Python and R, only DataFrames (which are untyped) make sense because those languages are not compile-time type-safe.\n• Encoders: Datasets use encoders to efficiently convert data between JVM objects and Spark’s internal Tungsten binary format."},{"id":"node-1769603202875-165950","type":"Path","startElement":"node-1769603202875-180089","endElement":"node-1769603202874-405322"},{"id":"node-1769603202875-190157","type":"Path","startElement":"node-1769603202875-893458","endElement":"node-1769603202874-405322"},{"id":"node-1769603202875-507767","type":"Path","startElement":"node-1769603202875-462091","endElement":"node-1769603202874-405322"}]},{"id":"path-1769603520915","type":"Path","startElement":"node-1769451521963-436559","endElement":"node-1769603202874-870967"},{"id":"path-1769603524729","type":"Path","startElement":"node-1769603202874-870967","endElement":"node-1769451530018-624828"},{"id":"node-1769623588904","position":{"x":-4158.0997314453125,"y":913.6651611328125},"size":{"width":11645.67333984375,"height":2943.08544921875},"type":"Container","title":"Applications","pan":{"x":13627.920626870593,"y":1233.3432778030233,"scale":4.689301586769297},"children":[{"id":"node-1769623761561","position":{"x":13681.03515625,"y":1359.9850463867188},"size":{"width":450,"height":300},"type":"Container","title":"Batch Apps","pan":{"x":-983.9859953962198,"y":-655.99066359748,"scale":0.15690529804500009},"children":[{"id":"node-1769623846019-709130","position":{"x":146.51382446289062,"y":-292.95825576782227},"size":{"width":1025.0066528320312,"height":760.1856994628906},"type":"CodeBox","content":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nimport os\n\ndef run():\n    # 1. Initialize SparkSession\n    spark = (SparkSession\n        .builder\n        .appName(\"Chapter3Features\")\n        .getOrCreate())\n\n    # 2. Define a Schema\n    # Defining a schema up front prevents Spark from the expensive task of\n    # inferring data types and helps detect errors early.\n    # Here we use the programmatic StructType approach.\n    schema = StructType([\n        StructField(\"Id\", IntegerType(), False),\n        StructField(\"First\", StringType(), False),\n        StructField(\"Last\", StringType(), False),\n        StructField(\"Url\", StringType(), False),\n        StructField(\"Published\", StringType(), False), # Will be converted later\n        StructField(\"Hits\", IntegerType(), False),\n        StructField(\"Campaigns\", ArrayType(StringType()), False) # Complex Type: Array\n    ])\n\n    # 3. Create a DataFrame from static data\n    data = [\n        [1, \"Jules\", \"Damji\", \"https://tinyurl.1\", \"1/4/2016\", 4535, [\"twitter\", \"LinkedIn\"]],\n        [2, \"Brooke\", \"Wenig\", \"https://tinyurl.2\", \"5/5/2018\", 8908, [\"twitter\", \"LinkedIn\"]],\n        [3, \"Denny\", \"Lee\", \"https://tinyurl.3\", \"6/7/2019\", 7659, [\"web\", \"twitter\", \"FB\"]],\n        [4, \"Tathagata\", \"Das\", \"https://tinyurl.4\", \"5/12/2018\", 10568, [\"twitter\", \"FB\"]],\n        [5, \"Matei\", \"Zaharia\", \"https://tinyurl.5\", \"5/14/2014\", 40578, [\"web\", \"twitter\", \"FB\"]],\n        [6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 25568, [\"twitter\", \"LinkedIn\"]]\n    ]\n    blogs_df = spark.createDataFrame(data, schema)\n\n    # 4. Columns and Expressions\n    # Use expr() to compute a new value and withColumn() to append it.\n    blogs_df = blogs_df.withColumn(\"Hits_Double\", expr(\"Hits * 2\"))\n\n    # 5. Projections and Filters\n    # select() is used for projections, while where() or filter() acts as a filter.\n    few_blogs_df = (blogs_df\n        .select(\"Id\", \"First\", \"Hits\")\n        .where(col(\"Hits\") > 5000))\n\n    # 6. Date and Time Transformations\n    # Convert StringType to TimestampType using to_timestamp().\n    # We then use year() to extract the year from the converted date.\n    blogs_ts_df = (blogs_df\n        .withColumn(\"PublishedDate\", to_timestamp(col(\"Published\"), \"M/d/yyyy\"))\n        .drop(\"Published\") # Drop original column\n        .withColumn(\"Year\", year(col(\"PublishedDate\"))))\n\n    # 7. Renaming Columns\n    # Use withColumnRenamed() to improve readability or comply with format requirements.\n    blogs_ts_df = blogs_ts_df.withColumnRenamed(\"Id\", \"Blog_Id\")\n\n    # 8. Aggregations and Sorting\n    # groupBy() and count() offer the ability to aggregate by column names.\n    # We can also use sum, avg, min, and max.\n    agg_df = (blogs_ts_df\n        .groupBy(\"Year\")\n        .agg(\n            count(\"Blog_Id\").alias(\"Count\"),\n            sum(\"Hits\").alias(\"Total_Hits\"),\n            avg(\"Hits\").alias(\"Avg_Hits\")\n        )\n        .orderBy(desc(\"Total_Hits\"))) # Sort in descending order\n\n    # 9. Working with Rows\n    # DataFrames are composed of generic Row objects.\n    agg_df.show()\n    first_row = agg_df.first()\n    print(f\"Total rows in result: {agg_df.count()}\") # Action: triggers execution\n\n    # 10. Writing Data\n    # Use DataFrameWriter to save data. Parquet is the default and preferred format.\n    # Save the file to the project root directory\n    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n    output_path = os.path.join(project_root, \"blog_agg\")\n    agg_df.write.format(\"parquet\").mode(\"overwrite\").save(output_path)\n\n    # user input to pause the program before terminating\n    input(\"Press Enter to exit...\")\n    spark.stop()\n\nif __name__ == \"__main__\":\n    run()\n\n\n\n"},{"id":"node-1769623897684-794335","position":{"x":-904.5276213727823,"y":-285.4992962573921},"size":{"width":956.3966274261475,"height":755.3170471191406},"type":"CodeBox","content":"from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import count, col\n\ndef run():\n    # 1. SparkSession: The entry point to Spark\n    spark = (SparkSession\n      .builder\n      .appName(\"PythonMnMCount\")\n      .getOrCreate())\n\n    # Get data path (e.g., \"data/mnm_dataset.csv\")\n    mnm_file = \"data/mnm_dataset.csv\"\n\n    # 2. Reading Data (Transformation): Lazy operation\n    # Uses schema inference and treats file as an unbounded table\n    mnm_df = (spark.read.format(\"csv\")\n      .option(\"header\", \"true\")\n      .option(\"inferSchema\", \"true\")\n      .load(mnm_file))\n\n    # 3. Chaining Transformations:\n    # .select and .where are Narrow Transformations (no shuffle)\n    # .groupBy and .orderBy are Wide Transformations (require shuffle)\n    count_mnm_df = (mnm_df\n      .select(\"State\", \"Color\", \"Count\")\n      .where(col(\"State\") != \"NY\") # Narrow: filtering out New York\n      .groupBy(\"State\", \"Color\")    # Wide: requires a data shuffle\n      .agg(count(\"Count\").alias(\"Total\"))\n      .orderBy(\"Total\", ascending=False)) # Wide: sorting requires exchange\n\n    # 4. Action: Triggers the execution of the entire DAG/lineage\n    # Until .show() or .count() is called, no computation occurs\n    count_mnm_df.show(n=10, truncate=False)\n\n    # Secondary Action to illustrate lazy reuse of lineage\n    print(\"Total Rows = %d\" % (count_mnm_df.count()))\n\n    # Terminating the session\n    spark.stop()\n\nif __name__ == \"__main__\":\n    run()\n\n\n\n"},{"id":"node-1769624144013","position":{"x":-902.8283081054688,"y":-434.3245849609375},"size":{"width":392.37689208984375,"height":126.7843017578125},"type":"NoteBox","content":"mnm-count.py"},{"id":"node-1769624144899","position":{"x":155.34608459472656,"y":-458.1411437988281},"size":{"width":405.59259033203125,"height":126.78427124023438},"type":"NoteBox","content":"blog-agg.py"}]},{"id":"node-1769623765719","position":{"x":14319.251953125,"y":1354.8392333984375},"size":{"width":450,"height":300},"type":"Container","title":"Stream Apps"},{"id":"node-1769623766535","position":{"x":14977.6083984375,"y":1355.52734375},"size":{"width":450,"height":300},"type":"Container","title":"MLlib Apps"},{"id":"node-1769623767152","position":{"x":15655.29296875,"y":1363.5848388671875},"size":{"width":450,"height":300},"type":"Container","title":"GraphX Apps"}]}]}]}}