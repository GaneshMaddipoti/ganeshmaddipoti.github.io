{"id":null,"name":"ApacheSpark","data":{"id":"root","size":{"width":1728,"height":997},"pan":{"x":-1052.1360739751071,"y":-121.89932592190877,"scale":0.6737226453814675},"children":[{"id":"line-1768776347399","type":"Line","lineCoordinates":[{"x":1033.8216552734375,"y":1239.6767578125},{"x":1033.8216552734375,"y":1239.6767578125}]},{"id":"line-1768776355611","type":"Line","lineCoordinates":[{"x":1124.3660888671875,"y":1114.84912109375},{"x":1124.3660888671875,"y":1114.84912109375}]},{"id":"node-1768994213987","position":{"x":-766.9822323322296,"y":84.10484504699707},"size":{"width":1899.7925415039062,"height":1183.1466674804688},"type":"Container","title":"Apache Spark","description":"Apache Spark is a unified analytics engine for large-scale data processing. (Cluster computing platform)\nAt its core, Spark is a “computational engine” that is responsible for scheduling, distributing, and monitoring applications consisting of many computational tasks across many worker machines, or a computing cluster. \nIf Hadoop MapReduce was the \"first generation\" of big data processing, Spark is the \"second generation\"—built for speed, ease of use, and versatility.\n\nFeatures\nEasy to use: we can develop applications, using a high-level API that let us focus on the content of our computation\nSpeed: It is up to 100x faster than Hadoop MapReduce because it processes data in-memory (RAM) rather than writing to the disk after every step.\nLazy Evaluation: Spark doesn't execute your code immediately. It builds a plan (called a DAG or Directed Acyclic Graph) and only runs it when you finally ask for a result. This allows it to optimize the entire process first.\nAs general engine: letting you combine multiple types of computations (e.g., SQL queries, text process‐ing, and machine learning) that might previously have required different engines.\n\nPolyglot: You can write Spark code in Python (PySpark), Scala, Java, or R.","pan":{"x":-2344.010836635869,"y":-1196.1859852779571,"scale":0.2916599436531507},"children":[{"id":"node-1768994224387-937691","position":{"x":-1017.8547973632812,"y":-363.2659683227539},"size":{"width":3463.5639038085938,"height":2132.4646606445312},"type":"Container","title":"Process","color":"transparent","pan":{"x":-1358.0302674294824,"y":59.19631636772616,"scale":0.9266052860849929},"children":[{"id":"node-1768994224388-298441","position":{"x":119.09141540527344,"y":773.4568481445312},"size":{"width":450,"height":300},"type":"Container","title":"Client","description":"\n","children":[{"id":"node-1768994224388-124447","position":{"x":175.23153686523438,"y":126.59458923339844},"size":{"width":450,"height":300},"type":"Container","title":"Spark Driver","description":"Driver Program: The \"brain\" of your application. It runs your main() function and converts your code into tasks.\nAt a high level, every Spark application consists of a driver program that launches various parallel operations on a cluster. \nThe driver program contains your application’s main function and defines distributed datasets on the cluster, then applies operations to them.\nDriver programs access Spark through a SparkContext object, which represents a connection to a computing cluster.\n\n\nResponsibilities\n1) Maintains the SparkContext/SparkSession: It is the entry point of the application.\n2) Analyzes Code: it converts your high-level code (SQL, Python, Scala) into a logical plan.\n3) Creates the DAG: It builds a Directed Acyclic Graph (DAG) of all the transformations you want to perform.\n4) Schedules Tasks: It breaks the DAG into \"Stages\" and then into \"Tasks,\" which it sends to the Executors (worker nodes) to be processed.\n","children":[{"id":"node-1768994224388-255178","position":{"x":88.16413879394531,"y":13.675613403320312},"size":{"width":785.7688293457031,"height":523.0265502929688},"type":"CodeBox","content":"#Application \nfrom pyspark.sql import SparkSession\n\n# 1. The Driver starts and creates the SparkSession\nspark = SparkSession.builder.appName(\"LogAnalysis\").get_home()\n# 2. The Driver creates a plan to read the data (Lazy Evaluation)\nlogs = spark.read.text(\"s3://my-bucket/logs/*.txt\")\n# 3. The Driver plans the transformation (Filtering)\naws_logs = logs.filter(logs.value.contains(\"AWS\"))\n# 4. The Driver triggers an 'Action' (.count())\n# This is where the Driver converts the DAG into actual tasks\nresult = aws_logs.count()\n# 5. The Driver receives the final number from the executors and prints it\nprint(f\"Total AWS entries: {result}\")\n\nspark.stop()\n\n\n\n\n\n"}]}]},{"id":"node-1768994224388-583630","position":{"x":958.575439453125,"y":771.3358764648438},"size":{"width":450,"height":300},"type":"Container","title":"Cluster Manager","pan":{"x":-711.5286562709382,"y":-474.3524375139589,"scale":0.19371024450000007},"children":[{"id":"node-1768994224388-895464","position":{"x":15.268295288085938,"y":-87.52663993835449},"size":{"width":450,"height":300},"type":"Container","title":"Apache YARN"},{"id":"node-1768994224388-474429","position":{"x":-467.78299820423126,"y":-88.37072658538818},"size":{"width":450,"height":300},"type":"Container","title":"Standalone"},{"id":"node-1768994224388-830986","position":{"x":965.3104095458984,"y":-98.9773280620575},"size":{"width":450,"height":300},"type":"Container","title":"Mesos"},{"id":"node-1768994224388-433762","position":{"x":493.5273609161377,"y":-91.90109252929688},"size":{"width":450,"height":300},"type":"Container","title":"Kubernetes"}]},{"id":"node-1768994224388-889554","position":{"x":1420.81787109375,"y":269.3124084472656},"size":{"width":450,"height":300},"type":"Container","title":"Worker Node","children":[{"id":"node-1768994224388-5378","position":{"x":89.96749877929688,"y":62.73999786376953},"size":{"width":718.8135986328125,"height":423.2466735839844},"type":"Container","title":"Application Master","children":[{"id":"node-1768994224388-993284","position":{"x":145.12289428710938,"y":88.24881744384766},"size":{"width":1090.2388916015625,"height":657.4938354492188},"type":"Container","title":"Spark Driver","description":"Driver Program: The \"brain\" of your application. It runs your main() function and converts your code into tasks.\nAt a high level, every Spark application consists of a driver program that launches various parallel operations on a cluster. \nThe driver program contains your application’s main function and defines distributed datasets on the cluster, then applies operations to them.\nDriver programs access Spark through a SparkContext object, which represents a connection to a computing cluster.\n\n\nResponsibilities\n1) Maintains the SparkContext/SparkSession: It is the entry point of the application.\n2) Analyzes Code: it converts your high-level code (SQL, Python, Scala) into a logical plan.\n3) Creates the DAG: It builds a Directed Acyclic Graph (DAG) of all the transformations you want to perform.\n4) Schedules Tasks: It breaks the DAG into \"Stages\" and then into \"Tasks,\" which it sends to the Executors (worker nodes) to be processed.\n","children":[{"id":"node-1768994224388-815872","position":{"x":153.72389221191406,"y":68.72859191894531},"size":{"width":1840.5922546386719,"height":1064.607666015625},"type":"CodeBox","content":"#Application \nfrom pyspark.sql import SparkSession\n\n# 1. The Driver starts and creates the SparkSession\nspark = SparkSession.builder.appName(\"LogAnalysis\").get_home()\n# 2. The Driver creates a plan to read the data (Lazy Evaluation)\nlogs = spark.read.text(\"s3://my-bucket/logs/*.txt\")\n# 3. The Driver plans the transformation (Filtering)\naws_logs = logs.filter(logs.value.contains(\"AWS\"))\n# 4. The Driver triggers an 'Action' (.count())\n# This is where the Driver converts the DAG into actual tasks\nresult = aws_logs.count()\n# 5. The Driver receives the final number from the executors and prints it\nprint(f\"Total AWS entries: {result}\")\n\nspark.stop()\n\n\n\n\n\n"}]}]}]},{"id":"node-1768994224388-836652","position":{"x":1909.6837158203125,"y":657.9249267578125},"size":{"width":450,"height":300},"type":"Container","title":"Worker Node","children":[{"id":"node-1768994224388-201049","position":{"x":229.38766479492188,"y":158.36561584472656},"size":{"width":450,"height":300},"type":"Container","title":"Executor","children":[{"id":"node-1768994224388-518941","position":{"x":181.1865997314453,"y":118.54624938964844},"size":{"width":450,"height":300},"type":"Container","title":"JVM"}]}]},{"id":"node-1768994224388-291383","position":{"x":1871.712890625,"y":1227.8822021484375},"size":{"width":450,"height":300},"type":"Container","title":"Worker Node"},{"id":"node-1768994224388-33816","type":"Path","startElement":"node-1768994224388-298441","endElement":"node-1768994224388-583630"},{"id":"node-1768994224388-898071","type":"Path","startElement":"node-1768994224388-583630","endElement":"node-1768994224388-836652"},{"id":"node-1768994224388-592941","type":"Path","startElement":"node-1768994224388-583630","endElement":"node-1768994224388-291383"},{"id":"node-1768994224388-957355","type":"Path","startElement":"node-1768994224388-583630","endElement":"node-1768994224388-889554"},{"id":"node-1768994224388-720171","position":{"x":66.03778076171875,"y":6.603759765625},"type":"Line","lineCoordinates":[{"x":690.7655029296875,"y":292.893798828125},{"x":703.627197265625,"y":1635.557373046875}]},{"id":"node-1768994224388-939734","position":{"x":369.8416290283203,"y":291.53314208984375},"size":{"width":364.4654235839844,"height":100},"type":"TitleBox","content":"Client Mode"},{"id":"node-1768994224388-377469","position":{"x":780.3194580078125,"y":287.870849609375},"size":{"width":360.314697265625,"height":95.837158203125},"type":"TitleBox","content":"Cluster Mode"},{"id":"node-1768994224388-360483","position":{"x":12.390594482421875,"y":1472.1219482421875},"size":{"width":567.1286010742188,"height":205.166748046875},"type":"NoteBox","content":"In Local Mode:\nThe entire Spark application (Driver and Executors) runs inside a single JVM on your local machine."},{"id":"node-1768994224388-656731","position":{"x":13.357620239257812,"y":1282.0543212890625},"size":{"width":561.3067474365234,"height":164.771728515625},"type":"NoteBox","content":"In enterprise setups, we don't submit jobs from our own laptop. Instead, we log into a Gateway (Edge) Node."},{"id":"node-1768994224388-25427","position":{"x":839.8312606811523,"y":1474.4766540527344},"size":{"width":602.2201538085938,"height":213.4378662109375},"type":"NoteBox","content":"In cluster mode:\nThe Cluster Manager starts the AM first, and then the AM launches the Spark Driver inside its own process"},{"id":"node-1769427527508-347447","position":{"x":-1168.5460690922373,"y":345.18267134390237},"size":{"width":970.424072265625,"height":553.9180908203125},"type":"Container","title":"Application Process","pan":{"x":-521.4567263983674,"y":-32.84365355594755,"scale":0.2692108290161262},"children":[{"id":"node-1769427527509-101739","position":{"x":-188.64120864868164,"y":146.11041259765625},"size":{"width":450,"height":300},"type":"Container","title":"Setup","description":"Submission: You submit the application (via spark-submit or a notebook).\n\nDriver Initialization: The Spark Driver starts and initializes the SparkSession and SparkContext.\n\nResource Request: The Driver contacts the Cluster Manager (YARN/K8s).\n\nExecutor Launch: The Cluster Manager starts Executors on worker nodes. The Executors register themselves with the Driver so the Driver knows how much \"horsepower\" it has available."},{"id":"node-1769427527509-438217","position":{"x":423.27001953125,"y":139.7908935546875},"size":{"width":450,"height":300},"type":"Container","title":"Logical Plan","description":"Lineage Building: Spark builds a logical graph of every operation you call.\n\nThe Action: The process is \"stuck\" in this phase until an Action (e.g., .save(), .collect()) is called.\n\nDAG Creation: Upon the Action, the DAG Scheduler takes the lineage and transforms it into a formal Directed Acyclic Graph."},{"id":"node-1769427527509-388223","position":{"x":1036.1981201171875,"y":141.15859985351562},"size":{"width":450,"height":300},"type":"Container","title":"Physical Plan","description":"The DAG Scheduler is the high-level orchestrator that breaks the graph into physical execution units.\n\nStage Splitting: The DAG Scheduler looks for Shuffle dependencies (Wide transformations).\n\nEverything that can be done in one go without moving data across the network is grouped into a Single Stage.\n\nEvery time data needs to be redistributed (e.g., a join or reduceBy), a New Stage is created.\n\nTask Set Creation: Within each stage, the scheduler looks at the number of data partitions. It creates a Task Set—one Task for every partition of data."},{"id":"node-1769427527509-286959","position":{"x":1645.2275390625,"y":141.09066772460938},"size":{"width":450,"height":300},"type":"Container","title":"Execution","description":"Now, the Task Scheduler takes over from the DAG Scheduler.\n\nTask Submission: The Task Scheduler looks at the available Executors and sends the Tasks to them.\n\nData Locality: The Task Scheduler tries to send the code to the node where the data already lives (e.g., \"moving the computation to the data\").\n\nExecution: The Executor receives the Task (bytecode) and runs it inside its JVM.\n\nNote for Python: If it's a PySpark job, the Executor's JVM launches a Python Worker process to handle the Python logic.\n\nShuffle Map: If there are multiple stages, the first stage writes its output to local disk (Shuffle files), and the next stage fetches that data."},{"id":"node-1769427527509-211650","position":{"x":2247.9852294921875,"y":130.32131958007812},"size":{"width":450,"height":300},"type":"Container","title":"Conclusion","description":"Result Aggregation: Once the final stage is complete, the results are sent back to the Driver (if you called .collect()) or written to the destination (like S3 or HDFS).\n\nCleanup: Once the main() method finishes or spark.stop() is called, the Driver informs the Cluster Manager to shut down the Executors and release the resources."},{"id":"node-1769427527509-771153","type":"Path","startElement":"node-1769427527509-101739","endElement":"node-1769427527509-438217"},{"id":"node-1769427527509-720649","type":"Path","startElement":"node-1769427527509-438217","endElement":"node-1769427527509-388223"},{"id":"node-1769427527509-326536","type":"Path","startElement":"node-1769427527509-388223","endElement":"node-1769427527509-286959"},{"id":"node-1769427527509-437968","type":"Path","startElement":"node-1769427527509-286959","endElement":"node-1769427527509-211650"},{"id":"node-1769427527509-835297","position":{"x":535.770751953125,"y":605.947509765625},"size":{"width":1149.5657958984375,"height":663.64697265625},"type":"Container","title":"DAG","children":[{"id":"node-1769427527509-680860","position":{"x":159.35037231445312,"y":350.3323059082031},"size":{"width":522.7493286132812,"height":102.443359375},"type":"TextBox","title":"Stage","color":"#ffb30033"},{"id":"node-1769427527509-698338","position":{"x":901.9952392578125,"y":348.0083465576172},"size":{"width":449.4486083984375,"height":97.55667114257812},"type":"TextBox","title":"Stage","color":"#ffb30033"},{"id":"node-1769427527509-532130","position":{"x":1543.5596923828125,"y":345.4484405517578},"size":{"width":537.16748046875,"height":97.35511779785156},"type":"TextBox","title":"Stage","color":"#ffb30033"},{"id":"node-1769427527509-905243","position":{"x":294.0989685058594,"y":574.1750793457031},"size":{"width":285.7440185546875,"height":100},"type":"TextBox","title":"Task","color":"#8d6e6333"},{"id":"node-1769427527509-631686","position":{"x":316.3985290527344,"y":775.1614990234375},"size":{"width":271.0838623046875,"height":100},"type":"TextBox","title":"Task","color":"#8d6e6333"},{"id":"node-1769427527509-215598","position":{"x":331.2255859375,"y":1014.6758422851562},"size":{"width":268.6405029296875,"height":104.88665771484375},"type":"TextBox","title":"Task","color":"#8d6e6333"},{"id":"node-1769427527509-644672","position":{"x":1017.4268798828125,"y":627.0839538574219},"size":{"width":273.5272216796875,"height":97.556640625},"type":"TextBox","title":"Task","color":"#8d6e6333"},{"id":"node-1769427527509-667858","position":{"x":1020.260009765625,"y":931.5952758789062},"size":{"width":280.857177734375,"height":100},"type":"TextBox","title":"Task","color":"#8d6e6333"},{"id":"node-1769427527509-9000","position":{"x":1689.9285888671875,"y":684.327880859375},"size":{"width":210,"height":100},"type":"TextBox","title":"Task","color":"#8d6e6333"},{"id":"node-1769427527509-83146","type":"Path","startElement":"node-1769427527509-905243","endElement":"node-1769427527509-631686"},{"id":"node-1769427527509-867240","type":"Path","startElement":"node-1769427527509-631686","endElement":"node-1769427527509-215598"},{"id":"node-1769427527509-623153","type":"Path","startElement":"node-1769427527509-215598","endElement":"node-1769427527509-644672"},{"id":"node-1769427527509-120505","type":"Path","startElement":"node-1769427527509-644672","endElement":"node-1769427527509-667858"},{"id":"node-1769427527509-889120","type":"Path","startElement":"node-1769427527509-667858","endElement":"node-1769427527509-9000"},{"id":"node-1769427527509-388391","position":{"x":129.27359008789062,"y":98.64378356933594},"size":{"width":1971.6580200195312,"height":114.66012573242188},"type":"TextBox","title":"Job","color":"#66bb6a33"}]},{"id":"node-1769445558338","position":{"x":534.9943084716797,"y":1468.0633544921875},"size":{"width":512.5509033203125,"height":312.447265625},"type":"Container","title":"transformations","color":"transparent","children":[{"id":"node-1769445695142","position":{"x":1.9686431884765625,"y":130.23260498046875},"size":{"width":450,"height":300},"type":"Container","title":"Narrow","description":"Narrow Transformations on RDDs (Resilient Distributed Datasets) are operations where each partition of the parent RDD is used by at most one partition of the child RDD. Since there is no need to shuffle data across the network between executors, these operations are highly efficient and fast.","color":"transparent","children":[{"id":"node-1769446197002","position":{"x":28.814559936523438,"y":35.44407653808594},"size":{"width":825.4368286132812,"height":507.24615478515625},"type":"CodeBox","content":"# 1. map(func): Applies a function to each element\nrdd_map = rdd.map(lambda x: (x, 1))\n\n# 2. filter(func): Returns elements that satisfy the condition\nrdd_filter = rdd.filter(lambda x: x > 10)\n\n# 3. flatMap(func): Similar to map, but flattens the result\nrdd_flatmap = rdd.flatMap(lambda line: line.split(\" \"))\n\n# 4. mapPartitions(func): Runs a function on each partition as a whole\ndef process_partition(iterator):\n    # Initialize a resource once here (e.g., a database connection)\n    return [x * 2 for x in iterator]\n\nrdd_partition = rdd.mapPartitions(process_partition)\n\n# 5. mapPartitionsWithIndex(func): Includes the partition index\nrdd_idx = rdd.mapPartitionsWithIndex(lambda idx, it: [f\"Part: {idx}, Val: {x}\" for x in it])\n\n# 6. mapValues(func): Applies function only to the value, keeping the key intact\nrdd_kv = sc.parallelize([(\"a\", 1), (\"b\", 2)])\nrdd_map_val = rdd_kv.mapValues(lambda v: v + 10)\n\n# 7. flatMapValues(func): Same as above, but allows returning multiple values\nrdd_flat_val = rdd_kv.flatMapValues(lambda v: range(1, v + 1))\n\n# 8. union(other): Combines two RDDs (does not remove duplicates)\n# intersection, subtract, cartesian also perform on 2 RDDs\nrdd_union = rdd1.union(rdd2)\n\n# 9. sample(withReplacement, fraction, seed): Takes a random subset\nrdd_sample = rdd.sample(False, 0.1, 42)\n\n# 10. glom(): Coalesces all elements within each partition into a list\n# Result: One list per partition\nrdd_glommed = rdd.glom()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","color":"transparent"}]},{"id":"node-1769445695699","position":{"x":531.364990234375,"y":127.78842163085938},"size":{"width":450,"height":300},"type":"Container","title":"Wide","description":"Wide Transformations are operations that require data to be redistributed across the cluster. This process is known as a Shuffle. Unlike narrow transformations, a single partition in the \"parent\" RDD may contribute data to multiple partitions in the \"child\" RDD, necessitating network I/O and disk I/O, which makes them more \"expensive\" in terms of performance.","color":"transparent","children":[{"id":"node-1769446747400","position":{"x":4.762611389160156,"y":19.495712280273438},"size":{"width":882.752685546875,"height":506.9356689453125},"type":"CodeBox","content":"# 1. repartition(numPartitions): Reshuffles data to increase or decrease partitions\n# This always triggers a full shuffle to ensure data is balanced.\nrdd_repartitioned = rdd.repartition(10)\n\n# 2. coalesce(numPartitions, shuffle=True): \n# While coalesce is usually narrow (shuffle=False), setting shuffle=True \n# forces a wide transformation to redistribute data.\nrdd_coalesced_wide = rdd.coalesce(2, shuffle=True)\n\n# 3. groupByKey(): Groups all values for each key into a single sequence\n# Warning: This can be memory-intensive as it pulls all values into a list.\nrdd_grouped = rdd_kv.groupByKey()\n\n# 4. reduceByKey(func): Merges values for each key using an associative function\n# More efficient than groupByKey because it performs local stays/combining before shuffling.\nrdd_reduced = rdd_kv.reduceByKey(lambda a, b: a + b)\n\n# 5. aggregateByKey(zeroValue, seqOp, combOp): \n# Highly flexible aggregation allowing different input and output types.\nrdd_agg = rdd_kv.aggregateByKey(0, lambda acc, v: acc + v, lambda acc1, acc2: acc1 + acc2)\n\n# 6. join(other): Performs an Inner Join between two RDDs\nrdd_joined = rdd1.join(rdd2)\n\n# 7. leftOuterJoin(other) / rightOuterJoin(other): \n# Performs outer joins; keys present in one RDD but not the other result in None.\nrdd_left_join = rdd1.leftOuterJoin(rdd2)\n\n# 8. cogroup(other): Groups data from both RDDs sharing the same key\n# Returns (Key, (Iterable_from_RDD1, Iterable_from_RDD2))\nrdd_cogrouped = rdd1.cogroup(rdd2)\n\n# 9. sortByKey(ascending=True): Sorts the RDD by the keys\nrdd_sorted = rdd_kv.sortByKey()\n\n# 10. sortBy(func): Sorts based on a custom function\nrdd_sorted_custom = rdd.sortBy(lambda x: x[1])\n\n# 11. distinct(): Removes duplicate records across the entire RDD\n# Requires a shuffle to compare records that might be on different partitions.\nrdd_distinct = rdd.distinct()\n\n# 12. intersection(other): Returns elements common to both RDDs\nrdd_intersect = rdd1.intersection(rdd2)\n\n# 13. subtract(other): Returns elements in rdd1 that are NOT in rdd2\nrdd_diff = rdd1.subtract(rdd2)\n\n\n\n\n\n","color":"transparent"}]}]},{"id":"node-1769445558995","position":{"x":1239.0589599609375,"y":1454.8355712890625},"size":{"width":520.816650390625,"height":322.762451171875},"type":"Container","title":"actions","description":"Actions are the operations that trigger the execution of the transformations recorded in the logical plan (DAG). While transformations are lazy, actions are eager: they compute a result and either return it to the Driver program or write it to an external storage system.","color":"transparent","children":[{"id":"node-1769446995613","position":{"x":26.761474609375,"y":20.09466552734375},"size":{"width":965.9417724609375,"height":556.2090454101562},"type":"CodeBox","content":"# 1. collect(): Returns the entire RDD as a list to the driver\n# Use only on small/filtered datasets.\ndata_list = rdd.collect()\n\n# 2. take(n): Returns the first n elements of the RDD\nfirst_five = rdd.take(5)\n\n# 3. first(): Returns the first element of the RDD (similar to take(1))\nfirst_row = rdd.first()\n\n# 4. top(n): Returns the top n elements based on default or custom ordering\ntop_three = rdd.top(3)\n\n# 5. takeSample(withReplacement, num, seed): Returns a fixed-size random sample\nsample_data = rdd.takeSample(False, 10, 123)\n\n# 6. count(): Returns the total number of elements in the RDD\ntotal_rows = rdd.count()\n\n# 7. countByKey(): Returns a dictionary of (key, count) pairs\n# Only available on RDDs of (Key, Value) pairs.\nkey_counts = rdd_kv.countByKey()\n\n# 8. countByValue(): Returns a dictionary of (value, count) pairs\nval_counts = rdd.countByValue()\n\n# 9. reduce(func): Reduces the elements of the RDD using a commutative function\n# Example: Summing all numbers\ntotal_sum = rdd.reduce(lambda a, b: a + b)\n\n# 10. fold(zeroValue, op): Similar to reduce but with a starting 'zero' value\n# Useful for providing an initial state for calculations.\nsum_with_base = rdd.fold(0, lambda acc, v: acc + v)\n\n# 11. aggregate(zeroValue, seqOp, combOp): \n# Complex reduction allowing different return types (e.g., calculating average).\n# (Sum, Count) = rdd.aggregate((0, 0), \n#                              lambda acc, v: (acc[0] + v, acc[1] + 1), \n#                              lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1]))\n\n# 12. saveAsTextFile(path): Saves the RDD as text files (one per partition)\nrdd.saveAsTextFile(\"hdfs:///data/output/results_txt\")\n\n# 13. saveAsPickleFile(path): Saves the RDD using Python's pickle serialization\nrdd.saveAsPickleFile(\"s3://my-bucket/output/data.pkl\")\n\n# 14. foreach(func): Runs a function (like a print or DB write) on each element\n# Note: Since this runs on Executors, print() won't show on the Driver console.\nrdd.foreach(lambda x: print(x))\n\n\n\n\n","color":"transparent"}]}]},{"id":"node-1769427567554-903900","position":{"x":-1179.5829953617686,"y":1192.6932914610898},"size":{"width":989.832763671875,"height":578.929931640625},"type":"Container","title":"Ecosystem","pan":{"x":63.35191514756972,"y":-17.658881293402807,"scale":0.45},"children":[{"id":"node-1769427567554-620463","position":{"x":868.9764404296875,"y":813.6956176757812},"size":{"width":450,"height":300},"type":"Container","title":"Spark Core","description":"Spark Core contains the basic functionality of Spark, \nincluding components for task scheduling, memory management, fault recovery, interacting with storage systems, and more. \nSpark Core is also home to the API that defines resilient distributed datasets (RDDs), which are Spark’s main programming abstraction. \nRDDs represent a collection of items distributed across many compute nodes that can be manipulated in parallel. \n","pan":{"x":-490.8382115794601,"y":-327.2254743863067,"scale":0.23914845000000007},"children":[{"id":"node-1769427567554-930557","position":{"x":801.1938495635986,"y":-303.746826171875},"size":{"width":450,"height":300},"type":"Container","title":"Partitioning","description":"In the Spark world, the partition is the smallest unit of data processing. \nSpark cannot process a dataset \"as a whole.\" \nInstead, it breaks the dataset down into these manageable chunks so it can apply the \"Divide and Conquer\" strategy across a cluster of computers.\n\n1. The Relationship: Partition - Task - Core\nTo understand Spark processing, you have to look at how these three elements link together:\nPartition: A physical slice of your data (e.g., 10,000 rows of a CSV).\nTask: A single unit of work (e.g., \"convert these strings to uppercase\").\nCore (Slot): A CPU thread on a Worker node.\nThe Rule: One Task processes exactly one Partition on one Core at a time.\n\n2. Parallelism vs. Partitioning\nIf you have a 1GB file and only 1 partition, \nSpark can only use 1 CPU core to process it—even if your cluster has 100 computers! The other 99 computers will sit idle.\nUnder-partitioning: Too few partitions = Low parallelism (slow).\nOver-partitioning: Too many partitions = High overhead (Spark spends more time managing tasks than actually processing data).\n\n3. How Spark decides the number of Partitions?\nSpark sets the initial number of partitions based on where the data comes from:\nFrom HDFS/S3: Usually based on the file \"blocks\" (defaulting to 128MB per partition).\nFrom Local Collections: If you use sc.parallelize(data), it usually defaults to the total number of cores in your cluster.\nFrom Shuffles: When you do a join or groupBy, Spark defaults to 200 partitions (this is a configurable setting called spark.sql.shuffle.partitions).\n\n4. What happens during processing?\nWhen you write code like rdd.map(lambda x: x + 1), Spark doesn't send the data to the code; it sends the code to the data.\nThe Driver sends the \"Map\" function to every Executor.\nEach Executor applies that function to the partitions it has in its local memory.\nBecause each partition is independent, they can all be processed at the exact same time without talking to each other.\n"},{"id":"node-1769427567554-901916","position":{"x":803.4044342041016,"y":60.608015060424805},"size":{"width":450,"height":300},"type":"Container","title":"RDD","description":"An RDD in Spark is simply an immutable distributed collection of objects. \nEach RDD is split into multiple partitions, which may be computed on different nodes of the cluster. \nRDDs can contain any type of Python, Java, or Scala objects, including user-defined classes.\nIn Spark all work is expressed as either creating new RDDs, transforming existing RDDs, or calling operations on RDDs to compute a result. \nUnder the hood, Spark automatically distributes the data contained in RDDs across your cluster and parallelizes the operations you perform on them.\n\nUsers create RDDs in two ways: \n1) by loading an external dataset, or \n2) by distributing a collection of objects (e.g., a list or set) in their driver program.\n\nOnce created, RDDs offer two types of operations: \n1) transformations and \n2) actions.\nTransformations construct a new RDD from a previous one.\nActions, on the other hand, compute a result based on an RDD, and either return it to the driver program or save it to an external storage system.\nReturn type: transformations return RDDs, whereas actions return some other data type.\n\nTo summarize, every Spark program and shell session will work as follows:\n1. Create some input RDDs from external data.\n2. Transform them to define new RDDs using transformations like filter().\n3. Ask Spark to persist() any intermediate RDDs that will need to be reused.\n4. Launch actions such as count() and first() to kick off a parallel computation, which is then optimized and executed by Spark.\n","children":[{"id":"node-1769427567554-769078","position":{"x":40.4796142578125,"y":34.61286926269531},"size":{"width":799.1734008789062,"height":507.9425354003906},"type":"CodeBox","content":"# READ\n#local collection\ndata = [1, 2, 3, 4, 5]\nrdd = sc.parallelize(data)\n\n# Read a single file, a directory, or a wildcard pattern\nrdd_text = sc.textFile(\"path/to/data.txt\")\n\n# You can also specify the minimum number of partitions\nrdd_partitioned = sc.textFile(\"path/to/data.txt\", minPartitions=4)\n\n# Useful for processing a directory of small logs\nrdd_whole = sc.wholeTextFiles(\"path/to/directory/\")\n# Result: [('file1.txt', 'content...'), ('file2.txt', 'content...')]\n\n#from json\nrdd_raw_json = sc.textFile(\"data.json\")\nrdd_parsed_json = rdd_raw_json.map(lambda line: json.loads(line))\n\n#from csv\nrdd_raw_csv = sc.textFile(\"data.csv\")\ndef parse_csv(line):\n    # StringIO allows the csv module to treat a string like a file\n    f = StringIO(line)\n    reader = csv.reader(f)\n    return next(reader)\nrdd_csv = rdd_raw_csv.map(parse_csv)\n\n#binary files\nrdd_binary = sc.binaryFiles(\"path/to/images/\")\n\n\n#TRANSFORMATIONS\n#map: Transforms each element into exactly one new element.\n#flatMap: Similar to map, but each input item can be mapped to 0 or more output items.\n\n# Example: Converting text to uppercase\nupper_rdd = rdd.map(lambda line: line.upper())\n\n# Example: Splitting lines into individual words\nwords_rdd = rdd.flatMap(lambda line: line.split(\" \"))\n\n# Keep only lines that contain the word 'Spark'\nfiltered_rdd = rdd.filter(lambda line: \"Spark\" in line)\n\n#ACTIONS\n#Action\t                Description\n#collect()\t        Returns the entire RDD as a list to the driver. (Warning: Only use for small datasets!)\n#count()\t        Returns the number of elements in the RDD.\n#take(n)\t        Returns the first $n$ elements of the RDD.\n#saveAsTextFile(path)\tWrites the RDD to a text file (or directory of files).\n\n# Triggering the execution\nprint(f\"Total words: {word_counts.count()}\")\n\n# Taking a sample of the results\nprint(word_counts.take(5))\n\n# Saving the results back to disk\nword_counts.saveAsTextFile(\"output/word_count_results\")\n\n"}]},{"id":"node-1769427567554-108400","position":{"x":816.8458251953125,"y":426.38631439208984},"size":{"width":450,"height":300},"type":"Container","title":"Key-value RDD","description":"Key-Value Pair RDDs (often called \"Pair RDDs\") are a specialized type of RDD where each element is a tuple consisting of two items: (Key, Value).\n","children":[{"id":"node-1769427567554-450010","position":{"x":200.54798889160156,"y":51.24694061279297},"size":{"width":647.838623046875,"height":438.4605407714844},"type":"CodeBox","content":"# Starting with a regular RDD of strings\nlines = sc.textFile(\"data.csv\")\n\n# Transforming into a Pair RDD: (User_ID, Transaction_Amount)\n# Input line example: \"user123,50.00\"\npair_rdd = lines.map(lambda x: (x.split(\",\")[0], float(x.split(\",\")[1])))\n\n# EFFICIENT: Sum of amounts per user\ntotal_per_user = pair_rdd.reduceByKey(lambda a, b: a + b)\n\n# LESS EFFICIENT: Group all transactions into a list per user\nall_transactions = pair_rdd.groupByKey()\n\n# RDD 1: (User_ID, Name)\nnames = sc.parallelize([(\"1\", \"Alice\"), (\"2\", \"Bob\")])\n\n# RDD 2: (User_ID, Purchase_Amount)\npurchases = sc.parallelize([(\"1\", 25.00), (\"1\", 15.00), (\"2\", 40.00)])\n\n# Join them together\n# Result: (\"1\", (\"Alice\", 25.00)), (\"1\", (\"Alice\", 15.00)), (\"2\", (\"Bob\", 40.00))\njoined_rdd = names.join(purchases)\n"}]},{"id":"node-1769432363165","position":{"x":-439.1964416503906,"y":-241.69775009155273},"size":{"width":1174.3028259277344,"height":980.4383087158203},"type":"CodeBox","content":"from pyspark import SparkConf, SparkContext\n\ndef has_word(line, word):\n    return word.lower() in line.lower()\n\nconf = SparkConf().setAppName(\"SparkByExamples.com\").setMaster(\"local[*]\")\nsc = SparkContext(conf = conf)\n\n#creating RDD from collection\ntokens = sc.parallelize([\"hello world\", \"spark by example\"])\nprint(tokens.count())\n\n#creating RDD from file\nlines = sc.textFile(\"data/book.txt\")\nlines.cache()\nprint(lines.count())\nprint(lines.first())\n\n# print line count having sundog (case-insensitive)\nfiltered_lines = lines.filter(lambda line: has_word(line, \"sundog\"))\nprint(filtered_lines.count())\nsc.stop()\n\n\n\n\n\n"}]},{"id":"node-1769427567554-342605","position":{"x":142.7048797607422,"y":194.82431030273438},"size":{"width":450,"height":300},"type":"Container","title":"Spark SQL"},{"id":"node-1769427567554-735041","position":{"x":675.9729614257812,"y":203.9559326171875},"size":{"width":450,"height":300},"type":"Container","title":"Spark Streaming"},{"id":"node-1769427567554-43279","position":{"x":1208.7093505859375,"y":197.23184204101562},"size":{"width":450,"height":300},"type":"Container","title":"MLlib"},{"id":"node-1769427567554-317441","position":{"x":1736.946044921875,"y":194.9818115234375},"size":{"width":450,"height":300},"type":"Container","title":"GraphX"},{"id":"node-1769427567554-723497","type":"Path","startElement":"node-1769427567554-342605","endElement":"node-1769427567554-620463"},{"id":"node-1769427567554-605140","type":"Path","startElement":"node-1769427567554-735041","endElement":"node-1769427567554-620463"},{"id":"node-1769427567554-853263","type":"Path","startElement":"node-1769427567554-43279","endElement":"node-1769427567554-620463"},{"id":"node-1769427567554-390445","type":"Path","startElement":"node-1769427567554-317441","endElement":"node-1769427567554-620463"}]}]},{"id":"node-1769451521963-436559","position":{"x":-2325.563131932874,"y":160.8556437617351},"size":{"width":851.019775390625,"height":753.9024047851562},"type":"Container","title":"Input Source","pan":{"x":-295.2836254623662,"y":-520.9859140114825,"scale":0.11526790915926297},"children":[{"id":"node-1769451521963-220966","position":{"x":419.6459655761719,"y":-37.47015380859375},"size":{"width":450,"height":300},"type":"Container","title":"Files","children":[{"id":"node-1769451521963-105140","position":{"x":23.702608108520508,"y":30.882164001464844},"size":{"width":809.8612823486328,"height":456.13658142089844},"type":"TextBox","title":"File formats","content":"Format\tStructure\tSchema Support\tPros\t                                         Cons\nParquet\tColumnar\tHigh\t                Extremely fast for queries\tSlow for \"write-heavy\" operations\nAvro\tRow-based\tExcellent\t             Handles changing fields well\tNot ideal for selective column reads\nJSON\tHierarchical\tInferred\t                     Easy to read; flexible\tHigh storage overhead; slow parsing\nCSV\t          Flat\t        Minimal\t                 Universal compatibility\tNo built-in data types; slow","color":"transparent"}]},{"id":"node-1769451521963-24884","position":{"x":973.4745788574219,"y":-50.64794731140137},"size":{"width":450,"height":300},"type":"Container","title":"Streams"},{"id":"node-1769451521963-427374","position":{"x":1537.9629669189453,"y":-52.605079650878906},"size":{"width":450,"height":300},"type":"Container","title":"Databases"},{"id":"node-1769451521964-157703","position":{"x":-150.94640350341797,"y":-38.054832458496094},"size":{"width":450,"height":300},"type":"Container","title":"Collections"}]},{"id":"node-1769451530018-624828","position":{"x":2947.3847440436884,"y":276.5956339961101},"size":{"width":1062.9115600585938,"height":688.2094116210938},"type":"Container","title":"Output","pan":{"x":-927.6990073224333,"y":-619.9880427550455,"scale":0.12709329141645007},"children":[{"id":"node-1769451530018-10157","position":{"x":338.58861541748047,"y":-175.84991073608398},"size":{"width":450,"height":300},"type":"Container","title":"Data warehouse Lakehouse","color":"transparent"},{"id":"node-1769451530018-583375","position":{"x":-722.421875,"y":-168.98964881896973},"size":{"width":450,"height":300},"type":"Container","title":"Files","color":"transparent"},{"id":"node-1769451530018-837784","position":{"x":872.5235748291016,"y":-182.4047393798828},"size":{"width":450,"height":300},"type":"Container","title":"Streams","color":"transparent"},{"id":"node-1769451530018-569505","position":{"x":-202.26616668701172,"y":-170.52281951904297},"size":{"width":450,"height":300},"type":"Container","title":"Databases","color":"transparent"},{"id":"node-1769451530018-463867","position":{"x":-207.32096195220947,"y":323.7085876464844},"size":{"width":450,"height":300},"type":"Container","title":"Console","color":"transparent"},{"id":"node-1769451530018-492893","position":{"x":340.82635498046875,"y":319.0240478515625},"size":{"width":450,"height":300},"type":"Container","title":"Memory","color":"transparent"}]},{"id":"path-1769510596142","type":"Path","startElement":"node-1769451521963-436559","endElement":"node-1768994224387-937691"},{"id":"path-1769510599574","type":"Path","startElement":"node-1768994224387-937691","endElement":"node-1769451530018-624828"}]}]}}