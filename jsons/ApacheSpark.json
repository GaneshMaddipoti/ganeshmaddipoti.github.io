{"id":null,"name":"ApacheSpark","data":{"id":"root","size":{"width":1728,"height":996},"pan":{"x":284.3184091682208,"y":137.22524106788023,"scale":0.6882011576632963},"children":[{"id":"line-1768775896776","type":"Line","lineCoordinates":[{"x":491.6481628417969,"y":288.8475341796875},{"x":768.648193359375,"y":288.8475341796875}]},{"id":"node-1768775941880","position":{"x":486.5516662597656,"y":322.22052001953125},"size":{"width":1267.6734619140625,"height":432.505615234375},"type":"TextBox","title":"About","content":"Apache Spark is a unified analytics engine for large-scale data processing. (Cluster computing platform)\nAt its core, Spark is a “computational engine” that is responsible for scheduling, distributing, and monitoring applications consisting of many computational tasks across many worker machines, or a computing cluster. \nIf Hadoop MapReduce was the \"first generation\" of big data processing, Spark is the \"second generation\"—built for speed, ease of use, and versatility.\n\nFeatures\nEasy to use: we can develop applications, using a high-level API that let us focus on the content of our computation\nSpeed: It is up to 100x faster than Hadoop MapReduce because it processes data in-memory (RAM) rather than writing to the disk after every step.\nLazy Evaluation: Spark doesn't execute your code immediately. It builds a plan (called a DAG or Directed Acyclic Graph) and only runs it when you finally ask for a result. This allows it to optimize the entire process first.\nAs general engine: letting you combine multiple types of computations (e.g., SQL queries, text process‐ing, and machine learning) that might previously have required different engines.\n\nPolyglot: You can write Spark code in Python (PySpark), Scala, Java, or R."},{"id":"line-1768776347399","type":"Line","lineCoordinates":[{"x":1033.8216552734375,"y":1239.6767578125},{"x":1033.8216552734375,"y":1239.6767578125}]},{"id":"line-1768776355611","type":"Line","lineCoordinates":[{"x":1124.3660888671875,"y":1114.84912109375},{"x":1124.3660888671875,"y":1114.84912109375}]},{"id":"node-1768776778906","position":{"x":488.4365539550781,"y":238.79788208007812},"size":{"width":213.70458984375,"height":47.15156555175781},"type":"TitleBox","content":"Apache Spark\n"},{"id":"node-1768777090579","position":{"x":486.7514343261719,"y":826.7827758789062},"size":{"width":1250.7160034179688,"height":750.920166015625},"type":"Container","title":"Architecture","pan":{"x":38,"y":258},"children":[{"id":"node-1768777131082","position":{"x":119.09141540527344,"y":773.4568481445312},"size":{"width":450,"height":300},"type":"Container","title":"Client","description":"\n","children":[{"id":"node-1768804676825","position":{"x":175.23153686523438,"y":126.59458923339844},"size":{"width":450,"height":300},"type":"Container","title":"Spark Driver","description":"Driver Program: The \"brain\" of your application. It runs your main() function and converts your code into tasks.\nAt a high level, every Spark application consists of a driver program that launches various parallel operations on a cluster. \nThe driver program contains your application’s main function and defines distributed datasets on the cluster, then applies operations to them.\nDriver programs access Spark through a SparkContext object, which represents a connection to a computing cluster.\n\n\nResponsibilities\n1) Maintains the SparkContext/SparkSession: It is the entry point of the application.\n2) Analyzes Code: it converts your high-level code (SQL, Python, Scala) into a logical plan.\n3) Creates the DAG: It builds a Directed Acyclic Graph (DAG) of all the transformations you want to perform.\n4) Schedules Tasks: It breaks the DAG into \"Stages\" and then into \"Tasks,\" which it sends to the Executors (worker nodes) to be processed.\n","children":[{"id":"node-1768807029041","position":{"x":88.16413879394531,"y":13.675613403320312},"size":{"width":785.7688293457031,"height":523.0265502929688},"type":"CodeBox","content":"#Application \nfrom pyspark.sql import SparkSession\n\n# 1. The Driver starts and creates the SparkSession\nspark = SparkSession.builder.appName(\"LogAnalysis\").get_home()\n# 2. The Driver creates a plan to read the data (Lazy Evaluation)\nlogs = spark.read.text(\"s3://my-bucket/logs/*.txt\")\n# 3. The Driver plans the transformation (Filtering)\naws_logs = logs.filter(logs.value.contains(\"AWS\"))\n# 4. The Driver triggers an 'Action' (.count())\n# This is where the Driver converts the DAG into actual tasks\nresult = aws_logs.count()\n# 5. The Driver receives the final number from the executors and prints it\nprint(f\"Total AWS entries: {result}\")\n\nspark.stop()\n\n\n\n\n\n"}]}]},{"id":"node-1768777133224","position":{"x":958.575439453125,"y":771.3358764648438},"size":{"width":450,"height":300},"type":"Container","title":"Cluster Manager","pan":{"x":-711.5286562709382,"y":-474.3524375139589,"scale":0.19371024450000007},"children":[{"id":"node-1768812826317","position":{"x":15.268295288085938,"y":-87.52663993835449},"size":{"width":450,"height":300},"type":"Container","title":"Apache YARN"},{"id":"node-1768812830350","position":{"x":-467.78299820423126,"y":-88.37072658538818},"size":{"width":450,"height":300},"type":"Container","title":"Standalone"},{"id":"node-1768812830842","position":{"x":965.3104095458984,"y":-98.9773280620575},"size":{"width":450,"height":300},"type":"Container","title":"Mesos"},{"id":"node-1768812869117","position":{"x":493.5273609161377,"y":-91.90109252929688},"size":{"width":450,"height":300},"type":"Container","title":"Kubernetes"}]},{"id":"node-1768777134492","position":{"x":1420.81787109375,"y":269.3124084472656},"size":{"width":450,"height":300},"type":"Container","title":"Worker Node","children":[{"id":"node-1768807279312","position":{"x":89.96749877929688,"y":62.73999786376953},"size":{"width":718.8135986328125,"height":423.2466735839844},"type":"Container","title":"Application Master","children":[{"id":"node-1768807335618-610","position":{"x":145.12289428710938,"y":88.24881744384766},"size":{"width":1090.2388916015625,"height":657.4938354492188},"type":"Container","title":"Spark Driver","description":"Driver Program: The \"brain\" of your application. It runs your main() function and converts your code into tasks.\nAt a high level, every Spark application consists of a driver program that launches various parallel operations on a cluster. \nThe driver program contains your application’s main function and defines distributed datasets on the cluster, then applies operations to them.\nDriver programs access Spark through a SparkContext object, which represents a connection to a computing cluster.\n\n\nResponsibilities\n1) Maintains the SparkContext/SparkSession: It is the entry point of the application.\n2) Analyzes Code: it converts your high-level code (SQL, Python, Scala) into a logical plan.\n3) Creates the DAG: It builds a Directed Acyclic Graph (DAG) of all the transformations you want to perform.\n4) Schedules Tasks: It breaks the DAG into \"Stages\" and then into \"Tasks,\" which it sends to the Executors (worker nodes) to be processed.\n","children":[{"id":"node-1768807335618-635","position":{"x":153.72389221191406,"y":68.72859191894531},"size":{"width":1840.5922546386719,"height":1064.607666015625},"type":"CodeBox","content":"#Application \nfrom pyspark.sql import SparkSession\n\n# 1. The Driver starts and creates the SparkSession\nspark = SparkSession.builder.appName(\"LogAnalysis\").get_home()\n# 2. The Driver creates a plan to read the data (Lazy Evaluation)\nlogs = spark.read.text(\"s3://my-bucket/logs/*.txt\")\n# 3. The Driver plans the transformation (Filtering)\naws_logs = logs.filter(logs.value.contains(\"AWS\"))\n# 4. The Driver triggers an 'Action' (.count())\n# This is where the Driver converts the DAG into actual tasks\nresult = aws_logs.count()\n# 5. The Driver receives the final number from the executors and prints it\nprint(f\"Total AWS entries: {result}\")\n\nspark.stop()\n\n\n\n\n\n"}]}]}]},{"id":"node-1768777135433","position":{"x":2021.1544189453125,"y":660.1998291015625},"size":{"width":450,"height":300},"type":"Container","title":"Worker Node","children":[{"id":"node-1768807431943","position":{"x":229.38766479492188,"y":158.36561584472656},"size":{"width":450,"height":300},"type":"Container","title":"Executor","children":[{"id":"node-1768807455812","position":{"x":181.1865997314453,"y":118.54624938964844},"size":{"width":450,"height":300},"type":"Container","title":"JVM"}]}]},{"id":"node-1768777136358","position":{"x":2056.712646484375,"y":1227.8822021484375},"size":{"width":450,"height":300},"type":"Container","title":"Worker Node"},{"id":"path-1768777149657","type":"Path","startElement":"node-1768777131082","endElement":"node-1768777133224"},{"id":"path-1768777155157","type":"Path","startElement":"node-1768777133224","endElement":"node-1768777135433"},{"id":"path-1768777156727","type":"Path","startElement":"node-1768777133224","endElement":"node-1768777136358"},{"id":"path-1768804144452","type":"Path","startElement":"node-1768777133224","endElement":"node-1768777134492"},{"id":"line-1768804173997","position":{"x":66.03778076171875,"y":6.603759765625},"type":"Line","lineCoordinates":[{"x":690.7655029296875,"y":292.893798828125},{"x":703.627197265625,"y":1635.557373046875}]},{"id":"node-1768804210603","position":{"x":369.8416290283203,"y":291.53314208984375},"size":{"width":364.4654235839844,"height":100},"type":"TitleBox","content":"Client Mode"},{"id":"node-1768807491334","position":{"x":780.3194580078125,"y":287.870849609375},"size":{"width":360.314697265625,"height":95.837158203125},"type":"TitleBox","content":"Cluster Mode"},{"id":"node-1768812685213","position":{"x":12.390594482421875,"y":1472.1219482421875},"size":{"width":567.1286010742188,"height":205.166748046875},"type":"NoteBox","content":"In Local Mode:\nThe entire Spark application (Driver and Executors) runs inside a single JVM on your local machine."},{"id":"node-1768813018684","position":{"x":13.357620239257812,"y":1282.0543212890625},"size":{"width":561.3067474365234,"height":164.771728515625},"type":"NoteBox","content":"In enterprise setups, we don't submit jobs from our own laptop. Instead, we log into a Gateway (Edge) Node."},{"id":"node-1768813294847-706","position":{"x":839.8312606811523,"y":1474.4766540527344},"size":{"width":602.2201538085938,"height":213.4378662109375},"type":"NoteBox","content":"In cluster mode:\nThe Cluster Manager starts the AM first, and then the AM launches the Spark Driver inside its own process"}]},{"id":"node-1768808101135","position":{"x":488.829345703125,"y":1640.876708984375},"size":{"width":1241.772216796875,"height":701.3006591796875},"type":"Container","title":"Ecosystem","children":[{"id":"node-1768808131873","position":{"x":868.9764404296875,"y":814.120849609375},"size":{"width":450,"height":300},"type":"Container","title":"Spark Core","description":"Spark Core contains the basic functionality of Spark, \nincluding components for task scheduling, memory management, fault recovery, interacting with storage systems, and more. \nSpark Core is also home to the API that defines resilient distributed datasets (RDDs), which are Spark’s main programming abstraction. \nRDDs represent a collection of items distributed across many compute nodes that can be manipulated in parallel. \n","pan":{"x":-490.8382115794601,"y":-327.2254743863067,"scale":0.23914845000000007},"children":[{"id":"node-1768808263983","position":{"x":-295.98136711120605,"y":12.057342529296875},"size":{"width":450,"height":300},"type":"Container","title":"Partitioning","description":"In the Spark world, the partition is the smallest unit of data processing. \nSpark cannot process a dataset \"as a whole.\" \nInstead, it breaks the dataset down into these manageable chunks so it can apply the \"Divide and Conquer\" strategy across a cluster of computers.\n\n1. The Relationship: Partition - Task - Core\nTo understand Spark processing, you have to look at how these three elements link together:\nPartition: A physical slice of your data (e.g., 10,000 rows of a CSV).\nTask: A single unit of work (e.g., \"convert these strings to uppercase\").\nCore (Slot): A CPU thread on a Worker node.\nThe Rule: One Task processes exactly one Partition on one Core at a time.\n\n2. Parallelism vs. Partitioning\nIf you have a 1GB file and only 1 partition, \nSpark can only use 1 CPU core to process it—even if your cluster has 100 computers! The other 99 computers will sit idle.\nUnder-partitioning: Too few partitions = Low parallelism (slow).\nOver-partitioning: Too many partitions = High overhead (Spark spends more time managing tasks than actually processing data).\n\n3. How Spark decides the number of Partitions?\nSpark sets the initial number of partitions based on where the data comes from:\nFrom HDFS/S3: Usually based on the file \"blocks\" (defaulting to 128MB per partition).\nFrom Local Collections: If you use sc.parallelize(data), it usually defaults to the total number of cores in your cluster.\nFrom Shuffles: When you do a join or groupBy, Spark defaults to 200 partitions (this is a configurable setting called spark.sql.shuffle.partitions).\n\n4. What happens during processing?\nWhen you write code like rdd.map(lambda x: x + 1), Spark doesn't send the data to the code; it sends the code to the data.\nThe Driver sends the \"Map\" function to every Executor.\nEach Executor applies that function to the partitions it has in its local memory.\nBecause each partition is independent, they can all be processed at the exact same time without talking to each other.\n"},{"id":"node-1768808271274","position":{"x":230.3988494873047,"y":11.772336959838867},"size":{"width":450,"height":300},"type":"Container","title":"RDD","description":"An RDD in Spark is simply an immutable distributed collection of objects. \nEach RDD is split into multiple partitions, which may be computed on different nodes of the cluster. \nRDDs can contain any type of Python, Java, or Scala objects, including user-defined classes.\nIn Spark all work is expressed as either creating new RDDs, transforming existing RDDs, or calling operations on RDDs to compute a result. \nUnder the hood, Spark automatically distributes the data contained in RDDs across your cluster and parallelizes the operations you perform on them.\n\nUsers create RDDs in two ways: \n1) by loading an external dataset, or \n2) by distributing a collection of objects (e.g., a list or set) in their driver program.\n\nOnce created, RDDs offer two types of operations: \n1) transformations and \n2) actions.\nTransformations construct a new RDD from a previous one.\nActions, on the other hand, compute a result based on an RDD, and either return it to the driver program or save it to an external storage system.\nReturn type: transformations return RDDs, whereas actions return some other data type.\n\nTo summarize, every Spark program and shell session will work as follows:\n1. Create some input RDDs from external data.\n2. Transform them to define new RDDs using transformations like filter().\n3. Ask Spark to persist() any intermediate RDDs that will need to be reused.\n4. Launch actions such as count() and first() to kick off a parallel computation, which is then optimized and executed by Spark.\n","children":[{"id":"node-1768808365766","position":{"x":50.690948486328125,"y":19.30596160888672},"size":{"width":799.1734008789062,"height":507.9425354003906},"type":"CodeBox","content":"# READ\n#local collection\ndata = [1, 2, 3, 4, 5]\nrdd = sc.parallelize(data)\n\n# Read a single file, a directory, or a wildcard pattern\nrdd_text = sc.textFile(\"path/to/data.txt\")\n\n# You can also specify the minimum number of partitions\nrdd_partitioned = sc.textFile(\"path/to/data.txt\", minPartitions=4)\n\n# Useful for processing a directory of small logs\nrdd_whole = sc.wholeTextFiles(\"path/to/directory/\")\n# Result: [('file1.txt', 'content...'), ('file2.txt', 'content...')]\n\n#from json\nrdd_raw_json = sc.textFile(\"data.json\")\nrdd_parsed_json = rdd_raw_json.map(lambda line: json.loads(line))\n\n#from csv\nrdd_raw_csv = sc.textFile(\"data.csv\")\ndef parse_csv(line):\n    # StringIO allows the csv module to treat a string like a file\n    f = StringIO(line)\n    reader = csv.reader(f)\n    return next(reader)\nrdd_csv = rdd_raw_csv.map(parse_csv)\n\n#binary files\nrdd_binary = sc.binaryFiles(\"path/to/images/\")\n\n\n#TRANSFORMATIONS\n#map: Transforms each element into exactly one new element.\n#flatMap: Similar to map, but each input item can be mapped to 0 or more output items.\n\n# Example: Converting text to uppercase\nupper_rdd = rdd.map(lambda line: line.upper())\n\n# Example: Splitting lines into individual words\nwords_rdd = rdd.flatMap(lambda line: line.split(\" \"))\n\n# Keep only lines that contain the word 'Spark'\nfiltered_rdd = rdd.filter(lambda line: \"Spark\" in line)\n\n#ACTIONS\n#Action\t                Description\n#collect()\t        Returns the entire RDD as a list to the driver. (Warning: Only use for small datasets!)\n#count()\t        Returns the number of elements in the RDD.\n#take(n)\t        Returns the first $n$ elements of the RDD.\n#saveAsTextFile(path)\tWrites the RDD to a text file (or directory of files).\n\n# Triggering the execution\nprint(f\"Total words: {word_counts.count()}\")\n\n# Taking a sample of the results\nprint(word_counts.take(5))\n\n# Saving the results back to disk\nword_counts.saveAsTextFile(\"output/word_count_results\")\n\n"}]},{"id":"node-1768808272024","position":{"x":781.0330200195312,"y":9.655006408691406},"size":{"width":450,"height":300},"type":"Container","title":"Key-value RDD","description":"Key-Value Pair RDDs (often called \"Pair RDDs\") are a specialized type of RDD where each element is a tuple consisting of two items: (Key, Value).\n","children":[{"id":"node-1768808421458","position":{"x":200.54798889160156,"y":51.24694061279297},"size":{"width":647.838623046875,"height":438.4605407714844},"type":"CodeBox","content":"# Starting with a regular RDD of strings\nlines = sc.textFile(\"data.csv\")\n\n# Transforming into a Pair RDD: (User_ID, Transaction_Amount)\n# Input line example: \"user123,50.00\"\npair_rdd = lines.map(lambda x: (x.split(\",\")[0], float(x.split(\",\")[1])))\n\n# EFFICIENT: Sum of amounts per user\ntotal_per_user = pair_rdd.reduceByKey(lambda a, b: a + b)\n\n# LESS EFFICIENT: Group all transactions into a list per user\nall_transactions = pair_rdd.groupByKey()\n\n# RDD 1: (User_ID, Name)\nnames = sc.parallelize([(\"1\", \"Alice\"), (\"2\", \"Bob\")])\n\n# RDD 2: (User_ID, Purchase_Amount)\npurchases = sc.parallelize([(\"1\", 25.00), (\"1\", 15.00), (\"2\", 40.00)])\n\n# Join them together\n# Result: (\"1\", (\"Alice\", 25.00)), (\"1\", (\"Alice\", 15.00)), (\"2\", (\"Bob\", 40.00))\njoined_rdd = names.join(purchases)\n"}]}]},{"id":"node-1768808133469","position":{"x":142.7048797607422,"y":194.82431030273438},"size":{"width":450,"height":300},"type":"Container","title":"Spark SQL"},{"id":"node-1768808134544","position":{"x":675.9729614257812,"y":203.9559326171875},"size":{"width":450,"height":300},"type":"Container","title":"Spark Streaming"},{"id":"node-1768808135915","position":{"x":1208.7093505859375,"y":197.23184204101562},"size":{"width":450,"height":300},"type":"Container","title":"MLlib"},{"id":"node-1768808136948","position":{"x":1736.946044921875,"y":194.9818115234375},"size":{"width":450,"height":300},"type":"Container","title":"GraphX"},{"id":"path-1768813243601","type":"Path","startElement":"node-1768808133469","endElement":"node-1768808131873"},{"id":"path-1768813245151","type":"Path","startElement":"node-1768808134544","endElement":"node-1768808131873"},{"id":"path-1768813246759","type":"Path","startElement":"node-1768808135915","endElement":"node-1768808131873"},{"id":"path-1768813248500","type":"Path","startElement":"node-1768808136948","endElement":"node-1768808131873"}]},{"id":"node-1768813516378","position":{"x":1836.416259765625,"y":821.9235229492188},"size":{"width":1279.94970703125,"height":766.26708984375},"type":"Container","title":"Application Process","pan":{"x":-300.2351164641202,"y":-179.74166304976848,"scale":0.405},"children":[{"id":"node-1768813890842","position":{"x":-188.64120864868164,"y":146.11041259765625},"size":{"width":450,"height":300},"type":"Container","title":"Setup","description":"Submission: You submit the application (via spark-submit or a notebook).\n\nDriver Initialization: The Spark Driver starts and initializes the SparkSession and SparkContext.\n\nResource Request: The Driver contacts the Cluster Manager (YARN/K8s).\n\nExecutor Launch: The Cluster Manager starts Executors on worker nodes. The Executors register themselves with the Driver so the Driver knows how much \"horsepower\" it has available."},{"id":"node-1768813891951","position":{"x":423.27001953125,"y":139.7908935546875},"size":{"width":450,"height":300},"type":"Container","title":"Logical Plan","description":"Lineage Building: Spark builds a logical graph of every operation you call.\n\nThe Action: The process is \"stuck\" in this phase until an Action (e.g., .save(), .collect()) is called.\n\nDAG Creation: Upon the Action, the DAG Scheduler takes the lineage and transforms it into a formal Directed Acyclic Graph."},{"id":"node-1768813893021","position":{"x":1036.1981201171875,"y":141.15859985351562},"size":{"width":450,"height":300},"type":"Container","title":"Physical Plan","description":"The DAG Scheduler is the high-level orchestrator that breaks the graph into physical execution units.\n\nStage Splitting: The DAG Scheduler looks for Shuffle dependencies (Wide transformations).\n\nEverything that can be done in one go without moving data across the network is grouped into a Single Stage.\n\nEvery time data needs to be redistributed (e.g., a join or reduceBy), a New Stage is created.\n\nTask Set Creation: Within each stage, the scheduler looks at the number of data partitions. It creates a Task Set—one Task for every partition of data."},{"id":"node-1768813894012","position":{"x":1645.2275390625,"y":141.09066772460938},"size":{"width":450,"height":300},"type":"Container","title":"Execution","description":"Now, the Task Scheduler takes over from the DAG Scheduler.\n\nTask Submission: The Task Scheduler looks at the available Executors and sends the Tasks to them.\n\nData Locality: The Task Scheduler tries to send the code to the node where the data already lives (e.g., \"moving the computation to the data\").\n\nExecution: The Executor receives the Task (bytecode) and runs it inside its JVM.\n\nNote for Python: If it's a PySpark job, the Executor's JVM launches a Python Worker process to handle the Python logic.\n\nShuffle Map: If there are multiple stages, the first stage writes its output to local disk (Shuffle files), and the next stage fetches that data."},{"id":"node-1768813895037","position":{"x":2247.9852294921875,"y":130.32131958007812},"size":{"width":450,"height":300},"type":"Container","title":"Conclusion","description":"Result Aggregation: Once the final stage is complete, the results are sent back to the Driver (if you called .collect()) or written to the destination (like S3 or HDFS).\n\nCleanup: Once the main() method finishes or spark.stop() is called, the Driver informs the Cluster Manager to shut down the Executors and release the resources."},{"id":"path-1768814122158","type":"Path","startElement":"node-1768813890842","endElement":"node-1768813891951"},{"id":"path-1768814123709","type":"Path","startElement":"node-1768813891951","endElement":"node-1768813893021"},{"id":"path-1768814124983","type":"Path","startElement":"node-1768813893021","endElement":"node-1768813894012"},{"id":"path-1768814126291","type":"Path","startElement":"node-1768813894012","endElement":"node-1768813895037"},{"id":"node-1768814282010","position":{"x":522.310302734375,"y":693.4407958984375},"size":{"width":1149.5657958984375,"height":663.64697265625},"type":"Container","title":"DAG","children":[{"id":"node-1768814312876","position":{"x":159.35037231445312,"y":350.3323059082031},"size":{"width":522.7493286132812,"height":102.443359375},"type":"TextBox","title":"Stage","color":"#ffb30033"},{"id":"node-1768814315752","position":{"x":901.9952392578125,"y":348.0083465576172},"size":{"width":449.4486083984375,"height":97.55667114257812},"type":"TextBox","title":"Stage","color":"#ffb30033"},{"id":"node-1768814317451","position":{"x":1543.5596923828125,"y":345.4484405517578},"size":{"width":537.16748046875,"height":97.35511779785156},"type":"TextBox","title":"Stage","color":"#ffb30033"},{"id":"node-1768814321243","position":{"x":294.0989685058594,"y":574.1750793457031},"size":{"width":285.7440185546875,"height":100},"type":"TextBox","title":"Task","color":"#8d6e6333"},{"id":"node-1768814322068","position":{"x":316.3985290527344,"y":775.1614990234375},"size":{"width":271.0838623046875,"height":100},"type":"TextBox","title":"Task","color":"#8d6e6333"},{"id":"node-1768814323084","position":{"x":331.2255859375,"y":1014.6758422851562},"size":{"width":268.6405029296875,"height":104.88665771484375},"type":"TextBox","title":"Task","color":"#8d6e6333"},{"id":"node-1768814324784","position":{"x":1017.4268798828125,"y":627.0839538574219},"size":{"width":273.5272216796875,"height":97.556640625},"type":"TextBox","title":"Task","color":"#8d6e6333"},{"id":"node-1768814325589","position":{"x":1020.260009765625,"y":931.5952758789062},"size":{"width":280.857177734375,"height":100},"type":"TextBox","title":"Task","color":"#8d6e6333"},{"id":"node-1768814326572","position":{"x":1689.9285888671875,"y":684.327880859375},"size":{"width":210,"height":100},"type":"TextBox","title":"Task","color":"#8d6e6333"},{"id":"path-1768814464855","type":"Path","startElement":"node-1768814321243","endElement":"node-1768814322068"},{"id":"path-1768814466601","type":"Path","startElement":"node-1768814322068","endElement":"node-1768814323084"},{"id":"path-1768814468918","type":"Path","startElement":"node-1768814323084","endElement":"node-1768814324784"},{"id":"path-1768814471934","type":"Path","startElement":"node-1768814324784","endElement":"node-1768814325589"},{"id":"path-1768814473793","type":"Path","startElement":"node-1768814325589","endElement":"node-1768814326572"},{"id":"node-1768814529282","position":{"x":129.27359008789062,"y":98.64378356933594},"size":{"width":1971.6580200195312,"height":114.66012573242188},"type":"TextBox","title":"Job","color":"#66bb6a33"}]}]}]}}