{"id":null,"name":"ApacheSpark","data":{"id":"root","size":{"width":1728,"height":997},"pan":{"x":-2915.818216154341,"y":-441.25774900139186,"scale":0.36288681333182193},"children":[{"id":"line-1768776347399","type":"Line","lineCoordinates":[{"x":1033.8216552734375,"y":1239.6767578125},{"x":1033.8216552734375,"y":1239.6767578125}]},{"id":"line-1768776355611","type":"Line","lineCoordinates":[{"x":1124.3660888671875,"y":1114.84912109375},{"x":1124.3660888671875,"y":1114.84912109375}]},{"id":"node-1768994213987","position":{"x":-2098.3423483371735,"y":-351.63134574890137},"size":{"width":2783.765670776367,"height":2346.3248901367188},"type":"Container","title":"Apache Spark","description":"History:\nThe need for large-scale computing was initially driven by Google, whose proprietary systems like the \n1) Google File System (GFS), (distributed)\n2) MapReduce (MR), (parallel programming pardigm based on functional programming)\n3) Bigtable (structured data across GFS)\nwere designed to handle data at a scale traditional relational databases could not. \nThese ideas provided a blueprint for Apache Hadoop, including the Hadoop Distributed File System (HDFS)\nMost of the work Google did was proprietary, but the ideas expressed in the afore‐mentioned three papers spurred innovative ideas elsewhere in the open source community—especially at Yahoo!, which was dealing with similar big data challenges of scale for its search engine.\nHowever, Hadoop MapReduce had several shortcomings:\n• It was operationally complex and difficult to administer.\n• It was inefficient for iterative or interactive jobs because it required writing intermediate results to local disk between every stage, resulting in heavy I/O overhead.\n• It was not conducive to combining different workloads like machine learning, streaming, or interactive SQL.\nTo address these issues, researchers at UC Berkeley started the Spark project in 2009. Spark was designed to be highly fault-tolerant and parallel, but it enhanced the system by supporting in-memory storage for intermediate results. By 2013, the project was donated to the Apache Software Foundation, and its creators formed Databricks.\n\n\nWhat is :\nApache Spark is a unified analytics engine for large-scale data processing. (Cluster computing platform)\nAt its core, Spark is a “computational engine” that is responsible for scheduling, distributing, and monitoring applications consisting of many computational tasks across many worker machines, or a computing cluster. \nIf Hadoop MapReduce was the \"first generation\" of big data processing, Spark is the \"second generation\"—built for speed, ease of use, and versatility.\n\nFeatures\nEasy to use: we can develop applications, using a high-level API that let us focus on the content of our computation\nSpeed: It is up to 100x faster than Hadoop MapReduce because it processes data in-memory (RAM) rather than writing to the disk after every step.\nModularity: Spark SQL, Spark Structured Streaming, Spark MLlib, and GraphX, combining all the workloads running under one engine. \nExtensibility: Spark decouples storage and compute, and using connectors we can connect to multiple sources. \nLazy Evaluation: Spark doesn't execute your code immediately. It builds a plan (called a DAG or Directed Acyclic Graph) and only runs it when you finally ask for a result. This allows it to optimize the entire process first.\nAs general engine: letting you combine multiple types of computations (e.g., SQL queries, text process‐ing, and machine learning) that might previously have required different engines.\nPolyglot: You can write Spark code in Python (PySpark), Scala, Java, or R.\n\n\nThe Unified Stack\nIn 2016, Spark was recognized for replacing disparate processing engines with a unified stack of components. This stack includes:\n• Spark SQL: For working with structured data, supporting ANSI SQL:2003-compliant queries.\n• Spark MLlib: A library of common machine learning algorithms built on high-level DataFrame APIs.\n• Spark Structured Streaming: A model that treats real-time data streams as continuously appended tables.\n• GraphX: A library for manipulating graphs and performing graph-parallel computations.\n\nDistributed Execution Architecture\nSpark operates as a distributed engine where components work collaboratively on a cluster of machines.\n• Spark Driver: Responsible for instantiating a SparkSession, orchestrating operations, and transforming code into DAG computations to be scheduled as tasks.\n• SparkSession: Introduced in Spark 2.0, it serves as a single unified entry point for all Spark functionality.\n• Cluster Manager: Manages and allocates resources for the cluster. Spark supports Standalone, YARN, Mesos, and Kubernetes managers.\n• Executors: Run on worker nodes, communicate with the driver, and are responsible for executing tasks.\n• Partitions: Spark treats data distributed across a cluster as a logical data abstraction in memory. Partitioning allows for efficient parallelism, where each executor's core is assigned a single partition of data to work on.\n\nThe Developer Experience\nSpark is favored by data engineers, data scientists, and machine learning engineers because it allows them to build complex applications using a single engine. \nData scientists use it to cleanse data and build model pipelines, while data engineers use it to build scalable data pipelines and perform ETL tasks. \nCommon use cases include parallel processing of large datasets, ad hoc interactive queries, and analyzing social network graphs\n\n\n\n\n\n","pan":{"x":-4859.316721799682,"y":-2888.8025078881137,"scale":0.17222228012774893},"children":[{"id":"node-1768994224387-937691","position":{"x":537.699951171875,"y":-2184.303512573242},"size":{"width":4260.523254394531,"height":2464.3912506103516},"type":"Container","title":"Deployment","color":"transparent","pan":{"x":-1514.3578173995975,"y":-51.44178759242393,"scale":1.0607067283562999},"children":[{"id":"node-1768994224388-298441","position":{"x":119.09141540527344,"y":773.4568481445312},"size":{"width":384.52947998046875,"height":288.747314453125},"type":"Container","title":"Client","description":"\n","children":[{"id":"node-1769514065885","position":{"x":12.449310302734375,"y":15.751785278320312},"size":{"width":738.5188598632812,"height":523.5322875976562},"type":"Container","title":"Spark application","children":[{"id":"node-1769514073966-720199","position":{"x":32.9354248046875,"y":26.578487396240234},"size":{"width":1389.4005126953125,"height":941.2673950195312},"type":"Container","title":"Spark driver","description":"Driver Program: The \"brain\" of your application. It runs your main() function and converts your code into tasks.\nAt a high level, every Spark application consists of a driver program that launches various parallel operations on a cluster. \nThe driver program contains your application’s main function and defines distributed datasets on the cluster, then applies operations to them.\nDriver programs access Spark through a SparkContext object, which represents a connection to a computing cluster.\n\n\nResponsibilities\n1) Maintains the SparkContext/SparkSession: It is the entry point of the application.\n2) Analyzes Code: it converts your high-level code (SQL, Python, Scala) into a logical plan.\n3) Creates the DAG: It builds a Directed Acyclic Graph (DAG) of all the transformations you want to perform.\n4) Schedules Tasks: It breaks the DAG into \"Stages\" and then into \"Tasks,\" which it sends to the Executors (worker nodes) to be processed.\n","children":[{"id":"node-1769514073967-321965","position":{"x":107.41201782226562,"y":85.80938720703125},"size":{"width":2528.6052856445312,"height":1542.2193908691406},"type":"Container","title":"SparkSession","description":"The Script Starts: The SparkSession is instantiated.\nThe Negotiation: The session's internal SparkContext reaches out to the Cluster Manager (like YARN).\nThe AM Launch: YARN allocates a container for the ApplicationMaster.\nThe Command Center: The SparkSession (the Driver) then uses that AM to start Executors across the rest of the cluster.","children":[{"id":"node-1769514073967-964486","position":{"x":51.0009765625,"y":88.201904296875},"size":{"width":4832.708160400391,"height":2648.916748046875},"type":"CodeBox","content":"#Application \nfrom pyspark.sql import SparkSession\n\n# 1. The Driver starts and creates the SparkSession\nspark = SparkSession.builder.appName(\"LogAnalysis\").get_home()\n# 2. The Driver creates a plan to read the data (Lazy Evaluation)\nlogs = spark.read.text(\"s3://my-bucket/logs/*.txt\")\n# 3. The Driver plans the transformation (Filtering)\naws_logs = logs.filter(logs.value.contains(\"AWS\"))\n# 4. The Driver triggers an 'Action' (.count())\n# This is where the Driver converts the DAG into actual tasks\nresult = aws_logs.count()\n# 5. The Driver receives the final number from the executors and prints it\nprint(f\"Total AWS entries: {result}\")\n\nspark.stop()\n\n\n\n\n\n\n"}]}]}]}]},{"id":"node-1768994224388-583630","position":{"x":958.575439453125,"y":771.3358764648438},"size":{"width":450,"height":300},"type":"Container","title":"Cluster Manager","description":"The cluster manager is responsible for managing and allocating resources for the\ncluster of nodes on which your Spark application runs. ","pan":{"x":-559.715758741801,"y":-411.0674357336627,"scale":0.21095045626050013},"children":[{"id":"node-1768994224388-895464","position":{"x":15.268295288085938,"y":-87.52663993835449},"size":{"width":450,"height":300},"type":"Container","title":"Apache YARN"},{"id":"node-1768994224388-474429","position":{"x":-467.78299820423126,"y":-88.37072658538818},"size":{"width":450,"height":300},"type":"Container","title":"Standalone"},{"id":"node-1768994224388-830986","position":{"x":965.3104095458984,"y":-98.9773280620575},"size":{"width":450,"height":300},"type":"Container","title":"Mesos"},{"id":"node-1768994224388-433762","position":{"x":493.5273609161377,"y":-91.90109252929688},"size":{"width":450,"height":300},"type":"Container","title":"Kubernetes"}]},{"id":"node-1768994224388-889554","position":{"x":1420.81787109375,"y":269.3124084472656},"size":{"width":450,"height":300},"type":"Container","title":"Worker Node","children":[{"id":"node-1768994224388-5378","position":{"x":89.96749877929688,"y":62.73999786376953},"size":{"width":718.8135986328125,"height":423.2466735839844},"type":"Container","title":"Application Master","children":[{"id":"node-1768994224388-993284","position":{"x":145.12289428710938,"y":88.24881744384766},"size":{"width":1090.2388916015625,"height":657.4938354492188},"type":"Container","title":"Spark Driver","description":"Driver Program: The \"brain\" of your application. It runs your main() function and converts your code into tasks.\nAt a high level, every Spark application consists of a driver program that launches various parallel operations on a cluster. \nThe driver program contains your application’s main function and defines distributed datasets on the cluster, then applies operations to them.\nDriver programs access Spark through a SparkContext object, which represents a connection to a computing cluster.\n\n\nResponsibilities\n1) Maintains the SparkContext/SparkSession: It is the entry point of the application.\n2) Analyzes Code: it converts your high-level code (SQL, Python, Scala) into a logical plan.\n3) Creates the DAG: It builds a Directed Acyclic Graph (DAG) of all the transformations you want to perform.\n4) Schedules Tasks: It breaks the DAG into \"Stages\" and then into \"Tasks,\" which it sends to the Executors (worker nodes) to be processed.\n","children":[{"id":"node-1768994224388-815872","position":{"x":153.72389221191406,"y":68.72859191894531},"size":{"width":1840.5922546386719,"height":1064.607666015625},"type":"CodeBox","content":"#Application \nfrom pyspark.sql import SparkSession\n\n# 1. The Driver starts and creates the SparkSession\nspark = SparkSession.builder.appName(\"LogAnalysis\").get_home()\n# 2. The Driver creates a plan to read the data (Lazy Evaluation)\nlogs = spark.read.text(\"s3://my-bucket/logs/*.txt\")\n# 3. The Driver plans the transformation (Filtering)\naws_logs = logs.filter(logs.value.contains(\"AWS\"))\n# 4. The Driver triggers an 'Action' (.count())\n# This is where the Driver converts the DAG into actual tasks\nresult = aws_logs.count()\n# 5. The Driver receives the final number from the executors and prints it\nprint(f\"Total AWS entries: {result}\")\n\nspark.stop()\n\n\n\n\n\n"}]}]}]},{"id":"node-1768994224388-836652","position":{"x":1909.6837158203125,"y":657.9249267578125},"size":{"width":450,"height":300},"type":"Container","title":"Worker Node","children":[{"id":"node-1768994224388-201049","position":{"x":229.38766479492188,"y":158.36561584472656},"size":{"width":450,"height":300},"type":"Container","title":"Executor","children":[{"id":"node-1768994224388-518941","position":{"x":181.1865997314453,"y":118.54624938964844},"size":{"width":450,"height":300},"type":"Container","title":"JVM"}]}]},{"id":"node-1768994224388-291383","position":{"x":1871.712890625,"y":1227.8822021484375},"size":{"width":450,"height":300},"type":"Container","title":"Worker Node"},{"id":"node-1768994224388-33816","type":"Path","startElement":"node-1768994224388-298441","endElement":"node-1768994224388-583630"},{"id":"node-1768994224388-898071","type":"Path","startElement":"node-1768994224388-583630","endElement":"node-1768994224388-836652"},{"id":"node-1768994224388-592941","type":"Path","startElement":"node-1768994224388-583630","endElement":"node-1768994224388-291383"},{"id":"node-1768994224388-957355","type":"Path","startElement":"node-1768994224388-583630","endElement":"node-1768994224388-889554"},{"id":"node-1768994224388-720171","position":{"x":66.03778076171875,"y":6.603759765625},"type":"Line","lineCoordinates":[{"x":690.7655029296875,"y":292.893798828125},{"x":703.627197265625,"y":1635.557373046875}]},{"id":"node-1768994224388-939734","position":{"x":369.8416290283203,"y":291.53314208984375},"size":{"width":364.4654235839844,"height":100},"type":"TitleBox","content":"Client Mode"},{"id":"node-1768994224388-377469","position":{"x":780.3194580078125,"y":287.870849609375},"size":{"width":360.314697265625,"height":95.837158203125},"type":"TitleBox","content":"Cluster Mode"},{"id":"node-1768994224388-656731","position":{"x":-1620.8863830566406,"y":91.47418212890625},"size":{"width":1616.4977569580078,"height":1906.1011962890625},"type":"NoteBox","content":"Modes:\nIn enterprise setups, we don't submit jobs from our own laptop. Instead, we log into a Gateway (Edge) Node.\nLocal Mode:\nThe entire Spark application (Driver and Executors) runs inside a single JVM on your local machine.\nClient Mode:\nThe driver will run in client machine(edge node), slim AM will run in cluster\nCluster:\nThe Cluster Manager starts the AM first, and then the AM launches the Spark Driver inside its own process"}]},{"id":"node-1769451521963-436559","position":{"x":1902.176240320544,"y":9085.44914962111},"size":{"width":1431.349365234375,"height":1059.3499145507812},"type":"Container","title":"Storage","description":"The choice of a storage solution is critical because it determines the end-to-end robustness and performance of a data pipeline. While expressing processing logic is a major step, the storage layer must ensure that the resulting data can be effectively queried for insights. \n• Scalability and Performance: The solution must be able to scale to the required data volume and provide the throughput and latency demanded by the workload.\n• Transaction Support: ACID transactions are essential for complex workloads where concurrent read and write operations occur, ensuring the quality and consistency of the end results.\n• Support for Diverse Data Formats: An optimal system must handle unstructured data (like raw logs), semi-structured data (like JSON), and structured tabular data.\n• Support for Diverse Workloads: The storage must be versatile enough to support a range of business needs, including SQL for BI analytics, batch ETL processing, real-time streaming, and Machine Learning/AI.\n• Openness: Using open data formats and standardized APIs is crucial to allow the data to be accessed by a variety of tools, which helps organizations avoid vendor lock-in and make the best business decisions","pan":{"x":-204.43327712038308,"y":-579.5589389224399,"scale":0.6346709922521182},"children":[{"id":"node-1769451521963-220966","position":{"x":-152.00000762939453,"y":-108.11656475067139},"size":{"width":450,"height":300},"type":"Container","title":"Files","description":"The sources detail several built-in data sources:\n• Parquet: This is the default and preferred data source for Spark. It is an open-source columnar format that offers I/O optimizations like compression, saving storage space and allowing quick access to columns.\n• JSON: A popular, easy-to-read format supported in both single-line and multiline modes.\n• CSV: A common text format where each field is delimited by a comma. It is popular among data and business analysts because it can be generated by spreadsheets.\n• Avro: Introduced as a built-in source in Spark 2.4, it is often used by Apache Kafka for message serialization.\n• ORC: An optimized columnar format that supports a vectorized reader, which reads blocks of rows at once to reduce CPU usage.\n• Images and Binary Files: Spark supports image files for machine learning applications. Spark 3.0 specifically added support for binary files, converting each file into a single DataFrame record containing raw content and metadata."},{"id":"node-1769451521963-24884","position":{"x":-146.11669921875,"y":272.5011463165283},"size":{"width":450,"height":300},"type":"Container","title":"Streams"},{"id":"node-1769451521963-427374","position":{"x":699.4516143798828,"y":-467.78081035614014},"size":{"width":1132.036865234375,"height":742.4023056030273},"type":"Container","title":"Lakehouse","description":"A lakehouse is a new paradigm in data storage that combines the best elements of data lakes and data warehouses for Online Analytical Processing (OLAP) workloads. This design provides data management features, traditionally found in databases, directly on the low-cost, scalable storage used for data lakes. It is considered the next generation of storage solutions, restored to meet the diverse requirements of modern data use cases.\n\nKey Features of a Lakehouse\n• Transaction Support: Like traditional databases, lakehouses provide ACID guarantees even in the presence of concurrent read and write workloads.\n• Schema Enforcement and Governance: The system prevents data with incorrect schemas from being inserted into a table while allowing the schema to be explicitly evolved to accommodate changing data.\n• Support for Diverse Data Types: Lakehouses can store structured, semi-structured, and unstructured data in open formats with standardized APIs for broad accessibility.\n• Support for Diverse Workloads: By breaking down isolated data silos, a single repository can support SQL, streaming, and machine learning.\n• Support for Upserts and Deletes: They allow data to be concurrently updated or deleted with transactional guarantees, which is essential for change-data-capture (CDC) and slowly changing dimension (SCD) operations.\n• Data Governance: These systems provide robust tools for auditing data changes and reasoning about data integrity for policy compliance.\n\nCore Architecture\nThe underlying architecture of a lakehouse typically involves storing large volumes of data in structured file formats on scalable filesystems and maintaining a transaction log. This log records a timeline of atomic changes, allowing the system to define table versions and provide snapshot isolation guarantees between readers and writers. In the Spark ecosystem, three major open-source projects—Apache Hudi, Apache Iceberg, and Delta Lake—are used to build these storage environments","children":[{"id":"node-1769682611951","position":{"x":158.88319396972656,"y":100.14640808105469},"size":{"width":1951.5828247070312,"height":1194.8221435546875},"type":"Container","title":"Delta Lake","description":"Delta Lake is an open-source storage format hosted by the Linux Foundation that brings ACID transactions and database-like management features to data lakes. \nIt is designed to integrate tightly with Apache Spark, supporting both batch and streaming workloads.\n\nCore Characteristics and Integration\n• Ease of Migration: Users can migrate existing Spark workloads from Parquet to Delta Lake by simply changing the format specification from format(\"parquet\") to format(\"delta\") in their read and write operations.\n• Unified Batch and Streaming: Unlike traditional formats (JSON, Parquet, ORC) where streaming writes can overwrite existing data, Delta Lake allows concurrent batch and streaming writes to the same table. It also enables multiple streaming queries to append data to a single table simultaneously with exactly-once guarantees.\n• Serializable Isolation: Delta Lake provides ACID guarantees even when multiple concurrent writers are performing SQL, batch, or streaming operations.\n\n\nConfiguration\nTo use Delta Lake with Spark 3.0 in an interactive shell, you must include the specific delta-core package: pyspark --packages io.delta:delta-core_2.12:0.7.0.\n\n\nData Quality: Schema Enforcement and Evolution\n• Schema Enforcement: To prevent data corruption, Delta Lake records the schema as table-level metadata. It blocks any write operation that does not match the table's schema, throwing an AnalysisException.\n• Schema Evolution: When legitimate changes to data structure occur, users can explicitly evolve the schema by adding the .option(\"mergeSchema\", \"true\") to their write command. In Spark 3.0, this can also be handled via ALTER TABLE SQL commands.\n\n\nDML Operations: Update, Delete, and Merge\nDelta Lake supports standard Data Manipulation Language (DML) operations, which are traditionally difficult to perform on raw data lakes:\n• UPDATE and DELETE: Users can fix errors or comply with data protection regulations (like GDPR) by deleting or updating records directly using Python, Scala, or SQL APIs.\n• Upserting with MERGE: The merge() operation allows for complex data pipelines such as Change Data Capture (CDC). It supports an extended syntax for conditional updates, deletes, and \"star syntax\" to update all columns at once.\n\n\nAuditing and Time Travel\n• Operation History: Every change to a table is recorded as a commit in a transaction log. Users can audit these changes by running deltaTable.history(), which shows versions, timestamps, and specific operation parameters.\n• Time Travel: This feature allows developers to query previous snapshots of a table using versionAsOf or timestampAsOf. This is highly valuable for reproducing machine learning experiments, auditing data changes, or rolling back accidental deletes or incorrect updates.\nThe name \"Delta Lake\" comes from an analogy to how streams flow into a sea to create deltas, which are nutrient-rich areas where valuable crops grow. Similarly, Delta Lake serves as the destination where data streams accumulate and become valuable through refinement."}]},{"id":"node-1769451521964-157703","position":{"x":-174.82684326171875,"y":-473.13165497779846},"size":{"width":450,"height":300},"type":"Container","title":"Collections"},{"id":"node-1769603578618","position":{"x":1455.1729125976562,"y":550.6155395507812},"size":{"width":450,"height":300},"type":"Container","title":"Hive Metastore"},{"id":"node-1769680560811","position":{"x":-133.2573699951172,"y":651.0698547363281},"size":{"width":450,"height":300},"type":"Container","title":"Database","description":"Spark Writing to Databases\nApache Spark can read from and write to a wide variety of databases by leveraging its extensive ecosystem of connectors.\n• JDBC Data Source: For traditional databases that provide JDBC drivers, such as PostgreSQL and MySQL, Spark uses its built-in JDBC data source along with the appropriate driver jars to facilitate data transfer.\n• Dedicated Connectors: For more modern or specialized databases like Azure Cosmos DB and Snowflake, Spark utilizes dedicated connectors invoked through specific format names.\n• OLAP Focus: While Spark can connect to these systems, it is primarily designed for Online Analytical Processing (OLAP) workloads—complex queries involving aggregates and joins—rather than the high-concurrency, low-latency Online Transaction Processing (OLTP) workloads typical of many databases.\n\nLimitations of Databases\n• Expensive to Scale Out: Unlike Spark, which scales horizontally on commodity hardware, most databases are not designed for scaling out. Solutions that can handle massive data volumes are typically proprietary and require specialized hardware, making them extremely expensive to acquire and maintain.\n• Limited Support for Non-SQL Analytics: Databases are optimized for SQL processing and often store data in complex, proprietary formats. This makes it difficult and inefficient for non-SQL tools, such as machine learning and deep learning systems, to access the data.\n• Lack of Extensibility: Traditional databases cannot be easily extended to perform complex, non-SQL based analytics like machine learning.\n• Data Growth: As data volumes have exploded from gigabytes to terabytes and petabytes, the performance capabilities of a single-machine database architecture have been far outpaced.\n\n\nThese constraints often result in databases becoming isolated data silos, which led to the adoption of data lakes to decouple storage from compute, and eventually lakehouses to restore transactional guarantees to scalable storage"},{"id":"node-1769680807283","position":{"x":502.61785888671875,"y":653.6690673828125},"size":{"width":450,"height":300},"type":"Container","title":"Data Lake","description":"Introduction to Data Lakes\nIn contrast to traditional databases, a data lake is a distributed storage solution that runs on commodity hardware and scales out horizontally. Its core architecture decouples the storage system from the compute system, allowing each to scale independently based on workload requirements. In a data lake, data is stored as files in open formats, which allows any processing engine to read or write the data using standard APIs. Organizations typically build data lakes by choosing a storage system (HDFS or cloud object stores), a file format (structured, semi-structured, or unstructured), and one or more processing engines (batch, streaming, or ML).\n\nHow Spark Connects to Data Lakes\nApache Spark is highly compatible with data lakes because it provides the necessary tools to handle diverse workloads, including batch, ETL, SQL, streaming, and machine learning. Spark integrates with data lakes in the following ways:\n• Diverse File Formats: Spark has built-in support for reading and writing unstructured, semi-structured, and structured formats like Parquet, ORC, and JSON.\n• Diverse Filesystems: Spark can access any storage system that supports Hadoop’s FileSystem APIs. This includes on-premises HDFS as well as cloud-based object stores like AWS S3, Azure Data Lake Storage, and Google Cloud Storage.\n• Cloud Configuration: For cloud storage, Spark must be configured to ensure secure access and to handle specific file operation semantics, such as eventual consistency in S3, which can otherwise lead to inconsistent results.\n\n\nLimitations of Data Lakes\nThe most significant drawback of data lakes is the lack of transactional (ACID) guarantees. These architectural sacrifices lead to several practical challenges:\n• Atomicity and Isolation: Data is written as many files in a distributed manner; if an operation fails, there is no mechanism to roll back the files already written, which can leave behind corrupted data.\n• Consistency and Data Quality: It is difficult to ensure that new data follows the same schema as existing data, often leading to accidental data corruption when incorrectly formatted files are written into a table.\n• Concurrent Workloads: Because it is hard to provide isolation across files, concurrent reading and writing can cause readers to get an inconsistent view of the data.\n• Inefficient Workarounds: To manage these issues, developers often resort to expensive tricks, such as rewriting entire subdirectories just to update a few records or staggering job schedules to avoid simultaneous access.\n\n"}]},{"id":"node-1769517247275","position":{"x":5291.653076171875,"y":-2212.5059509277344},"size":{"width":3542.773193359375,"height":2455.9905281066895},"type":"Container","title":"Spark UI","description":"Spark includes a graphical user interface that you can use to inspect or monitor Spark applications in their various stages of decomposition—that is jobs, stages, and tasks.\n\nDepending on how Spark is deployed, the driver launches a web UI, running by\ndefault on port 4040, where you can view metrics and details such as:\n• A list of scheduler stages and tasks\n• A summary of RDD sizes and memory usage\n• Information about the environment\n• Information about the running executors\n• All the Spark SQL queries"},{"id":"node-1769603202874-870967","position":{"x":-3641.0796000675364,"y":578.1758519558689},"size":{"width":12502.79135131836,"height":7891.2879638671875},"type":"Container","title":"Process","pan":{"x":-1395.810647557786,"y":-1776.7347784170636,"scale":2.438799685503126},"children":[{"id":"node-1769603202874-895150","position":{"x":887.5931396484375,"y":1017.7151489257812},"size":{"width":455.5670166015625,"height":248.8387451171875},"type":"Container","title":"Spark Core","description":"Spark Core contains the basic functionality of Spark, \nincluding components for task scheduling, memory management, fault recovery, interacting with storage systems, and more. \nSpark Core is also home to the API that defines resilient distributed datasets (RDDs), which are Spark’s main programming abstraction. \nRDDs represent a collection of items distributed across many compute nodes that can be manipulated in parallel. \n","pan":{"x":-490.8382115794601,"y":-327.2254743863067,"scale":0.23914845000000007},"children":[{"id":"node-1769770377513","position":{"x":166.92007446289062,"y":-157.89246559143066},"size":{"width":546.7041320800781,"height":659.7731857299805},"type":"TextBox","title":"RDD","content":"The RDD is the most basic abstraction in Spark. There are three vital characteristics associated with an RDD:\n• Dependencies\n• Partitions (with some locality information)\n• Compute function: Partition => Iterator[T]\n\nAn RDD in Spark is simply an immutable distributed collection of objects. \nEach RDD is split into multiple partitions, which may be computed on different nodes of the cluster. \nRDDs can contain any type of Python, Java, or Scala objects, including user-defined classes.\nIn Spark all work is expressed as either creating new RDDs, transforming existing RDDs, or calling operations on RDDs to compute a result. \nUnder the hood, Spark automatically distributes the data contained in RDDs across your cluster and parallelizes the operations you perform on them.\n\nUsers create RDDs in two ways: \n1) by loading an external dataset, or \n2) by distributing a collection of objects (e.g., a list or set) in their driver program.\n\nOnce created, RDDs offer two types of operations: \n1) transformations and \n2) actions.\nTransformations construct a new RDD from a previous one.\nActions, on the other hand, compute a result based on an RDD, and either return it to the driver program or save it to an external storage system.\nReturn type: transformations return RDDs, whereas actions return some other data type.\n\nTo summarize, every Spark program and shell session will work as follows:\n1. Create some input RDDs from external data.\n2. Transform them to define new RDDs using transformations like filter().\n3. Ask Spark to persist() any intermediate RDDs that will need to be reused.\n4. Launch actions such as count() and first() to kick off a parallel computation, which is then optimized and executed by Spark."},{"id":"node-1769770384241","position":{"x":-393.67050981521606,"y":-156.9798288345337},"size":{"width":492.32282638549805,"height":666.0073471069336},"type":"TextBox","title":"Partitioning"},{"id":"node-1769770384921","position":{"x":791.8010864257812,"y":-157.9226942062378},"size":{"width":547.2512817382812,"height":652.2035598754883},"type":"TextBox","title":"Key-value RDD","content":"Key-Value Pair RDDs (often called \"Pair RDDs\") are a specialized type of RDD where each element is a tuple consisting of two items: (Key, Value)."}]},{"id":"node-1769603202874-405322","position":{"x":865.4361114501953,"y":523.4250793457031},"size":{"width":450,"height":300},"type":"Container","title":"Spark SQL","description":"However, the original RDD model has limitations because the compute function is opaque to Spark. \nBecause Spark cannot inspect the computation or the specific data types, it has no way to optimize the expression or rearrange it into an efficient query plan\n\nStructuring Spark\nTo address these issues, Spark 2.x introduced schemes to express computations using common patterns found in data analysis, such as filtering, selecting, and aggregating. \nThese operators are available via a domain-specific language (DSL) in Spark's supported languages (Java, Python, Scala, R, and SQL). \nThis structure allows data to be arranged in a tabular format and yields benefits like better performance, space efficiency, and expressivity. \nFor example, a query to average ages that requires complex lambda functions in the RDD API becomes much more expressive and simpler when using the DataFrame API.\n\nSpark SQL and the Underlying Engine\nThe Spark SQL engine is the substrate upon which these Structured APIs are built, allowing for optimized query plans and compact code generation.\n• Catalyst Optimizer: This component converts a query into an execution plan through four phases: Analysis, Logical Optimization, Physical Planning, and Code Generation.\n• Project Tungsten: This focuses on whole-stage code generation, which collapses the whole query into a single function to improve CPU efficiency and performance.\nIn summary, the high-level DataFrame and Dataset APIs are more expressive and intuitive than the low-level RDD API, allowing you to tell Spark what to do rather than how to do it\n\n\nTables in Spark hold data along with relevant metadata, such as the schema, table name, and physical location. All of this is stored in a central metastore; by default, Spark uses the Apache Hive metastore. Spark allows you to create two types of tables:\n• Managed Tables: Spark manages both the metadata and the data in the file store. If you drop a managed table, both the metadata and the actual data are deleted.\n• Unmanaged Tables: Spark only manages the metadata, while you manage the data yourself in an external source. Dropping an unmanaged table only removes the metadata, not the data.\nViews can also be created on top of existing tables. These can be session-scoped temporary views, which are visible only to a single SparkSession, or global temporary views, which are visible across all SparkSessions within a Spark application. Unlike tables, views do not actually hold data and disappear after the Spark application or session terminates.\n\n\n","pan":{"x":-294.02587011379444,"y":-250.24761529369096,"scale":0.17580460327901115},"children":[{"id":"node-1769603202874-772675","position":{"x":-113.40399169921875,"y":140.0596160888672},"size":{"width":999.2720336914062,"height":666.0823974609375},"type":"Container","title":"Catalyst Optimiser","description":"Spark Application Concepts\nUnderstanding how Spark transforms code into execution requires familiarity with several key terms:\n• Application: A user program built on Spark APIs, consisting of a driver and executors.\n• SparkSession: The unified entry point for programming Spark with its Structured APIs.\n• Job: A parallel computation spawned in response to a Spark action.\n• Stage: A division of a job based on operation boundaries; stages dictate data transfer among executors.\n• Task: The smallest unit of execution, which is sent to a Spark executor; each task maps to a single core and works on a single partition of data.\n\nTransformations, Actions, and Lazy Evaluation\nSpark operations are classified into two categories:\n• Transformations: These operations, such as select() or filter(), are immutable, meaning they return a new DataFrame without altering the original. They are evaluated lazily, meaning Spark records them as a lineage but does not execute them immediately.\n• Actions: Operations like show(), count(), or collect() trigger the actual execution of all recorded transformations.\nNarrow vs. Wide Dependencies\n• Narrow Transformations: Any transformation where a single output partition can be computed from a single input partition (e.g., filter()), requiring no exchange of data across the cluster.\n• Wide Transformations: Operations like groupBy() or orderBy() that require Spark to perform a shuffle, where data from multiple partitions is read, combined, and written to disk","color":"transparent","pan":{"x":-696.6431370263754,"y":-101.79280621682756,"scale":0.2651052570145665},"children":[{"id":"node-1769603202874-364068","position":{"x":-226.55312728881836,"y":143.19412231445312},"size":{"width":458.93631744384766,"height":336.3800048828125},"type":"Container","title":"Unresolved logical plan","description":"Submission: You submit the application (via spark-submit or a notebook).\n\nDriver Initialization: The Spark Driver starts and initializes the SparkSession and SparkContext.\n\nResource Request: The Driver contacts the Cluster Manager (YARN/K8s).\n\nExecutor Launch: The Cluster Manager starts Executors on worker nodes. The Executors register themselves with the Driver so the Driver knows how much \"horsepower\" it has available."},{"id":"node-1769603202874-323678","position":{"x":596.9351806640625,"y":155.15977478027344},"size":{"width":402.72259521484375,"height":336.98834228515625},"type":"Container","title":"Logical Plan","description":"Lineage Building: Spark builds a logical graph of every operation you call.\n\nThe Action: The process is \"stuck\" in this phase until an Action (e.g., .save(), .collect()) is called.\n\nDAG Creation: Upon the Action, the DAG Scheduler takes the lineage and transforms it into a formal Directed Acyclic Graph."},{"id":"node-1769603202874-350604","position":{"x":2055.0413818359375,"y":177.41152954101562},"size":{"width":425.3331298828125,"height":314.805908203125},"type":"Container","title":"Physical plans","description":"The DAG Scheduler is the high-level orchestrator that breaks the graph into physical execution units.\n\nStage Splitting: The DAG Scheduler looks for Shuffle dependencies (Wide transformations).\n\nEverything that can be done in one go without moving data across the network is grouped into a Single Stage.\n\nEvery time data needs to be redistributed (e.g., a join or reduceBy), a New Stage is created.\n\nTask Set Creation: Within each stage, the scheduler looks at the number of data partitions. It creates a Task Set—one Task for every partition of data."},{"id":"node-1769603202875-163736","position":{"x":2062.9361572265625,"y":816.796142578125},"size":{"width":450,"height":300},"type":"Container","title":"Cost model","description":"Now, the Task Scheduler takes over from the DAG Scheduler.\n\nTask Submission: The Task Scheduler looks at the available Executors and sends the Tasks to them.\n\nData Locality: The Task Scheduler tries to send the code to the node where the data already lives (e.g., \"moving the computation to the data\").\n\nExecution: The Executor receives the Task (bytecode) and runs it inside its JVM.\n\nNote for Python: If it's a PySpark job, the Executor's JVM launches a Python Worker process to handle the Python logic.\n\nShuffle Map: If there are multiple stages, the first stage writes its output to local disk (Shuffle files), and the next stage fetches that data."},{"id":"node-1769603202875-324822","position":{"x":2082.2591552734375,"y":1300.3270874023438},"size":{"width":450,"height":300},"type":"Container","title":"Selected Physical plan","description":"Result Aggregation: Once the final stage is complete, the results are sent back to the Driver (if you called .collect()) or written to the destination (like S3 or HDFS).\n\nCleanup: Once the main() method finishes or spark.stop() is called, the Driver informs the Cluster Manager to shut down the Executors and release the resources."},{"id":"node-1769603202875-218275","type":"Path","startElement":"node-1769603202874-364068","endElement":"node-1769603202874-323678"},{"id":"node-1769603202875-775384","type":"Path","startElement":"node-1769603202874-350604","endElement":"node-1769603202875-163736"},{"id":"node-1769603202875-879846","type":"Path","startElement":"node-1769603202875-163736","endElement":"node-1769603202875-324822"},{"id":"node-1769603202875-2388","position":{"x":-531.6144027709961,"y":1568.8668212890625},"size":{"width":736.8849868774414,"height":577.6143798828125},"type":"Container","title":"DAG","color":"transparent","pan":{"x":192.34930508730406,"y":-71.93022096163043,"scale":0.3968561569385292},"children":[{"id":"node-1769603202875-228447","position":{"x":159.35037231445312,"y":350.3323059082031},"size":{"width":522.7493286132812,"height":102.443359375},"type":"TextBox","title":"Stage","content":"Each job gets divided into smaller sets of tasks called stages that depend on each other.","color":"#ffb30033"},{"id":"node-1769603202875-674028","position":{"x":901.9952392578125,"y":348.0083465576172},"size":{"width":449.4486083984375,"height":97.55667114257812},"type":"TextBox","title":"Stage","color":"#ffb30033"},{"id":"node-1769603202875-463171","position":{"x":1543.5596923828125,"y":345.4484405517578},"size":{"width":537.16748046875,"height":97.35511779785156},"type":"TextBox","title":"Stage","color":"#ffb30033"},{"id":"node-1769603202875-685170","position":{"x":294.0989685058594,"y":574.1750793457031},"size":{"width":285.7440185546875,"height":100},"type":"TextBox","title":"Task","content":"A single unit of work or execution that will be sent to a Spark executor.","color":"#8d6e6333"},{"id":"node-1769603202875-951096","position":{"x":316.3985290527344,"y":775.1614990234375},"size":{"width":271.0838623046875,"height":100},"type":"TextBox","title":"Task","color":"#8d6e6333"},{"id":"node-1769603202875-17544","position":{"x":331.2255859375,"y":1014.6758422851562},"size":{"width":268.6405029296875,"height":104.88665771484375},"type":"TextBox","title":"Task","color":"#8d6e6333"},{"id":"node-1769603202875-4471","position":{"x":1017.4268798828125,"y":627.0839538574219},"size":{"width":273.5272216796875,"height":97.556640625},"type":"TextBox","title":"Task","color":"#8d6e6333"},{"id":"node-1769603202875-271039","position":{"x":1020.260009765625,"y":931.5952758789062},"size":{"width":280.857177734375,"height":100},"type":"TextBox","title":"Task","color":"#8d6e6333"},{"id":"node-1769603202875-968589","position":{"x":1689.9285888671875,"y":684.327880859375},"size":{"width":210,"height":100},"type":"TextBox","title":"Task","color":"#8d6e6333"},{"id":"node-1769603202875-44105","type":"Path","startElement":"node-1769603202875-685170","endElement":"node-1769603202875-951096"},{"id":"node-1769603202875-610236","type":"Path","startElement":"node-1769603202875-951096","endElement":"node-1769603202875-17544"},{"id":"node-1769603202875-590045","type":"Path","startElement":"node-1769603202875-17544","endElement":"node-1769603202875-4471"},{"id":"node-1769603202875-553807","type":"Path","startElement":"node-1769603202875-4471","endElement":"node-1769603202875-271039"},{"id":"node-1769603202875-997808","type":"Path","startElement":"node-1769603202875-271039","endElement":"node-1769603202875-968589"},{"id":"node-1769603202875-76851","position":{"x":129.27359008789062,"y":98.64378356933594},"size":{"width":1971.6580200195312,"height":114.66012573242188},"type":"TextBox","title":"Job","content":"A parallel computation consisting of multiple tasks that gets spawned in response to a Spark action (e.g., save(), collect()).","color":"#d4e15733"}]},{"id":"node-1769603202875-217121","position":{"x":319.58042907714844,"y":1555.470458984375},"size":{"width":747.2286376953125,"height":586.6494140625},"type":"Container","title":"transformations","color":"transparent","pan":{"x":171.92500647436663,"y":29.313739233529837,"scale":0.7033483926633795},"children":[{"id":"node-1769603202875-972403","position":{"x":185.98728942871094,"y":193.77567291259766},"size":{"width":470.3021240234375,"height":485.1553649902344},"type":"Container","title":"Narrow","description":"Narrow Transformations on RDDs (Resilient Distributed Datasets) are operations where each partition of the parent RDD is used by at most one partition of the child RDD. \nSince there is no need to shuffle data across the network between executors, these operations are highly efficient and fast.","color":"transparent","children":[{"id":"node-1769603202875-461893","position":{"x":39.36027526855469,"y":589.0945892333984},"size":{"width":846.5283203125,"height":308.635009765625},"type":"CodeBox","title":"Examples","content":"# 1. map(func): Applies a function to each element\nrdd_map = rdd.map(lambda x: (x, 1))\n\n# 2. filter(func): Returns elements that satisfy the condition\nrdd_filter = rdd.filter(lambda x: x > 10)\n\n# 3. flatMap(func): Similar to map, but flattens the result\nrdd_flatmap = rdd.flatMap(lambda line: line.split(\" \"))\n\n# 4. mapPartitions(func): Runs a function on each partition as a whole\ndef process_partition(iterator):\n    # Initialize a resource once here (e.g., a database connection)\n    return [x * 2 for x in iterator]\n\nrdd_partition = rdd.mapPartitions(process_partition)\n\n# 5. mapPartitionsWithIndex(func): Includes the partition index\nrdd_idx = rdd.mapPartitionsWithIndex(lambda idx, it: [f\"Part: {idx}, Val: {x}\" for x in it])\n\n# 6. mapValues(func): Applies function only to the value, keeping the key intact\nrdd_kv = sc.parallelize([(\"a\", 1), (\"b\", 2)])\nrdd_map_val = rdd_kv.mapValues(lambda v: v + 10)\n\n# 7. flatMapValues(func): Same as above, but allows returning multiple values\nrdd_flat_val = rdd_kv.flatMapValues(lambda v: range(1, v + 1))\n\n# 8. union(other): Combines two RDDs (does not remove duplicates)\n# intersection, subtract, cartesian also perform on 2 RDDs\nrdd_union = rdd1.union(rdd2)\n\n# 9. sample(withReplacement, fraction, seed): Takes a random subset\nrdd_sample = rdd.sample(False, 0.1, 42)\n\n# 10. glom(): Coalesces all elements within each partition into a list\n# Result: One list per partition\nrdd_glommed = rdd.glom()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","color":"transparent"},{"id":"node-1769603202875-895800","position":{"x":43.36832046508789,"y":66.76531219482422},"size":{"width":190.66616821289062,"height":89.02900695800781},"type":"NoteBox"},{"id":"node-1769603202875-118104","position":{"x":533.857177734375,"y":65.2459487915039},"size":{"width":180.12045288085938,"height":81.99851989746094},"type":"NoteBox"},{"id":"node-1769603202875-103970","position":{"x":49.16535186767578,"y":189.4945526123047},"size":{"width":183.6356964111328,"height":90.78662109375},"type":"NoteBox"},{"id":"node-1769603202875-999729","position":{"x":48.270965576171875,"y":316.4006042480469},"size":{"width":183.63568115234375,"height":87.27139282226562},"type":"NoteBox"},{"id":"node-1769603202875-918605","position":{"x":546.356689453125,"y":316.8954620361328},"size":{"width":178.36285400390625,"height":87.27139282226562},"type":"NoteBox"},{"id":"node-1769603202875-377259","position":{"x":540.28564453125,"y":190.0965805053711},"size":{"width":174.84759521484375,"height":89.02900695800781},"type":"NoteBox"},{"id":"node-1769603202875-392310","type":"Path","startElement":"node-1769603202875-895800","endElement":"node-1769603202875-118104"},{"id":"node-1769603202875-834862","type":"Path","startElement":"node-1769603202875-103970","endElement":"node-1769603202875-377259"},{"id":"node-1769603202875-679380","type":"Path","startElement":"node-1769603202875-999729","endElement":"node-1769603202875-918605"}]},{"id":"node-1769603202875-779354","position":{"x":737.3499755859375,"y":190.93630981445312},"size":{"width":467.8658447265625,"height":485.96746826171875},"type":"Container","title":"Wide","description":"Wide Transformations are operations that require data to be redistributed across the cluster. This process is known as a Shuffle. Unlike narrow transformations, a single partition in the \"parent\" RDD may contribute data to multiple partitions in the \"child\" RDD, necessitating network I/O and disk I/O, which makes them more \"expensive\" in terms of performance.","color":"transparent","children":[{"id":"node-1769603202875-776928","position":{"x":24.093727111816406,"y":583.6126861572266},"size":{"width":873.9658203125,"height":310.10980224609375},"type":"CodeBox","title":"Examples","content":"# 1. repartition(numPartitions): Reshuffles data to increase or decrease partitions\n# This always triggers a full shuffle to ensure data is balanced.\nrdd_repartitioned = rdd.repartition(10)\n\n# 2. coalesce(numPartitions, shuffle=True): \n# While coalesce is usually narrow (shuffle=False), setting shuffle=True \n# forces a wide transformation to redistribute data.\nrdd_coalesced_wide = rdd.coalesce(2, shuffle=True)\n\n# 3. groupByKey(): Groups all values for each key into a single sequence\n# Warning: This can be memory-intensive as it pulls all values into a list.\nrdd_grouped = rdd_kv.groupByKey()\n\n# 4. reduceByKey(func): Merges values for each key using an associative function\n# More efficient than groupByKey because it performs local stays/combining before shuffling.\nrdd_reduced = rdd_kv.reduceByKey(lambda a, b: a + b)\n\n# 5. aggregateByKey(zeroValue, seqOp, combOp): \n# Highly flexible aggregation allowing different input and output types.\nrdd_agg = rdd_kv.aggregateByKey(0, lambda acc, v: acc + v, lambda acc1, acc2: acc1 + acc2)\n\n# 6. join(other): Performs an Inner Join between two RDDs\nrdd_joined = rdd1.join(rdd2)\n\n# 7. leftOuterJoin(other) / rightOuterJoin(other): \n# Performs outer joins; keys present in one RDD but not the other result in None.\nrdd_left_join = rdd1.leftOuterJoin(rdd2)\n\n# 8. cogroup(other): Groups data from both RDDs sharing the same key\n# Returns (Key, (Iterable_from_RDD1, Iterable_from_RDD2))\nrdd_cogrouped = rdd1.cogroup(rdd2)\n\n# 9. sortByKey(ascending=True): Sorts the RDD by the keys\nrdd_sorted = rdd_kv.sortByKey()\n\n# 10. sortBy(func): Sorts based on a custom function\nrdd_sorted_custom = rdd.sortBy(lambda x: x[1])\n\n# 11. distinct(): Removes duplicate records across the entire RDD\n# Requires a shuffle to compare records that might be on different partitions.\nrdd_distinct = rdd.distinct()\n\n# 12. intersection(other): Returns elements common to both RDDs\nrdd_intersect = rdd1.intersection(rdd2)\n\n# 13. subtract(other): Returns elements in rdd1 that are NOT in rdd2\nrdd_diff = rdd1.subtract(rdd2)\n\n\n\n\n\n","color":"transparent"},{"id":"node-1769603202875-995897","position":{"x":115.5076675415039,"y":63.126739501953125},"size":{"width":178.36727905273438,"height":87.2787857055664},"type":"NoteBox"},{"id":"node-1769603202875-777077","position":{"x":121.01758575439453,"y":193.6598663330078},"size":{"width":178.36727905273438,"height":87.27879333496094},"type":"NoteBox"},{"id":"node-1769603202875-733243","position":{"x":126.43240356445312,"y":320.99928283691406},"size":{"width":180.1246337890625,"height":89.03616333007812},"type":"NoteBox"},{"id":"node-1769603202875-145305","position":{"x":572.1084594726562,"y":65.0386734008789},"size":{"width":183.639404296875,"height":82.00666809082031},"type":"NoteBox"},{"id":"node-1769603202875-510517","position":{"x":575.9323120117188,"y":188.61366271972656},"size":{"width":181.8819580078125,"height":85.52142333984375},"type":"NoteBox"},{"id":"node-1769603202875-412560","position":{"x":576.2415161132812,"y":315.7033996582031},"size":{"width":180.1246337890625,"height":85.52142333984375},"type":"NoteBox"},{"id":"node-1769603202875-487421","type":"Path","startElement":"node-1769603202875-995897","endElement":"node-1769603202875-510517"},{"id":"node-1769603202875-242237","type":"Path","startElement":"node-1769603202875-777077","endElement":"node-1769603202875-145305"},{"id":"node-1769603202875-814741","type":"Path","startElement":"node-1769603202875-733243","endElement":"node-1769603202875-510517"},{"id":"node-1769603202875-514194","type":"Path","startElement":"node-1769603202875-777077","endElement":"node-1769603202875-412560"},{"id":"node-1769603202875-269980","type":"Path","startElement":"node-1769603202875-733243","endElement":"node-1769603202875-412560"},{"id":"node-1769603202875-707810","type":"Path","startElement":"node-1769603202875-733243","endElement":"node-1769603202875-145305"},{"id":"node-1769603202875-851980","type":"Path","startElement":"node-1769603202875-995897","endElement":"node-1769603202875-412560"}]}]},{"id":"node-1769603994983","position":{"x":281.4161071777344,"y":442.6715393066406},"size":{"width":242.2058868408203,"height":131.324951171875},"type":"NoteBox","content":"Analysis"},{"id":"node-1769603997635","position":{"x":1002.779296875,"y":480.7586669921875},"size":{"width":356.42901611328125,"height":195.96441650390625},"type":"NoteBox","content":"Logical Optimization"},{"id":"node-1769603999496","position":{"x":1771.625244140625,"y":469.09661865234375},"size":{"width":317.4986572265625,"height":219.85614013671875},"type":"NoteBox","content":"Physical Planning"},{"id":"node-1769604084287","position":{"x":283.4914245605469,"y":71.7498664855957},"size":{"width":241.5773162841797,"height":131.142578125},"type":"NoteBox","content":"Catalog"},{"id":"node-1769604125675","position":{"x":1293.0606079101562,"y":155.6121826171875},"size":{"width":460.7640380859375,"height":336.6885070800781},"type":"Container","title":"Optimized logical plan"},{"id":"path-1769604154160","type":"Path","startElement":"node-1769603202874-323678","endElement":"node-1769604125675"},{"id":"path-1769604209144","type":"Path","startElement":"node-1769604125675","endElement":"node-1769603202874-350604"},{"id":"node-1769771524204-58000","position":{"x":1156.7887210363688,"y":1552.8744148525084},"size":{"width":794.5930480957031,"height":587.0516967773438},"type":"CodeBox","title":"Actions","content":"# 1. collect(): Returns the entire RDD as a list to the driver\n# Use only on small/filtered datasets.\ndata_list = rdd.collect()\n\n# 2. take(n): Returns the first n elements of the RDD\nfirst_five = rdd.take(5)\n\n# 3. first(): Returns the first element of the RDD (similar to take(1))\nfirst_row = rdd.first()\n\n# 4. top(n): Returns the top n elements based on default or custom ordering\ntop_three = rdd.top(3)\n\n# 5. takeSample(withReplacement, num, seed): Returns a fixed-size random sample\nsample_data = rdd.takeSample(False, 10, 123)\n\n# 6. count(): Returns the total number of elements in the RDD\ntotal_rows = rdd.count()\n\n# 7. countByKey(): Returns a dictionary of (key, count) pairs\n# Only available on RDDs of (Key, Value) pairs.\nkey_counts = rdd_kv.countByKey()\n\n# 8. countByValue(): Returns a dictionary of (value, count) pairs\nval_counts = rdd.countByValue()\n\n# 9. reduce(func): Reduces the elements of the RDD using a commutative function\n# Example: Summing all numbers\ntotal_sum = rdd.reduce(lambda a, b: a + b)\n\n# 10. fold(zeroValue, op): Similar to reduce but with a starting 'zero' value\n# Useful for providing an initial state for calculations.\nsum_with_base = rdd.fold(0, lambda acc, v: acc + v)\n\n# 11. aggregate(zeroValue, seqOp, combOp): \n# Complex reduction allowing different return types (e.g., calculating average).\n# (Sum, Count) = rdd.aggregate((0, 0), \n#                              lambda acc, v: (acc[0] + v, acc[1] + 1), \n#                              lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1]))\n\n# 12. saveAsTextFile(path): Saves the RDD as text files (one per partition)\nrdd.saveAsTextFile(\"hdfs:///data/output/results_txt\")\n\n# 13. saveAsPickleFile(path): Saves the RDD using Python's pickle serialization\nrdd.saveAsPickleFile(\"s3://my-bucket/output/data.pkl\")\n\n# 14. foreach(func): Runs a function (like a print or DB write) on each element\n# Note: Since this runs on Executors, print() won't show on the Driver console.\nrdd.foreach(lambda x: print(x))\n\n\n\n\n\n\n","color":"transparent"}]},{"id":"node-1769603202875-362691","position":{"x":1251.7396240234375,"y":161.08135986328125},"size":{"width":855.0147705078125,"height":642.56640625},"type":"Container","title":"Project Tungsten","pan":{"x":-159.1235769129687,"y":-232.23084272517832,"scale":0.7132080275573428},"children":[{"id":"node-1769604529613","position":{"x":-171.00670623779297,"y":-51.51927185058594},"size":{"width":450,"height":300},"type":"Container","title":"Selected Physical plan"},{"id":"node-1769604530293","position":{"x":628.1767883300781,"y":-44.142906188964844},"size":{"width":450,"height":300},"type":"Container","title":"RDD"},{"id":"node-1769604538856","position":{"x":290.9449920654297,"y":278.4501037597656},"size":{"width":355.8094177246094,"height":211.28073120117188},"type":"NoteBox","content":"Code generation"},{"id":"path-1769604590391","type":"Path","startElement":"node-1769604529613","endElement":"node-1769604530293"}]},{"id":"node-1769603202875-874970","type":"Path","startElement":"node-1769603202874-772675","endElement":"node-1769603202875-362691"}]},{"id":"node-1769603202875-428713","position":{"x":1207.9105224609375,"y":-111.04102754592896},"size":{"width":450,"height":300},"type":"Container","title":"Spark Streaming"},{"id":"node-1769603202875-639500","position":{"x":1736.9188232421875,"y":-113.43353700637817},"size":{"width":450,"height":300},"type":"Container","title":"MLlib"},{"id":"node-1769603202875-196732","position":{"x":2247.131591796875,"y":-120.0151596069336},"size":{"width":450,"height":300},"type":"Container","title":"GraphX"},{"id":"node-1769603202875-267998","type":"Path","startElement":"node-1769603202874-405322","endElement":"node-1769603202874-895150"},{"id":"node-1769603202875-160287","type":"Path","startElement":"node-1769603202875-428713","endElement":"node-1769603202874-405322"},{"id":"node-1769603202875-795750","type":"Path","startElement":"node-1769603202875-639500","endElement":"node-1769603202874-405322"},{"id":"node-1769603202875-177908","type":"Path","startElement":"node-1769603202875-196732","endElement":"node-1769603202874-405322"},{"id":"node-1769603202875-180089","position":{"x":-394.8951480248005,"y":-104.49277696230112},"size":{"width":450,"height":300},"type":"Container","title":"SQL"},{"id":"node-1769603202875-893458","position":{"x":153.3223679206341,"y":-104.49277696230112},"size":{"width":450,"height":300},"type":"Container","title":"Dataframe","description":"The DataFrame API, \nInspired by pandas DataFrames in structure, format, and a few specific operations,\nSpark DataFrames are distributed in-memory tables with named columns and schemas. \nThey support basic data types like integers and strings, as well as complex types like maps, arrays, and structs. \nWhile Spark can infer schemas, the sources recommend defining your schema up front to relieve Spark of the expensive task of inferring types and to detect errors early.\n• Columns and Expressions: Named columns in DataFrames are objects with public methods that can be used in logical or mathematical expressions.\n• Rows: A record in a DataFrame is represented as a generic Row object, where fields can be accessed by an index.\n• Common Operations: Data is typically loaded using a DataFrameReader and written back using a DataFrameWriter. Common transformations include projections and filters (using select() and where()), as well as aggregations (using groupBy(), orderBy(), and count())."},{"id":"node-1769603202875-462091","position":{"x":677.0658219001263,"y":-99.59797986440955},"size":{"width":450,"height":300},"type":"Container","title":"Dataset","description":"The Dataset API\nThe Dataset API provides a unified interface where DataFrames are considered \"untyped\" while Datasets are \"typed\".\n• Typed vs. Untyped: In Scala and Java, Datasets are collections of strongly typed objects, whereas in Python and R, only DataFrames (which are untyped) make sense because those languages are not compile-time type-safe.\n• Encoders: Datasets use encoders to efficiently convert data between JVM objects and Spark’s internal Tungsten binary format."},{"id":"node-1769603202875-165950","type":"Path","startElement":"node-1769603202875-180089","endElement":"node-1769603202874-405322"},{"id":"node-1769603202875-190157","type":"Path","startElement":"node-1769603202875-893458","endElement":"node-1769603202874-405322"},{"id":"node-1769603202875-507767","type":"Path","startElement":"node-1769603202875-462091","endElement":"node-1769603202874-405322"},{"id":"node-1769690949684-333370","position":{"x":-479.7187102503012,"y":-1647.4840077884583},"size":{"width":3078.9849548339844,"height":1316.6897547841072},"type":"Container","title":"Applications","pan":{"x":12066.669772243433,"y":1750.5651452831162,"scale":0.6334551103173994},"children":[{"id":"node-1769765582511-821093","position":{"x":11993.418486493834,"y":1846.1126153321734},"size":{"width":1862.4088134765625,"height":1756.2827758789062},"type":"CodeBox","title":"Banking App","content":"from pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom pyspark.sql import types as T\nfrom pyspark.sql.window import Window\nfrom pyspark import StorageLevel\n\ndef run():\n    # CONFIG: The Security Vault\n    builder = SparkSession.builder.appName(\"BankDailyReconciliation\")\n\n    # SECURITY: Kerberos Authentication for HDFS/Hadoop (On-Prem)\n    # This ensures secure access to HDFS, YARN, and Hive services.\n    builder = builder.config(\"spark.kerberos.keytab\", \"/etc/security/keytabs/spark.keytab\") \\\n                     .config(\"spark.kerberos.principal\", \"spark_prod@RETAILBANK.COM\")\n\n    # SECURITY: AWS S3 Authentication (Cloud)\n    # We specify a Credential Provider to avoid hardcoding Access Keys/Secrets.\n    # 'InstanceProfileCredentialsProvider' automatically uses the IAM Role assigned to the EC2/Cluster.\n    builder = builder.config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"com.amazonaws.auth.InstanceProfileCredentialsProvider\")\n\n    spark = builder.enableHiveSupport().getOrCreate()\n\n    # 1. INPUTS: Gathering the Money Trail\n\n    # --- S3: Mobile App Logs (JSON) ---\n    # INFO: JSON normally infers schema (full scan). We use explicit schema for speed/safety.\n    # This prevents Spark from reading the data twice (once for schema, once for data).\n    # Added txn_ts (String) for date parsing and merchant_types (Array) for HOFs\n    # Programmatic Schema Definition (StructType)\n    # This is safer than DDL strings as it allows comments per field and compile-time checks in some languages.\n    # json_schema = \"txn_id STRING, cust_id STRING, amount DOUBLE, status STRING, txn_ts STRING, merchant_types ARRAY\"\n    json_schema = T.StructType([\n        T.StructField(\"txn_id\", T.StringType(), True),\n        T.StructField(\"cust_id\", T.StringType(), True),\n        T.StructField(\"amount\", T.DoubleType(), True),\n        T.StructField(\"status\", T.StringType(), True),\n        T.StructField(\"txn_ts\", T.StringType(), True),\n        T.StructField(\"merchant_types\", T.ArrayType(T.StringType()), True)\n    ])\n    mobile_logs_df = spark.read.schema(json_schema).json(\"s3a://bank-audit-logs/2023/10/24/*.json\")\n\n    # DATAFRAME READER API REFERENCE:\n    # spark.read provides a unified interface for loading data.\n    # Common Formats:\n    #   .parquet(\"path\") -> Efficient, columnar, stores schema (Default for Spark)\n    #   .csv(\"path\")     -> Check .option(\"header\", \"true\") and .option(\"inferSchema\", \"true\")\n    #   .json(\"path\")    -> Semi-structured, good for logs\n    #   .text(\"path\")    -> Reads lines as a single \"value\" column\n    #   .orc(\"path\")     -> Optimized Row Columnar (similar to Parquet)\n    #   .format(\"avro\")  -> Requires external package\n    # Key Methods:\n    #   .schema(...)     -> Define explicit schema to avoid expensive inference scans\n    #   .option(k, v)    -> Pass format-specific config (delimiter, mode, charset)\n\n    # --- JDBC: Customer Master Data (Oracle DB) ---\n    # Secured via SSL and Credentials from an internal vault\n    customers_df = spark.read.format(\"jdbc\") \\\n        .option(\"url\", \"jdbc:oracle:thin:@//db-prod:1521/PROD_SERVICE?ssl=true\") \\\n        .option(\"user\", \"reconciler_app\") \\\n        .option(\"password\", \"Vault_Secret_P@ss\") \\\n        .option(\"dbtable\", \"customer_profiles\") \\\n        .load()\n\n    # 2. TRANSFORMATIONS: Cleaning & Matching\n    # Narrow: Basic Cleanup (No data movement between nodes)\n    # [LAZY] Defines the query plan but executes nothing yet.\n\n    # Define a simple UDF (User Defined Function)\n    # NOTE: Native Spark functions are always preferred over UDFs for performance (Vectorization vs Row-by-Row Python)\n    @F.udf(\"string\")\n    def categorize_amount(amt):\n        if amt < 100: return \"Low\"\n        elif amt < 1000: return \"Medium\"\n        else: return \"High\"\n\n    # EXPR: Using SQL Expression for flexible column derivation (mixes SQL with DataFrame API)\n    clean_txns = mobile_logs_df \\\n        .withColumn(\"txn_date\", F.to_date(\"txn_ts\")) \\\n        .withColumn(\"txn_hour\", F.hour(\"txn_ts\")) \\\n        .withColumnRenamed(\"status\", \"txn_status\") \\\n        .withColumn(\"amt_category\", categorize_amount(F.col(\"amount\"))) \\\n        .withColumn(\"txn_fee_est\", F.expr(\"amount * 0.05 + 1.50\")) \\\n        .withColumn(\"has_risky_merchant\", F.exists(F.col(\"merchant_types\"), lambda x: x == F.lit(\"Casino\"))) \\\n        .select(\"txn_id\", \"cust_id\", \"amount\", \"txn_fee_est\", \"txn_status\", \"txn_date\", \"txn_hour\", \"amt_category\", \"has_risky_merchant\", \"merchant_types\") \\\n        .filter(\"txn_status = 'SUCCESS'\")\n\n    # 2.1 COMPLEX TYPES: Flattening Arrays\n    # Common exam pattern: \"explode\" creates a new row for each element in the array.\n    # Note: This changes the grain of the data (one transaction -> multiple rows).\n    # We create a side-stream here to avoid duplicating amounts in the main aggregation.\n    exploded_txns = clean_txns.withColumn(\"merchant\", F.explode(\"merchant_types\"))\n\n    # Wide: The \"Heavy Lift\" (Joins & Aggregations cause Shuffles)\n    # Matching logs to customer profiles to find high-risk transfers\n    # [LAZY] Still just building the DAG (Directed Acyclic Graph).\n    fraud_check_df = clean_txns.join(customers_df, \"cust_id\") \\\n        .groupBy(\"cust_id\", \"risk_level\") \\\n        .agg(F.sum(\"amount\").alias(\"total_daily_volume\")) \\\n        .filter(\"total_daily_volume > 10000\")\n\n    # WINDOW FUNCTION: Ranking High-Risk Customers\n    # Calculate rank within each risk level based on total volume\n    # This transform stays narrow within the existing partitions from the join/groupBy\n    window_spec = Window.partitionBy(\"risk_level\").orderBy(F.col(\"total_daily_volume\").desc())\n    fraud_check_df = fraud_check_df.withColumn(\"rank_in_group\", F.dense_rank().over(window_spec))\n\n    # 3. PERFORMANCE: Caching the High-Risk list\n    # We cache this because both the 'Fraud Dept' and 'Branch Managers' need it\n    # [LAZY] Cache is a hint. Data is only cached when the first ACTION runs.\n    fraud_check_df.cache()\n    # persist(StorageLevel.MEMORY_AND_DISK) to allow spilling to disk.\n    fraud_check_df.persist(StorageLevel.MEMORY_AND_DISK)\n\n    # 4. OUTPUTS: Refined Storage (Delta Lake)\n    # Managed Table: Internal use for Databricks SQL Analysts\n    # [ACTION] This triggers the actual Spark Job (read -> process -> write).\n    # [SECURITY] Metastore communication is secured via Kerberos (configured at top).\n    fraud_check_df.write.format(\"delta\") \\\n        .mode(\"overwrite\") \\\n        .saveAsTable(\"audit_db.high_risk_daily_summary\")\n\n    # External Table: Shared with a 3rd party auditing firm (Specific S3 Bucket)\n    # [ACTION] Triggers another job. Since we cached above, this uses the cached data!\n    # [OPTIMIZATION] coalesce(1) combines partitions into 1 file without a full shuffle.\n    # [SECURITY] S3 access uses the IAM Role (Instance Profile) configured at top.\n    fraud_check_df.coalesce(1).write.format(\"parquet\") \\\n        .option(\"path\", \"s3a://external-auditor-dropzone/daily_reports/\") \\\n        .saveAsTable(\"audit_db.external_audit_report\")\n\n    # 5. VIEW: Creating a SQL window for the dashboard\n    # [METADATA] Registers the view name on the driver. No job triggered.\n    fraud_check_df.createOrReplaceTempView(\"v_daily_fraud_monitoring\")\n\n    # 6. CATALOG API: Exploring Metadata\n    # List tables to verify our Managed and External tables + Views\n    print(\"Catalog Objects in audit_db:\")\n    # We use listTables to see Table(name, database, description, tableType, isTemporary)\n    for t in spark.catalog.listTables(\"audit_db\"):\n        print(f\"Name: {t.name}, Type: {t.tableType}\")\n\n    spark.stop()\n\nif __name__ == \"__main__\":\n    run()\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}]}]},{"id":"path-1769603520915","type":"Path","startElement":"node-1769451521963-436559","endElement":"node-1769603202874-870967"},{"id":"path-1769690869984","type":"Path","startElement":"node-1769603202874-870967","endElement":"node-1769451521963-436559"},{"id":"node-1769766630489-684482","position":{"x":-3702.954844358276,"y":-2170.3930840599887},"size":{"width":3884.2339401245117,"height":2371.839192390442},"type":"CodeBox","title":"Installation","content":"pip install pyspark\n# A critical prerequisite is having Java 8 or above installed with the JAVA_HOME environment variable properly set\n\n# bin: Contains interactive shells such as pyspark, spark-shell, spark-sql, and sparkR.\n# sbin: Contains administrative scripts to start and stop Spark components.\n# kubernetes: Provides Dockerfiles for creating images for Spark on Kubernetes clusters.\n# data: Populated with .txt files used as input for various Spark components.\n\n\n\n\n\n\n\n\n\n\n"}]}]}}