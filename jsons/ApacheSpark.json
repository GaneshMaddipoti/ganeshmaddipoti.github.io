{"id":null,"name":"ApacheSpark","data":{"id":"root","size":{"width":1728,"height":996},"pan":{"x":-134.41604184001895,"y":776.3526200842158,"scale":0.637649033524776},"children":[{"id":"line-1768775896776","type":"Line","lineCoordinates":[{"x":491.6481628417969,"y":288.8475341796875},{"x":768.648193359375,"y":288.8475341796875}]},{"id":"node-1768775941880","position":{"x":486.5516662597656,"y":322.22052001953125},"size":{"width":1267.6734619140625,"height":432.505615234375},"type":"TextBox","title":"About","content":"Apache Spark is a unified analytics engine for large-scale data processing. (Cluster computing platform)\nAt its core, Spark is a “computational engine” that is responsible for scheduling, distributing, and monitoring applications consisting of many computational tasks across many worker machines, or a computing cluster. \nIf Hadoop MapReduce was the \"first generation\" of big data processing, Spark is the \"second generation\"—built for speed, ease of use, and versatility.\n\nFeatures\nEasy to use: we can develop applications, using a high-level API that let us focus on the content of our computation\nSpeed: It is up to 100x faster than Hadoop MapReduce because it processes data in-memory (RAM) rather than writing to the disk after every step.\nLazy Evaluation: Spark doesn't execute your code immediately. It builds a plan (called a DAG or Directed Acyclic Graph) and only runs it when you finally ask for a result. This allows it to optimize the entire process first.\nAs general engine: letting you combine multiple types of computations (e.g., SQL queries, text process‐ing, and machine learning) that might previously have required different engines.\n\nPolyglot: You can write Spark code in Python (PySpark), Scala, Java, or R."},{"id":"line-1768776347399","type":"Line","lineCoordinates":[{"x":1033.8216552734375,"y":1239.6767578125},{"x":1033.8216552734375,"y":1239.6767578125}]},{"id":"line-1768776355611","type":"Line","lineCoordinates":[{"x":1124.3660888671875,"y":1114.84912109375},{"x":1124.3660888671875,"y":1114.84912109375}]},{"id":"node-1768776778906","position":{"x":488.4365539550781,"y":238.79788208007812},"size":{"width":213.70458984375,"height":47.15156555175781},"type":"TitleBox","content":"Apache Spark\n"},{"id":"node-1768777090579","position":{"x":489.7679748535156,"y":832.845458984375},"size":{"width":1274.9583129882812,"height":738.1610107421875},"type":"Container","title":"Architecture","pan":{"x":38,"y":258},"children":[{"id":"node-1768777131082","position":{"x":119.09141540527344,"y":773.4568481445312},"size":{"width":450,"height":300},"type":"Container","title":"Client","description":"\n","children":[{"id":"node-1768804676825","position":{"x":175.23153686523438,"y":126.59458923339844},"size":{"width":450,"height":300},"type":"Container","title":"Spark Driver","description":"Driver Program: The \"brain\" of your application. It runs your main() function and converts your code into tasks.\nAt a high level, every Spark application consists of a driver program that launches various parallel operations on a cluster. \nThe driver program contains your application’s main function and defines distributed datasets on the cluster, then applies operations to them.\nDriver programs access Spark through a SparkContext object, which represents a connection to a computing cluster.\n\n\nResponsibilities\n1) Maintains the SparkContext/SparkSession: It is the entry point of the application.\n2) Analyzes Code: it converts your high-level code (SQL, Python, Scala) into a logical plan.\n3) Creates the DAG: It builds a Directed Acyclic Graph (DAG) of all the transformations you want to perform.\n4) Schedules Tasks: It breaks the DAG into \"Stages\" and then into \"Tasks,\" which it sends to the Executors (worker nodes) to be processed.\n","children":[{"id":"node-1768807029041","position":{"x":88.16413879394531,"y":13.675613403320312},"size":{"width":785.7688293457031,"height":523.0265502929688},"type":"CodeBox","content":"#Application \nfrom pyspark.sql import SparkSession\n\n# 1. The Driver starts and creates the SparkSession\nspark = SparkSession.builder.appName(\"LogAnalysis\").get_home()\n# 2. The Driver creates a plan to read the data (Lazy Evaluation)\nlogs = spark.read.text(\"s3://my-bucket/logs/*.txt\")\n# 3. The Driver plans the transformation (Filtering)\naws_logs = logs.filter(logs.value.contains(\"AWS\"))\n# 4. The Driver triggers an 'Action' (.count())\n# This is where the Driver converts the DAG into actual tasks\nresult = aws_logs.count()\n# 5. The Driver receives the final number from the executors and prints it\nprint(f\"Total AWS entries: {result}\")\n\nspark.stop()\n\n\n\n\n\n"}]}]},{"id":"node-1768777133224","position":{"x":958.9923095703125,"y":771.3358764648438},"size":{"width":450,"height":300},"type":"Container","title":"Cluster Manager"},{"id":"node-1768777134492","position":{"x":1420.81787109375,"y":269.3124084472656},"size":{"width":450,"height":300},"type":"Container","title":"Worker Node","children":[{"id":"node-1768807279312","position":{"x":89.96749877929688,"y":62.73999786376953},"size":{"width":718.8135986328125,"height":423.2466735839844},"type":"Container","title":"Application Master","children":[{"id":"node-1768807335618-610","position":{"x":145.12289428710938,"y":88.24881744384766},"size":{"width":1090.2388916015625,"height":657.4938354492188},"type":"Container","title":"Spark Driver","description":"Driver Program: The \"brain\" of your application. It runs your main() function and converts your code into tasks.\nAt a high level, every Spark application consists of a driver program that launches various parallel operations on a cluster. \nThe driver program contains your application’s main function and defines distributed datasets on the cluster, then applies operations to them.\nDriver programs access Spark through a SparkContext object, which represents a connection to a computing cluster.\n\n\nResponsibilities\n1) Maintains the SparkContext/SparkSession: It is the entry point of the application.\n2) Analyzes Code: it converts your high-level code (SQL, Python, Scala) into a logical plan.\n3) Creates the DAG: It builds a Directed Acyclic Graph (DAG) of all the transformations you want to perform.\n4) Schedules Tasks: It breaks the DAG into \"Stages\" and then into \"Tasks,\" which it sends to the Executors (worker nodes) to be processed.\n","children":[{"id":"node-1768807335618-635","position":{"x":153.72389221191406,"y":68.72859191894531},"size":{"width":1840.5922546386719,"height":1064.607666015625},"type":"CodeBox","content":"#Application \nfrom pyspark.sql import SparkSession\n\n# 1. The Driver starts and creates the SparkSession\nspark = SparkSession.builder.appName(\"LogAnalysis\").get_home()\n# 2. The Driver creates a plan to read the data (Lazy Evaluation)\nlogs = spark.read.text(\"s3://my-bucket/logs/*.txt\")\n# 3. The Driver plans the transformation (Filtering)\naws_logs = logs.filter(logs.value.contains(\"AWS\"))\n# 4. The Driver triggers an 'Action' (.count())\n# This is where the Driver converts the DAG into actual tasks\nresult = aws_logs.count()\n# 5. The Driver receives the final number from the executors and prints it\nprint(f\"Total AWS entries: {result}\")\n\nspark.stop()\n\n\n\n\n\n"}]}]}]},{"id":"node-1768777135433","position":{"x":2021.1544189453125,"y":660.1998291015625},"size":{"width":450,"height":300},"type":"Container","title":"Worker Node","children":[{"id":"node-1768807431943","position":{"x":229.38766479492188,"y":158.36561584472656},"size":{"width":450,"height":300},"type":"Container","title":"Executor","children":[{"id":"node-1768807455812","position":{"x":181.1865997314453,"y":118.54624938964844},"size":{"width":450,"height":300},"type":"Container","title":"JVM"}]}]},{"id":"node-1768777136358","position":{"x":2056.712646484375,"y":1227.8822021484375},"size":{"width":450,"height":300},"type":"Container","title":"Worker Node"},{"id":"path-1768777149657","type":"Path","startElement":"node-1768777131082","endElement":"node-1768777133224"},{"id":"path-1768777155157","type":"Path","startElement":"node-1768777133224","endElement":"node-1768777135433"},{"id":"path-1768777156727","type":"Path","startElement":"node-1768777133224","endElement":"node-1768777136358"},{"id":"path-1768804144452","type":"Path","startElement":"node-1768777133224","endElement":"node-1768777134492"},{"id":"line-1768804173997","position":{"x":66.03778076171875,"y":6.603759765625},"type":"Line","lineCoordinates":[{"x":690.7655029296875,"y":292.893798828125},{"x":703.627197265625,"y":1635.557373046875}]},{"id":"node-1768804210603","position":{"x":369.8416290283203,"y":291.53314208984375},"size":{"width":364.4654235839844,"height":100},"type":"TitleBox","content":"Client Mode"},{"id":"node-1768807491334","position":{"x":780.3194580078125,"y":287.870849609375},"size":{"width":360.314697265625,"height":95.837158203125},"type":"TitleBox","content":"Cluster Mode"}]},{"id":"node-1768807543063","position":{"x":895.9692993164062,"y":1446.971923828125},"size":{"width":262.51068115234375,"height":106.02978515625},"type":"NoteBox","content":"In cluster mode:\nThe Cluster Manager starts the AM first, and then the AM launches the Spark Driver inside its own process"},{"id":"node-1768808101135","position":{"x":488.829345703125,"y":1640.876708984375},"size":{"width":1164.61865234375,"height":653.0797119140625},"type":"Container","title":"Ecosystem","children":[{"id":"node-1768808131873","position":{"x":868.9764404296875,"y":814.120849609375},"size":{"width":450,"height":300},"type":"Container","title":"Spark Core","description":"Spark Core contains the basic functionality of Spark, \nincluding components for task scheduling, memory management, fault recovery, interacting with storage systems, and more. \nSpark Core is also home to the API that defines resilient distributed datasets (RDDs), which are Spark’s main programming abstraction. \nRDDs represent a collection of items distributed across many compute nodes that can be manipulated in parallel. \n","pan":{"x":-490.8382115794601,"y":-327.2254743863067,"scale":0.23914845000000007},"children":[{"id":"node-1768808263983","position":{"x":-295.98136711120605,"y":12.057342529296875},"size":{"width":450,"height":300},"type":"Container","title":"Partitioning","description":"In the Spark world, the partition is the smallest unit of data processing. \nSpark cannot process a dataset \"as a whole.\" \nInstead, it breaks the dataset down into these manageable chunks so it can apply the \"Divide and Conquer\" strategy across a cluster of computers.\n\n1. The Relationship: Partition - Task - Core\nTo understand Spark processing, you have to look at how these three elements link together:\nPartition: A physical slice of your data (e.g., 10,000 rows of a CSV).\nTask: A single unit of work (e.g., \"convert these strings to uppercase\").\nCore (Slot): A CPU thread on a Worker node.\nThe Rule: One Task processes exactly one Partition on one Core at a time.\n\n2. Parallelism vs. Partitioning\nIf you have a 1GB file and only 1 partition, \nSpark can only use 1 CPU core to process it—even if your cluster has 100 computers! The other 99 computers will sit idle.\nUnder-partitioning: Too few partitions = Low parallelism (slow).\nOver-partitioning: Too many partitions = High overhead (Spark spends more time managing tasks than actually processing data).\n\n3. How Spark decides the number of Partitions?\nSpark sets the initial number of partitions based on where the data comes from:\nFrom HDFS/S3: Usually based on the file \"blocks\" (defaulting to 128MB per partition).\nFrom Local Collections: If you use sc.parallelize(data), it usually defaults to the total number of cores in your cluster.\nFrom Shuffles: When you do a join or groupBy, Spark defaults to 200 partitions (this is a configurable setting called spark.sql.shuffle.partitions).\n\n4. What happens during processing?\nWhen you write code like rdd.map(lambda x: x + 1), Spark doesn't send the data to the code; it sends the code to the data.\nThe Driver sends the \"Map\" function to every Executor.\nEach Executor applies that function to the partitions it has in its local memory.\nBecause each partition is independent, they can all be processed at the exact same time without talking to each other.\n"},{"id":"node-1768808271274","position":{"x":230.3988494873047,"y":11.772336959838867},"size":{"width":450,"height":300},"type":"Container","title":"RDD","description":"An RDD in Spark is simply an immutable distributed collection of objects. \nEach RDD is split into multiple partitions, which may be computed on different nodes of the cluster. \nRDDs can contain any type of Python, Java, or Scala objects, including user-defined classes.\nIn Spark all work is expressed as either creating new RDDs, transforming existing RDDs, or calling operations on RDDs to compute a result. \nUnder the hood, Spark automatically distributes the data contained in RDDs across your cluster and parallelizes the operations you perform on them.\n\nUsers create RDDs in two ways: \n1) by loading an external dataset, or \n2) by distributing a collection of objects (e.g., a list or set) in their driver program.\n\nOnce created, RDDs offer two types of operations: \n1) transformations and \n2) actions.\nTransformations construct a new RDD from a previous one.\nActions, on the other hand, compute a result based on an RDD, and either return it to the driver program or save it to an external storage system.\nReturn type: transformations return RDDs, whereas actions return some other data type.\n\nTo summarize, every Spark program and shell session will work as follows:\n1. Create some input RDDs from external data.\n2. Transform them to define new RDDs using transformations like filter().\n3. Ask Spark to persist() any intermediate RDDs that will need to be reused.\n4. Launch actions such as count() and first() to kick off a parallel computation, which is then optimized and executed by Spark.\n","children":[{"id":"node-1768808365766","position":{"x":50.690948486328125,"y":19.30596160888672},"size":{"width":799.1734008789062,"height":507.9425354003906},"type":"CodeBox","content":"# READ\n#local collection\ndata = [1, 2, 3, 4, 5]\nrdd = sc.parallelize(data)\n\n# Read a single file, a directory, or a wildcard pattern\nrdd_text = sc.textFile(\"path/to/data.txt\")\n\n# You can also specify the minimum number of partitions\nrdd_partitioned = sc.textFile(\"path/to/data.txt\", minPartitions=4)\n\n# Useful for processing a directory of small logs\nrdd_whole = sc.wholeTextFiles(\"path/to/directory/\")\n# Result: [('file1.txt', 'content...'), ('file2.txt', 'content...')]\n\n#from json\nrdd_raw_json = sc.textFile(\"data.json\")\nrdd_parsed_json = rdd_raw_json.map(lambda line: json.loads(line))\n\n#from csv\nrdd_raw_csv = sc.textFile(\"data.csv\")\ndef parse_csv(line):\n    # StringIO allows the csv module to treat a string like a file\n    f = StringIO(line)\n    reader = csv.reader(f)\n    return next(reader)\nrdd_csv = rdd_raw_csv.map(parse_csv)\n\n#binary files\nrdd_binary = sc.binaryFiles(\"path/to/images/\")\n\n\n#TRANSFORMATIONS\n#map: Transforms each element into exactly one new element.\n#flatMap: Similar to map, but each input item can be mapped to 0 or more output items.\n\n# Example: Converting text to uppercase\nupper_rdd = rdd.map(lambda line: line.upper())\n\n# Example: Splitting lines into individual words\nwords_rdd = rdd.flatMap(lambda line: line.split(\" \"))\n\n# Keep only lines that contain the word 'Spark'\nfiltered_rdd = rdd.filter(lambda line: \"Spark\" in line)\n\n#ACTIONS\n#Action\t                Description\n#collect()\t        Returns the entire RDD as a list to the driver. (Warning: Only use for small datasets!)\n#count()\t        Returns the number of elements in the RDD.\n#take(n)\t        Returns the first $n$ elements of the RDD.\n#saveAsTextFile(path)\tWrites the RDD to a text file (or directory of files).\n\n# Triggering the execution\nprint(f\"Total words: {word_counts.count()}\")\n\n# Taking a sample of the results\nprint(word_counts.take(5))\n\n# Saving the results back to disk\nword_counts.saveAsTextFile(\"output/word_count_results\")\n\n"}]},{"id":"node-1768808272024","position":{"x":781.0330200195312,"y":9.655006408691406},"size":{"width":450,"height":300},"type":"Container","title":"Key-value RDD","description":"Key-Value Pair RDDs (often called \"Pair RDDs\") are a specialized type of RDD where each element is a tuple consisting of two items: (Key, Value).\n","children":[{"id":"node-1768808421458","position":{"x":200.54798889160156,"y":51.24694061279297},"size":{"width":647.838623046875,"height":438.4605407714844},"type":"CodeBox","content":"# Starting with a regular RDD of strings\nlines = sc.textFile(\"data.csv\")\n\n# Transforming into a Pair RDD: (User_ID, Transaction_Amount)\n# Input line example: \"user123,50.00\"\npair_rdd = lines.map(lambda x: (x.split(\",\")[0], float(x.split(\",\")[1])))\n\n# EFFICIENT: Sum of amounts per user\ntotal_per_user = pair_rdd.reduceByKey(lambda a, b: a + b)\n\n# LESS EFFICIENT: Group all transactions into a list per user\nall_transactions = pair_rdd.groupByKey()\n\n# RDD 1: (User_ID, Name)\nnames = sc.parallelize([(\"1\", \"Alice\"), (\"2\", \"Bob\")])\n\n# RDD 2: (User_ID, Purchase_Amount)\npurchases = sc.parallelize([(\"1\", 25.00), (\"1\", 15.00), (\"2\", 40.00)])\n\n# Join them together\n# Result: (\"1\", (\"Alice\", 25.00)), (\"1\", (\"Alice\", 15.00)), (\"2\", (\"Bob\", 40.00))\njoined_rdd = names.join(purchases)\n"}]}]},{"id":"node-1768808133469","position":{"x":142.7048797607422,"y":194.82431030273438},"size":{"width":450,"height":300},"type":"Container"},{"id":"node-1768808134544","position":{"x":675.9729614257812,"y":203.9559326171875},"size":{"width":450,"height":300},"type":"Container"},{"id":"node-1768808135915","position":{"x":1208.7093505859375,"y":197.23184204101562},"size":{"width":450,"height":300},"type":"Container"},{"id":"node-1768808136948","position":{"x":1736.946044921875,"y":194.9818115234375},"size":{"width":450,"height":300},"type":"Container"}]}]}}