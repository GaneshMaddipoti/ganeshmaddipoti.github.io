{"id":null,"name":"Apache-Spark","data":{"id":"root","size":{"width":1728,"height":996},"pan":{"x":1294.0548931147327,"y":734.1641538366961,"scale":2.9976186805394147},"children":[{"id":"node-1768386812421","position":{"x":1354.2491455078125,"y":763.9320678710938},"size":{"width":458.478515625,"height":300},"type":"Container","title":"Apache Spark","description":"Apache Spark is a unified analytics engine for large-scale data processing. (Cluster computing platform)\nAt its core, Spark is a “computational engine” that is responsible for scheduling, distributing, and monitoring applications consisting of many computational tasks across many worker machines, or a computing cluster. \nIf Hadoop MapReduce was the \"first generation\" of big data processing, Spark is the \"second generation\"—built for speed, ease of use, and versatility.\n\nFeatures\nEasy to use: we can develop applications, using a high-level API that let us focus on the content of our computation\nSpeed: It is up to 100x faster than Hadoop MapReduce because it processes data in-memory (RAM) rather than writing to the disk after every step.\nLazy Evaluation: Spark doesn't execute your code immediately. It builds a plan (called a DAG or Directed Acyclic Graph) and only runs it when you finally ask for a result. This allows it to optimize the entire process first.\nAs general engine: letting you combine multiple types of computations (e.g., SQL queries, text process‐ing, and machine learning) that might previously have required different engines.\n\nPolyglot: You can write Spark code in Python (PySpark), Scala, Java, or R.\n\n\n\n\n\n\n\n","pan":{"x":-460.84062555276887,"y":-319.4435672790166,"scale":0.27835478184371387},"children":[{"id":"node-1768387001355","position":{"x":-149.17035293579102,"y":-287.57633113861084},"size":{"width":861.72607421875,"height":356.93047600984573},"type":"Container","title":"Architecture","description":"Spark operates on a Master-Slave architecture. It doesn't usually store data itself; instead, it reaches out to data sources (like S3 or HDFS), pulls the data into memory, and processes it across a cluster of computers.","pan":{"x":-910.6463920454369,"y":-385.7885035415292,"scale":0.27243575730637165},"children":[{"id":"node-1768387053759","position":{"x":-922.859619140625,"y":-10.98482871055603},"size":{"width":565.7882995605469,"height":367.23626708984375},"type":"Container","title":"Driver","description":"Driver Program: The \"brain\" of your application. It runs your main() function and converts your code into tasks.\nAt a high level, every Spark application consists of a driver program that launches various parallel operations on a cluster. \nThe driver program contains your application’s main function and defines distributed datasets on the cluster, then applies operations to them.\nDriver programs access Spark through a SparkContext object, which represents a connection to a computing cluster.\n\n\nResponsibilities\n1) Maintains the SparkContext/SparkSession: It is the entry point of the application.\n2) Analyzes Code: it converts your high-level code (SQL, Python, Scala) into a logical plan.\n3) Creates the DAG: It builds a Directed Acyclic Graph (DAG) of all the transformations you want to perform.\n4) Schedules Tasks: It breaks the DAG into \"Stages\" and then into \"Tasks,\" which it sends to the Executors (worker nodes) to be processed.\n\n","children":[{"id":"node-1768400322044","position":{"x":524.2629699707031,"y":77.6949462890625},"size":{"width":600.8836517333984,"height":461.4035415649414},"type":"CodeBox","content":"from pyspark.sql import SparkSession\n\n# 1. The Driver starts and creates the SparkSession\nspark = SparkSession.builder.appName(\"LogAnalysis\").get_home()\n\n# 2. The Driver creates a plan to read the data (Lazy Evaluation)\nlogs = spark.read.text(\"s3://my-bucket/logs/*.txt\")\n\n# 3. The Driver plans the transformation (Filtering)\naws_logs = logs.filter(logs.value.contains(\"AWS\"))\n\n# 4. The Driver triggers an 'Action' (.count())\n# This is where the Driver converts the DAG into actual tasks\nresult = aws_logs.count()\n\n# 5. The Driver receives the final number from the executors and prints it\nprint(f\"Total AWS entries: {result}\")\n\nspark.stop()"}]},{"id":"node-1768387067534","position":{"x":-78.00450134277344,"y":-70.75757789611816},"size":{"width":883.8558349609375,"height":552.2539672851562},"type":"Container","title":"Cluster Manager","description":"Cluster Manager: The \"allocator\" (e.g., YARN, Kubernetes, or Spark Standalone) that manages resources and decides which machines do what.\nUnder the hood, Spark is designed to efficiently scale up from one to many thousands of compute nodes. \nTo achieve this while maximizing flexibility, Spark can run over a variety of cluster managers, including Hadoop YARN, Apache Mesos, and a simple\ncluster manager included in Spark itself called the Standalone Scheduler."},{"id":"node-1768387081204","position":{"x":1275.6825561523438,"y":401.58311462402344},"size":{"width":562.68115234375,"height":364.18634033203125},"type":"Container","title":"Executor","pan":{"x":-2543.0732062324178,"y":-1567.0243553749474,"scale":0.18301617063661463}},{"id":"node-1768387081721","position":{"x":1632.4658203125,"y":38.89530944824219},"size":{"width":560.2086181640625,"height":345.6688690185547},"type":"Container","title":"Executor"},{"id":"node-1768387082055","position":{"x":1248.9713592529297,"y":-310.5077381134033},"size":{"width":552.4395751953125,"height":338.85069465637207},"type":"Container","title":"Executor","description":"Executors: The \"workers\" that live on the worker nodes. They execute the tasks assigned by the driver and store data in memory or disk."},{"id":"path-1768387468528","type":"Path","startElement":"node-1768387053759","endElement":"node-1768387067534"},{"id":"path-1768387476110","type":"Path","startElement":"node-1768387067534","endElement":"node-1768387082055"},{"id":"path-1768387478644","type":"Path","startElement":"node-1768387067534","endElement":"node-1768387081721"},{"id":"path-1768387480407","type":"Path","startElement":"node-1768387067534","endElement":"node-1768387081204"}]},{"id":"node-1768388409134","position":{"x":166.90554809570312,"y":534.0734710693359},"size":{"width":229.0255889892578,"height":135.33189392089844},"type":"Container","title":"Spark Core","description":"Spark Core contains the basic functionality of Spark, \nincluding components for task scheduling, memory management, fault recovery, interacting with storage systems, and more. \nSpark Core is also home to the API that defines resilient distributed datasets (RDDs), which are Spark’s main programming abstraction. \nRDDs represent a collection of items distributed across many compute nodes that can be manipulated in parallel. \n\n\n\n","pan":{"x":-933.5782997965923,"y":-551.4321432582925,"scale":0.10080685857804739},"children":[{"id":"node-1768479286026","position":{"x":82.58241558074951,"y":-267.50473403930664},"size":{"width":457.0928111076355,"height":306.80996799468994},"type":"Container","title":"RDD","description":"An RDD in Spark is simply an immutable distributed collection of objects. \nEach RDD is split into multiple partitions, which may be computed on different nodes of the cluster. \nRDDs can contain any type of Python, Java, or Scala objects, including user-defined classes.\nIn Spark all work is expressed as either creating new RDDs, transforming existing RDDs, or calling operations on RDDs to compute a result. \nUnder the hood, Spark automatically distributes the data contained in RDDs across your cluster and parallelizes the operations you perform on them.\n\nUsers create RDDs in two ways: \n1) by loading an external dataset, or \n2) by distributing a collection of objects (e.g., a list or set) in their driver program.\n\nOnce created, RDDs offer two types of operations: \n1) transformations and \n2) actions.\nTransformations construct a new RDD from a previous one.\nActions, on the other hand, compute a result based on an RDD, and either return it to the driver program or save it to an external storage system.\nReturn type: transformations return RDDs, whereas actions return some other data type.\n\nTo summarize, every Spark program and shell session will work as follows:\n1. Create some input RDDs from external data.\n2. Transform them to define new RDDs using transformations like filter().\n3. Ask Spark to persist() any intermediate RDDs that will need to be reused.\n4. Launch actions such as count() and first() to kick off a parallel computation, which is then optimized and executed by Spark.\n\n\n\n","pan":{"x":-1057.7400355640511,"y":-754.9665213558559,"scale":0.1151348394855992},"children":[{"id":"path-1768487448169","type":"Path","startElement":"node-1768487423523","endElement":"node-1768487419660"},{"id":"path-1768487449581","type":"Path","startElement":"node-1768487419660","endElement":"node-1768487423102"},{"id":"path-1768487450985","type":"Path","startElement":"node-1768487423102","endElement":"node-1768487423856"},{"id":"node-1768548214469","position":{"x":737.2796363830566,"y":-585.8026313781738},"size":{"width":2225.3373641967773,"height":2144.29931640625},"type":"CodeBox","content":"# READ\n#local collection\ndata = [1, 2, 3, 4, 5]\nrdd = sc.parallelize(data)\n\n# Read a single file, a directory, or a wildcard pattern\nrdd_text = sc.textFile(\"path/to/data.txt\")\n\n# You can also specify the minimum number of partitions\nrdd_partitioned = sc.textFile(\"path/to/data.txt\", minPartitions=4)\n\n# Useful for processing a directory of small logs\nrdd_whole = sc.wholeTextFiles(\"path/to/directory/\")\n# Result: [('file1.txt', 'content...'), ('file2.txt', 'content...')]\n\n#from json\nrdd_raw_json = sc.textFile(\"data.json\")\nrdd_parsed_json = rdd_raw_json.map(lambda line: json.loads(line))\n\n#from csv\nrdd_raw_csv = sc.textFile(\"data.csv\")\ndef parse_csv(line):\n    # StringIO allows the csv module to treat a string like a file\n    f = StringIO(line)\n    reader = csv.reader(f)\n    return next(reader)\nrdd_csv = rdd_raw_csv.map(parse_csv)\n\n#binary files\nrdd_binary = sc.binaryFiles(\"path/to/images/\")\n\n\n#TRANSFORMATIONS\n#map: Transforms each element into exactly one new element.\n#flatMap: Similar to map, but each input item can be mapped to 0 or more output items.\n\n# Example: Converting text to uppercase\nupper_rdd = rdd.map(lambda line: line.upper())\n\n# Example: Splitting lines into individual words\nwords_rdd = rdd.flatMap(lambda line: line.split(\" \"))\n\n# Keep only lines that contain the word 'Spark'\nfiltered_rdd = rdd.filter(lambda line: \"Spark\" in line)\n\n#ACTIONS\n#Action\t                Description\n#collect()\t        Returns the entire RDD as a list to the driver. (Warning: Only use for small datasets!)\n#count()\t        Returns the number of elements in the RDD.\n#take(n)\t        Returns the first $n$ elements of the RDD.\n#saveAsTextFile(path)\tWrites the RDD to a text file (or directory of files).\n\n# Triggering the execution\nprint(f\"Total words: {word_counts.count()}\")\n\n# Taking a sample of the results\nprint(word_counts.take(5))\n\n# Saving the results back to disk\nword_counts.saveAsTextFile(\"output/word_count_results\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}]},{"id":"node-1768556021544","position":{"x":604.0129547119141,"y":-269.1215362548828},"size":{"width":450,"height":300},"type":"Container","title":"Key-Value RDD","description":"Key-Value Pair RDDs (often called \"Pair RDDs\") are a specialized type of RDD where each element is a tuple consisting of two items: (Key, Value).\nThese are essential for many real-world big data tasks like grouping, aggregating, and joining datasets. \nThey act as the \"MapReduce\" engine within Spark.\n\nWhen you perform operations on keys, Spark often has to move data across the network so that all values for the same key sit on the same executor. This process is called a Shuffle.\n\n\n","children":[{"id":"node-1768556789159","position":{"x":395.96976470947266,"y":9.968986511230469},"size":{"width":516.3421630859375,"height":524.8188018798828},"type":"CodeBox","content":"# Starting with a regular RDD of strings\nlines = sc.textFile(\"data.csv\")\n\n# Transforming into a Pair RDD: (User_ID, Transaction_Amount)\n# Input line example: \"user123,50.00\"\npair_rdd = lines.map(lambda x: (x.split(\",\")[0], float(x.split(\",\")[1])))\n\n# EFFICIENT: Sum of amounts per user\ntotal_per_user = pair_rdd.reduceByKey(lambda a, b: a + b)\n\n# LESS EFFICIENT: Group all transactions into a list per user\nall_transactions = pair_rdd.groupByKey()\n\n# RDD 1: (User_ID, Name)\nnames = sc.parallelize([(\"1\", \"Alice\"), (\"2\", \"Bob\")])\n\n# RDD 2: (User_ID, Purchase_Amount)\npurchases = sc.parallelize([(\"1\", 25.00), (\"1\", 15.00), (\"2\", 40.00)])\n\n# Join them together\n# Result: (\"1\", (\"Alice\", 25.00)), (\"1\", (\"Alice\", 15.00)), (\"2\", (\"Bob\", 40.00))\njoined_rdd = names.join(purchases)\n\n\n\n\n\n\n\n\n\n\n\n"}]},{"id":"node-1768573563712","position":{"x":-426.9848327636719,"y":-261.45479583740234},"size":{"width":450,"height":300},"type":"Container","title":"Partitioning","description":"In the Spark world, the partition is the smallest unit of data processing. \nSpark cannot process a dataset \"as a whole.\" \nInstead, it breaks the dataset down into these manageable chunks so it can apply the \"Divide and Conquer\" strategy across a cluster of computers.\n\n1. The Relationship: Partition - Task - Core\nTo understand Spark processing, you have to look at how these three elements link together:\nPartition: A physical slice of your data (e.g., 10,000 rows of a CSV).\nTask: A single unit of work (e.g., \"convert these strings to uppercase\").\nCore (Slot): A CPU thread on a Worker node.\nThe Rule: One Task processes exactly one Partition on one Core at a time.\n\n2. Parallelism vs. Partitioning\nIf you have a 1GB file and only 1 partition, \nSpark can only use 1 CPU core to process it—even if your cluster has 100 computers! The other 99 computers will sit idle.\nUnder-partitioning: Too few partitions = Low parallelism (slow).\nOver-partitioning: Too many partitions = High overhead (Spark spends more time managing tasks than actually processing data).\n\n3. How Spark decides the number of Partitions?\nSpark sets the initial number of partitions based on where the data comes from:\nFrom HDFS/S3: Usually based on the file \"blocks\" (defaulting to 128MB per partition).\nFrom Local Collections: If you use sc.parallelize(data), it usually defaults to the total number of cores in your cluster.\nFrom Shuffles: When you do a join or groupBy, Spark defaults to 200 partitions (this is a configurable setting called spark.sql.shuffle.partitions).\n\n4. What happens during processing?\nWhen you write code like rdd.map(lambda x: x + 1), Spark doesn't send the data to the code; it sends the code to the data.\nThe Driver sends the \"Map\" function to every Executor.\nEach Executor applies that function to the partitions it has in its local memory.\nBecause each partition is independent, they can all be processed at the exact same time without talking to each other.\n\n\n\n"}]},{"id":"node-1768388430825","position":{"x":-322.98431396484375,"y":199.42774963378906},"size":{"width":276.8353500366211,"height":178.64666748046875},"type":"Container","title":"Spark SQL","description":"Spark SQL is Spark’s package for working with structured data. \nIt allows querying data via SQL as well as the Apache Hive variant of SQL—called the Hive Query Lan‐guage (HQL)—and it supports many sources of data, including Hive tables, Parquet,and JSON. \nBeyond providing a SQL interface to Spark, Spark SQL allows developers to intermix SQL queries with the programmatic data manipulations supported by RDDs in Python, Java, and Scala, all within a single application, thus combining SQL with complex analytics.","pan":{"x":-570.6707013924719,"y":-421.75499539250563,"scale":0.11518724582507835},"children":[{"id":"node-1768503338467","position":{"x":10,"y":10},"size":{"width":450,"height":300},"type":"Container","title":"Dataframes","pan":{"x":-3.6363635575475737,"y":-7.272727115095714},"children":[{"id":"node-1768504933261","position":{"x":10,"y":10},"size":{"width":859.6336059570312,"height":534.0027847290039},"type":"CodeBox","content":"# Reading a single Parquet file or a folder of Parquet files\ndf_parquet = spark.read.parquet(\"path/to/data.parquet\")\n\n# Viewing the schema and data\ndf_parquet.printSchema()\ndf_parquet.show(5)\n\n//csv\ndf_csv = spark.read.format(\"csv\") \\\n    .option(\"header\", \"true\") \\\n    .option(\"inferSchema\", \"true\") \\\n    .option(\"sep\", \",\") \\\n    .load(\"path/to/data.csv\")\n\n# Standard JSON (one JSON object per line)\ndf_json = spark.read.json(\"path/to/data.json\")\n\n# If your JSON file is a single array or pretty-printed across multiple lines\ndf_json_multiline = spark.read.option(\"multiLine\", \"true\").json(\"path/to/data.json\")\n\n# AVRO\ndf_avro = spark.read.format(\"avro\").load(\"path/to/data.avro\")\n\n# Text files\ndf_text = spark.read.text(\"path/to/logfile.txt\")\n\n# Example of filtering lines that contain 'ERROR'\nerrors = df_text.filter(df_text.value.contains(\"ERROR\"))\n\n\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n\n# Define the structure\nmanual_schema = StructType([\n    StructField(\"user_id\", IntegerType(), True),\n    StructField(\"user_name\", StringType(), True),\n    StructField(\"salary\", DoubleType(), True)\n])\n\n# Apply it while reading\ndf = spark.read.format(\"csv\") \\\n    .schema(manual_schema) \\\n    .option(\"header\", \"true\") \\\n    .load(\"path/to/data.csv\")\n\n\n"}]}]},{"id":"node-1768388535067","position":{"x":25.621116638183594,"y":196.03311157226562},"size":{"width":283.84828186035156,"height":186.39199829101562},"type":"Container","title":"Spark Streaming","description":"Spark Streaming is a Spark component that enables processing of live streams of data.\nExamples: logfiles generated by production web servers, or queues of messages containing status updates posted by users of a web service. \nSpark Streaming provides an API for manipulating data streams that closely matches the Spark Core’s RDD API, making it easy for programmers to learn the project and move between applications that manipulate data stored in memory, on disk, or arriving in real time. \nUnderneath its API, Spark Streaming was designed to provide the same degree of fault tolerance, throughput, and scalability as Spark Core.\n\n"},{"id":"node-1768388535701","position":{"x":411.8883361816406,"y":198.8733367919922},"size":{"width":278.16790771484375,"height":178.11936950683594},"type":"Container","title":"MLlib","description":"Spark comes with a library containing common machine learning (ML) functionality, called MLlib. \nMLlib provides multiple types of machine learning algorithms, including classification, regression, clustering, and collaborative filtering, as well as supporting functionality such as model evaluation and data import. \nIt also provides some lower-level ML primitives, including a generic gradient descent optimization algorithm. \nAll of these methods are designed to scale out across a cluster.\n\n"},{"id":"node-1768388536134","position":{"x":799.5757751464844,"y":197.4532470703125},"size":{"width":274.37603759765625,"height":178.51351928710938},"type":"Container","title":"GraphX","description":"GraphX is a library for manipulating graphs (e.g., a social network’s friend graph) and performing graph-parallel computations. \nLike Spark Streaming and Spark SQL, GraphX extends the Spark RDD API, allowing us to create a directed graph with arbitrary properties attached to each vertex and edge. \nGraphX also provides various operators for manipulating graphs (e.g., subgraph and mapVertices) and a library of common graph algorithms (e.g., PageRank and triangle counting)."}]}]}}