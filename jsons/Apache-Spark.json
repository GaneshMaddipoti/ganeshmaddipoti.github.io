{"id":null,"name":"Apache-Spark","data":{"id":"root","size":{"width":1571,"height":905},"pan":{"x":1540.8525653564188,"y":1010.7441797453273,"scale":426.6247296777573},"children":[{"id":"node-1768386812421","position":{"x":1354.2491455078125,"y":763.9320678710938},"size":{"width":458.478515625,"height":300},"type":"Container","title":"Apache Spark","description":"Apache Spark is a unified analytics engine for large-scale data processing. (Cluster computing platform)\nAt its core, Spark is a “computational engine” that is responsible for scheduling, distributing, and monitoring applications consisting of many computational tasks across many worker machines, or a computing cluster. \nIf Hadoop MapReduce was the \"first generation\" of big data processing, Spark is the \"second generation\"—built for speed, ease of use, and versatility.\n\nFeatures\nEasy to use: we can develop applications, using a high-level API that let us focus on the content of our computation\nSpeed: It is up to 100x faster than Hadoop MapReduce because it processes data in-memory (RAM) rather than writing to the disk after every step.\nLazy Evaluation: Spark doesn't execute your code immediately. It builds a plan (called a DAG or Directed Acyclic Graph) and only runs it when you finally ask for a result. This allows it to optimize the entire process first.\nAs general engine: letting you combine multiple types of computations (e.g., SQL queries, text process‐ing, and machine learning) that might previously have required different engines.\n\nPolyglot: You can write Spark code in Python (PySpark), Scala, Java, or R.\n\n\n\n\n\n\n\n","pan":{"x":-460.84062555276887,"y":-319.4435672790166,"scale":0.27835478184371387},"children":[{"id":"node-1768387001355","position":{"x":-149.17035293579102,"y":-287.57633113861084},"size":{"width":861.72607421875,"height":356.93047600984573},"type":"Container","title":"Architecture","description":"Spark operates on a Master-Slave architecture. It doesn't usually store data itself; instead, it reaches out to data sources (like S3 or HDFS), pulls the data into memory, and processes it across a cluster of computers.","pan":{"x":-752.4942311949649,"y":-320.2812295443693,"scale":0.3027063970070796},"children":[{"id":"node-1768387053759","position":{"x":-727.5428466796875,"y":-2.1067922115325928},"size":{"width":565.7882995605469,"height":367.23626708984375},"type":"Container","title":"Driver","description":"Driver Program: The \"brain\" of your application. It runs your main() function and converts your code into tasks.\nAt a high level, every Spark application consists of a driver program that launches various parallel operations on a cluster. \nThe driver program contains your application’s main function and defines distributed datasets on the cluster, then applies operations to them.\nDriver programs access Spark through a SparkContext object, which represents a connection to a computing cluster.\n\n\nResponsibilities\n1) Maintains the SparkContext/SparkSession: It is the entry point of the application.\n2) Analyzes Code: it converts your high-level code (SQL, Python, Scala) into a logical plan.\n3) Creates the DAG: It builds a Directed Acyclic Graph (DAG) of all the transformations you want to perform.\n4) Schedules Tasks: It breaks the DAG into \"Stages\" and then into \"Tasks,\" which it sends to the Executors (worker nodes) to be processed.\n\n","children":[{"id":"node-1768400322044","position":{"x":524.2629699707031,"y":77.6949462890625},"size":{"width":600.8836517333984,"height":461.4035415649414},"type":"CodeBox","content":"from pyspark.sql import SparkSession\n\n# 1. The Driver starts and creates the SparkSession\nspark = SparkSession.builder.appName(\"LogAnalysis\").get_home()\n\n# 2. The Driver creates a plan to read the data (Lazy Evaluation)\nlogs = spark.read.text(\"s3://my-bucket/logs/*.txt\")\n\n# 3. The Driver plans the transformation (Filtering)\naws_logs = logs.filter(logs.value.contains(\"AWS\"))\n\n# 4. The Driver triggers an 'Action' (.count())\n# This is where the Driver converts the DAG into actual tasks\nresult = aws_logs.count()\n\n# 5. The Driver receives the final number from the executors and prints it\nprint(f\"Total AWS entries: {result}\")\n\nspark.stop()"}]},{"id":"node-1768387067534","position":{"x":-78.00450134277344,"y":-70.75757789611816},"size":{"width":883.8558349609375,"height":552.2539672851562},"type":"Container","title":"Cluster Manager","description":"Cluster Manager: The \"allocator\" (e.g., YARN, Kubernetes, or Spark Standalone) that manages resources and decides which machines do what.\nUnder the hood, Spark is designed to efficiently scale up from one to many thousands of compute nodes. \nTo achieve this while maximizing flexibility, Spark can run over a variety of cluster managers, including Hadoop YARN, Apache Mesos, and a simple\ncluster manager included in Spark itself called the Standalone Scheduler."},{"id":"node-1768387081204","position":{"x":1275.6825561523438,"y":401.58311462402344},"size":{"width":562.68115234375,"height":364.18634033203125},"type":"Container","title":"Executor","pan":{"x":-2543.0732062324178,"y":-1567.0243553749474,"scale":0.18301617063661463}},{"id":"node-1768387081721","position":{"x":1632.4658203125,"y":38.89530944824219},"size":{"width":560.2086181640625,"height":345.6688690185547},"type":"Container","title":"Executor"},{"id":"node-1768387082055","position":{"x":1248.9713592529297,"y":-310.5077381134033},"size":{"width":552.4395751953125,"height":338.85069465637207},"type":"Container","title":"Executor","description":"Executors: The \"workers\" that live on the worker nodes. They execute the tasks assigned by the driver and store data in memory or disk."},{"id":"path-1768387468528","type":"Path","startElement":"node-1768387053759","endElement":"node-1768387067534"},{"id":"path-1768387476110","type":"Path","startElement":"node-1768387067534","endElement":"node-1768387082055"},{"id":"path-1768387478644","type":"Path","startElement":"node-1768387067534","endElement":"node-1768387081721"},{"id":"path-1768387480407","type":"Path","startElement":"node-1768387067534","endElement":"node-1768387081204"}]},{"id":"node-1768388409134","position":{"x":166.90554809570312,"y":533.8389129638672},"size":{"width":229.0255889892578,"height":135.33189392089844},"type":"Container","title":"Spark Core","description":"Spark Core contains the basic functionality of Spark, \nincluding components for task scheduling, memory management, fault recovery, interacting with storage systems, and more. \nSpark Core is also home to the API that defines resilient distributed datasets (RDDs), which are Spark’s main programming abstraction. \nRDDs represent a collection of items distributed across many compute nodes that can be manipulated in parallel. \n\n\n\n","pan":{"x":-772.1007246762396,"y":-456.2365883623364,"scale":0.11438396227480506},"children":[{"id":"node-1768479286026","position":{"x":-630.7999114990234,"y":-261.5486946105957},"size":{"width":485.0409698486328,"height":298.73345947265625},"type":"Container","title":"RDD","description":"An RDD in Spark is simply an immutable distributed collection of objects. \nEach RDD is split into multiple partitions, which may be computed on different nodes of the cluster. \nRDDs can contain any type of Python, Java, or Scala objects, including user-defined classes.\nIn Spark all work is expressed as either creating new RDDs, transforming existing RDDs, or calling operations on RDDs to compute a result. \nUnder the hood, Spark automatically distributes the data contained in RDDs across your cluster and parallelizes the operations you perform on them.\n\nUsers create RDDs in two ways: \n1) by loading an external dataset, or \n2) by distributing a collection of objects (e.g., a list or set) in their driver program.\n\nOnce created, RDDs offer two types of operations: \n1) transformations and \n2) actions.\nTransformations construct a new RDD from a previous one.\nActions, on the other hand, compute a result based on an RDD, and either return it to the driver program or save it to an external storage system.\nReturn type: transformations return RDDs, whereas actions return some other data type.\n\nTo summarize, every Spark program and shell session will work as follows:\n1. Create some input RDDs from external data.\n2. Transform them to define new RDDs using transformations like filter().\n3. Ask Spark to persist() any intermediate RDDs that will need to be reused.\n4. Launch actions such as count() and first() to kick off a parallel computation, which is then optimized and executed by Spark.\n\n\n\n","pan":{"x":-906.0429460640464,"y":-558.0257350074504,"scale":0.17433922005000008},"children":[{"id":"node-1768487419660","position":{"x":-107.8808364868164,"y":28.793411254882812},"size":{"width":450,"height":300},"type":"Container","title":"Input RDD","description":"\n","pan":{"x":-24.35805529465864,"y":-21.986260589749634,"scale":0.4725559294495047},"children":[{"id":"node-1768502562802","position":{"x":10,"y":10},"size":{"width":866.6438446044922,"height":538.4972610473633},"type":"CodeBox","content":"#local collection\ndata = [1, 2, 3, 4, 5]\nrdd = sc.parallelize(data)\n\n# Read a single file, a directory, or a wildcard pattern\nrdd_text = sc.textFile(\"path/to/data.txt\")\n\n# You can also specify the minimum number of partitions\nrdd_partitioned = sc.textFile(\"path/to/data.txt\", minPartitions=4)\n\n# Useful for processing a directory of small logs\nrdd_whole = sc.wholeTextFiles(\"path/to/directory/\")\n# Result: [('file1.txt', 'content...'), ('file2.txt', 'content...')]\n\n#from json\nrdd_raw_json = sc.textFile(\"data.json\")\nrdd_parsed_json = rdd_raw_json.map(lambda line: json.loads(line))\n\n#from csv\nrdd_raw_csv = sc.textFile(\"data.csv\")\ndef parse_csv(line):\n    # StringIO allows the csv module to treat a string like a file\n    f = StringIO(line)\n    reader = csv.reader(f)\n    return next(reader)\nrdd_csv = rdd_raw_csv.map(parse_csv)\n\n#binary files\nrdd_binary = sc.binaryFiles(\"path/to/images/\")\n\n\n\n"}]},{"id":"node-1768487423102","position":{"x":477.8385009765625,"y":24.395034790039062},"size":{"width":450,"height":300},"type":"Container","title":"Transformations","children":[{"id":"node-1768505189295","position":{"x":63.63458251953125,"y":55.20872497558594},"size":{"width":748.7021331787109,"height":434.9215316772461},"type":"CodeBox","content":"#map: Transforms each element into exactly one new element.\n#flatMap: Similar to map, but each input item can be mapped to 0 or more output items.\n\n# Example: Converting text to uppercase\nupper_rdd = rdd.map(lambda line: line.upper())\n\n# Example: Splitting lines into individual words\nwords_rdd = rdd.flatMap(lambda line: line.split(\" \"))\n\n# Keep only lines that contain the word 'Spark'\nfiltered_rdd = rdd.filter(lambda line: \"Spark\" in line)\n\n#Many RDD operations work on \"Pair RDDs\" (where each element is a tuple (key, value)).\n#reduceByKey: Merges values for each key using a function. \n#It is highly efficient because it performs a local combine before shuffling data.\n#groupByKey: Groups all values for a key into a list. \n#Avoid this on large datasets as it can cause \"Out of Memory\" errors.\nword_pairs = words_rdd.map(lambda word: (word, 1))\nword_counts = word_pairs.reduceByKey(lambda a, b: a + b)\n\n\n\n\n\n\n\n"}]},{"id":"node-1768487423523","position":{"x":-700.7673034667969,"y":25.114788055419922},"size":{"width":450,"height":300},"type":"Container","title":"Source Data","description":"1. The Unified Data Source API\nOne of Spark’s greatest strengths is the DataFrameReader API. \nIt provides a consistent interface for loading data, regardless of the format. The standard syntax follows this pattern:\n\nspark.read.format(\"format\") \\\n    .option(\"key\", \"value\") \\\n    .load(\"path\")\n\n2. Common Built-in File Formats\nSpark comes with native support for the most popular data storage formats used in Big Data.\nFormat\tBest Use Case\t          Key Features\nParquet\tProduction Data Lakes\t Columnar storage, highly efficient compression, supports schema evolution. (Spark's default).\nCSV\t        Simple Data Exchange\t Human-readable, but lacks schema information and is slower to parse.\nJSON\tSemi-structured Data\t Good for nested data; Spark can automatically infer the schema from the file.\nORC\t        Hive Integration\t         Optimized Row Columnar; similar to Parquet but often used in Hadoop/Hive ecosystems.\nAvro\tData Serialization\t         Row-based, excellent for write-heavy workloads and schema evolution.\nText \tLog Processing\t         Loads each line as a single string column named value.\n\n3. External Connectors (The Ecosystem)\nBeyond files, Spark uses connectors (often requiring specific JAR files) to pull data from:\nRelational Databases (JDBC): Connect to MySQL, PostgreSQL, Oracle, or SQL Server.\nNoSQL Databases: MongoDB, Cassandra, CosmosDB, and HBase.\nCloud Storage: Integration with AWS S3 (s3a://), Azure Blob Storage/ADLS (abfss://), and Google Cloud Storage (gs://).\nModern Table Formats: Support for Delta Lake, Apache Iceberg, and Apache Hudi, which bring ACID transactions to your data lake.\n\n4. Streaming Sources\nFor Spark Structured Streaming, the sources are slightly different because they must provide a continuous flow of data:\nApache Kafka: The most common source for real-time event streaming.\nAmazon Kinesis: Managed streaming service on AWS.\nFile Source: Monitors a directory for new files as they arrive.\nSocket Source: (Primarily for testing) Reads UTF-8 text data from a network socket.\n\n5. Key Considerations when Loading Data\nWhen preparing your content, it’s helpful to mention these three concepts that impact performance:\nSchema Inference: Spark can try to \"guess\" the data types (inferSchema=\"true\"), but for production, you should explicitly define the schema to save processing time and ensure data quality.\nPartition Discovery: If data is stored in folders like /year=2024/month=01/, Spark automatically recognizes these as columns.\nData Locality: Spark tries to read data from the node where it is physically stored to minimize network traffic.\n\n"},{"id":"node-1768487423856","position":{"x":1068.2027893066406,"y":30.033096313476562},"size":{"width":450,"height":300},"type":"Container","title":"Actions"},{"id":"path-1768487448169","type":"Path","startElement":"node-1768487423523","endElement":"node-1768487419660"},{"id":"path-1768487449581","type":"Path","startElement":"node-1768487419660","endElement":"node-1768487423102"},{"id":"path-1768487450985","type":"Path","startElement":"node-1768487423102","endElement":"node-1768487423856"}]}]},{"id":"node-1768388430825","position":{"x":-322.98431396484375,"y":199.42774963378906},"size":{"width":276.8353500366211,"height":178.64666748046875},"type":"Container","title":"Spark SQL","description":"Spark SQL is Spark’s package for working with structured data. \nIt allows querying data via SQL as well as the Apache Hive variant of SQL—called the Hive Query Lan‐guage (HQL)—and it supports many sources of data, including Hive tables, Parquet,and JSON. \nBeyond providing a SQL interface to Spark, Spark SQL allows developers to intermix SQL queries with the programmatic data manipulations supported by RDDs in Python, Java, and Scala, all within a single application, thus combining SQL with complex analytics.","pan":{"x":-570.6707013924719,"y":-421.75499539250563,"scale":0.11518724582507835},"children":[{"id":"node-1768503338467","position":{"x":10,"y":10},"size":{"width":450,"height":300},"type":"Container","title":"Dataframes","pan":{"x":-3.6363635575475737,"y":-7.272727115095714},"children":[{"id":"node-1768504933261","position":{"x":10,"y":10},"size":{"width":859.6336059570312,"height":534.0027847290039},"type":"CodeBox","content":"# Reading a single Parquet file or a folder of Parquet files\ndf_parquet = spark.read.parquet(\"path/to/data.parquet\")\n\n# Viewing the schema and data\ndf_parquet.printSchema()\ndf_parquet.show(5)\n\n//csv\ndf_csv = spark.read.format(\"csv\") \\\n    .option(\"header\", \"true\") \\\n    .option(\"inferSchema\", \"true\") \\\n    .option(\"sep\", \",\") \\\n    .load(\"path/to/data.csv\")\n\n# Standard JSON (one JSON object per line)\ndf_json = spark.read.json(\"path/to/data.json\")\n\n# If your JSON file is a single array or pretty-printed across multiple lines\ndf_json_multiline = spark.read.option(\"multiLine\", \"true\").json(\"path/to/data.json\")\n\n# AVRO\ndf_avro = spark.read.format(\"avro\").load(\"path/to/data.avro\")\n\n# Text files\ndf_text = spark.read.text(\"path/to/logfile.txt\")\n\n# Example of filtering lines that contain 'ERROR'\nerrors = df_text.filter(df_text.value.contains(\"ERROR\"))\n\n\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n\n# Define the structure\nmanual_schema = StructType([\n    StructField(\"user_id\", IntegerType(), True),\n    StructField(\"user_name\", StringType(), True),\n    StructField(\"salary\", DoubleType(), True)\n])\n\n# Apply it while reading\ndf = spark.read.format(\"csv\") \\\n    .schema(manual_schema) \\\n    .option(\"header\", \"true\") \\\n    .load(\"path/to/data.csv\")\n\n\n"}]}]},{"id":"node-1768388535067","position":{"x":25.621116638183594,"y":196.03311157226562},"size":{"width":283.84828186035156,"height":186.39199829101562},"type":"Container","title":"Spark Streaming","description":"Spark Streaming is a Spark component that enables processing of live streams of data.\nExamples: logfiles generated by production web servers, or queues of messages containing status updates posted by users of a web service. \nSpark Streaming provides an API for manipulating data streams that closely matches the Spark Core’s RDD API, making it easy for programmers to learn the project and move between applications that manipulate data stored in memory, on disk, or arriving in real time. \nUnderneath its API, Spark Streaming was designed to provide the same degree of fault tolerance, throughput, and scalability as Spark Core.\n\n"},{"id":"node-1768388535701","position":{"x":411.8883361816406,"y":198.8733367919922},"size":{"width":278.16790771484375,"height":178.11936950683594},"type":"Container","title":"MLlib","description":"Spark comes with a library containing common machine learning (ML) functionality, called MLlib. \nMLlib provides multiple types of machine learning algorithms, including classification, regression, clustering, and collaborative filtering, as well as supporting functionality such as model evaluation and data import. \nIt also provides some lower-level ML primitives, including a generic gradient descent optimization algorithm. \nAll of these methods are designed to scale out across a cluster.\n\n"},{"id":"node-1768388536134","position":{"x":799.5757751464844,"y":197.4532470703125},"size":{"width":274.37603759765625,"height":178.51351928710938},"type":"Container","title":"GraphX","description":"GraphX is a library for manipulating graphs (e.g., a social network’s friend graph) and performing graph-parallel computations. \nLike Spark Streaming and Spark SQL, GraphX extends the Spark RDD API, allowing us to create a directed graph with arbitrary properties attached to each vertex and edge. \nGraphX also provides various operators for manipulating graphs (e.g., subgraph and mapVertices) and a library of common graph algorithms (e.g., PageRank and triangle counting)."}]}]}}