{"id":null,"name":"Apache-Spark","data":{"id":"root","size":{"width":1571,"height":905},"pan":{"x":1437.4828975085861,"y":795.452342706491,"scale":20.94220636735018},"children":[{"id":"node-1768386812421","position":{"x":1354.2491455078125,"y":763.9832153320312},"size":{"width":458.478515625,"height":300},"type":"Container","title":"Apache Spark","description":"Apache Spark is a unified analytics engine for large-scale data processing. (Cluster computing platform)\nAt its core, Spark is a “computational engine” that is responsible for scheduling, distributing, and monitoring applications consisting of many computational tasks across many worker machines, or a computing cluster. \nIf Hadoop MapReduce was the \"first generation\" of big data processing, Spark is the \"second generation\"—built for speed, ease of use, and versatility.\n\nFeatures\nEasy to use: we can develop applications, using a high-level API that let us focus on the content of our computation\nSpeed: It is up to 100x faster than Hadoop MapReduce because it processes data in-memory (RAM) rather than writing to the disk after every step.\nLazy Evaluation: Spark doesn't execute your code immediately. It builds a plan (called a DAG or Directed Acyclic Graph) and only runs it when you finally ask for a result. This allows it to optimize the entire process first.\nAs general engine: letting you combine multiple types of computations (e.g., SQL queries, text process‐ing, and machine learning) that might previously have required different engines.\n\nPolyglot: You can write Spark code in Python (PySpark), Scala, Java, or R.\n\n\n\n\n\n\n\n","pan":{"x":-460.84062555276887,"y":-319.4435672790166,"scale":0.27835478184371387},"children":[{"id":"node-1768387001355","position":{"x":-149.17035293579102,"y":-287.57633113861084},"size":{"width":861.72607421875,"height":356.93047600984573},"type":"Container","title":"Architecture","description":"Spark operates on a Master-Slave architecture. It doesn't usually store data itself; instead, it reaches out to data sources (like S3 or HDFS), pulls the data into memory, and processes it across a cluster of computers.","pan":{"x":-752.4942311949649,"y":-320.2812295443693,"scale":0.3027063970070796},"children":[{"id":"node-1768387053759","position":{"x":-723.7504272460938,"y":-14.748149633407593},"size":{"width":565.7882995605469,"height":367.23626708984375},"type":"Container","title":"Driver","description":"Driver Program: The \"brain\" of your application. It runs your main() function and converts your code into tasks.\nAt a high level, every Spark application consists of a driver program that launches various parallel operations on a cluster. \nThe driver program contains your application’s main function and defines distributed datasets on the cluster, then applies operations to them.\nDriver programs access Spark through a SparkContext object, which represents a connection to a computing cluster.\n\n"},{"id":"node-1768387067534","position":{"x":-78.00450134277344,"y":-70.75757789611816},"size":{"width":883.8558349609375,"height":552.2539672851562},"type":"Container","title":"Cluster Manager","description":"Cluster Manager: The \"allocator\" (e.g., YARN, Kubernetes, or Spark Standalone) that manages resources and decides which machines do what.\nUnder the hood, Spark is designed to efficiently scale up from one to many thousands of compute nodes. \nTo achieve this while maximizing flexibility, Spark can run over a variety of cluster managers, including Hadoop YARN, Apache Mesos, and a simple\ncluster manager included in Spark itself called the Standalone Scheduler."},{"id":"node-1768387081204","position":{"x":1275.6825561523438,"y":401.58311462402344},"size":{"width":562.68115234375,"height":364.18634033203125},"type":"Container","title":"Executor","pan":{"x":-2543.0732062324178,"y":-1567.0243553749474,"scale":0.18301617063661463}},{"id":"node-1768387081721","position":{"x":1632.4658203125,"y":38.89530944824219},"size":{"width":560.2086181640625,"height":345.6688690185547},"type":"Container","title":"Executor"},{"id":"node-1768387082055","position":{"x":1248.9713592529297,"y":-310.5077381134033},"size":{"width":552.4395751953125,"height":338.85069465637207},"type":"Container","title":"Executor","description":"Executors: The \"workers\" that live on the worker nodes. They execute the tasks assigned by the driver and store data in memory or disk."},{"id":"path-1768387468528","type":"Path","startElement":"node-1768387053759","endElement":"node-1768387067534"},{"id":"path-1768387476110","type":"Path","startElement":"node-1768387067534","endElement":"node-1768387082055"},{"id":"path-1768387478644","type":"Path","startElement":"node-1768387067534","endElement":"node-1768387081721"},{"id":"path-1768387480407","type":"Path","startElement":"node-1768387067534","endElement":"node-1768387081204"}]},{"id":"node-1768388409134","position":{"x":166.90554809570312,"y":533.8389129638672},"size":{"width":229.0255889892578,"height":135.5474090576172},"type":"Container","title":"Spark Core","description":"Spark Core contains the basic functionality of Spark, \nincluding components for task scheduling, memory management, fault recovery, interacting with storage systems, and more. \nSpark Core is also home to the API that defines resilient distributed datasets (RDDs), which are Spark’s main programming abstraction. \nRDDs represent a collection of items distributed across many compute nodes that can be manipulated in parallel. \n\n\n\n"},{"id":"node-1768388430825","position":{"x":-322.98431396484375,"y":199.42774963378906},"size":{"width":276.8353500366211,"height":178.64666748046875},"type":"Container","title":"Spark SQL","description":"Spark SQL is Spark’s package for working with structured data. \nIt allows querying data via SQL as well as the Apache Hive variant of SQL—called the Hive Query Lan‐guage (HQL)—and it supports many sources of data, including Hive tables, Parquet,and JSON. \nBeyond providing a SQL interface to Spark, Spark SQL allows developers to intermix SQL queries with the programmatic data manipulations supported by RDDs in Python, Java, and Scala, all within a single application, thus combining SQL with complex analytics."},{"id":"node-1768388535067","position":{"x":25.621116638183594,"y":196.03311157226562},"size":{"width":283.84828186035156,"height":186.39199829101562},"type":"Container","title":"Spark Streaming","description":"Spark Streaming is a Spark component that enables processing of live streams of data.\nExamples: logfiles generated by production web servers, or queues of messages containing status updates posted by users of a web service. \nSpark Streaming provides an API for manipulating data streams that closely matches the Spark Core’s RDD API, making it easy for programmers to learn the project and move between applications that manipulate data stored in memory, on disk, or arriving in real time. \nUnderneath its API, Spark Streaming was designed to provide the same degree of fault tolerance, throughput, and scalability as Spark Core.\n\n"},{"id":"node-1768388535701","position":{"x":411.8883361816406,"y":198.8733367919922},"size":{"width":278.16790771484375,"height":178.11936950683594},"type":"Container","title":"MLlib","description":"Spark comes with a library containing common machine learning (ML) functionality, called MLlib. \nMLlib provides multiple types of machine learning algorithms, including classification, regression, clustering, and collaborative filtering, as well as supporting functionality such as model evaluation and data import. \nIt also provides some lower-level ML primitives, including a generic gradient descent optimization algorithm. \nAll of these methods are designed to scale out across a cluster.\n\n"},{"id":"node-1768388536134","position":{"x":799.5757751464844,"y":197.4532470703125},"size":{"width":274.37603759765625,"height":178.51351928710938},"type":"Container","title":"GraphX","description":"GraphX is a library for manipulating graphs (e.g., a social network’s friend graph) and performing graph-parallel computations. \nLike Spark Streaming and Spark SQL, GraphX extends the Spark RDD API, allowing us to create a directed graph with arbitrary properties attached to each vertex and edge. \nGraphX also provides various operators for manipulating graphs (e.g., subgraph and mapVertices) and a library of common graph algorithms (e.g., PageRank and triangle counting)."}]}]}}