[{"id":"Text-1759066374728","text":"Apache Spark","x":174.88888931274414,"y":114.33333349227905,"width":144.9375,"height":24.9375,"bgColor":"#E0E0E0"},{"id":"Note-1759067336405","text":"<div>Apache Spark is open source, distributed, unified analytics engine for fast, large-scale data processing.</div><div>Spark provides in-memory storage for intermediate computations, making it much faster than Hadoop MapReduce\n</div><div>Run programs upto 100* faster than Hadoop in memory, and 10* faster on disk.</div><div><div>Features native support for SQL, Streaming, ML, and Graph processing through consistent API</div></div><div>DAG(Dircted acyclic graph) engine which will optimize workflows.</div><div>Operates seamlessly with diverse data sources inluding cloud storage, local files, and databases.</div><div>Spark’s design philosophy centers around four key characteristics:\n• Speed\n• Ease of use\n• Modularity </div><div>• Extensibility</div>","x":175.88888931274414,"y":163.33333349227905,"width":846,"height":225.96875,"bgColor":"wheat"},{"id":"Container-1759067422267","elements":[{"id":"Text-1759067429560","text":"Architecture","x":2,"y":2.6666667461395264,"width":118.90625,"height":24.9375,"bgColor":"#E0E0E0"},{"id":"Node-1759067448094","text":"Driver Program\n(SparkSession/\nSpakContext)","x":96,"y":135.66666674613953,"width":143.9375,"height":105,"bgColor":"#F0F0F0"},{"id":"Node-1759067474223","text":"Cluster Manager\n(Spark / YARN)","x":442,"y":160.66666674613953,"width":180,"height":54.9375,"bgColor":"#F0F0F0"},{"id":"Node-1759067521911","text":"Worker Node\nExecutor\n(cache)\n(task)","x":742,"y":22.666666746139526,"width":135,"height":75.9375,"bgColor":"#F0F0F0"},{"id":"Node-1759067569427","text":"Worker Node\nExecutor\n(cache)\n(task)","x":746,"y":141.66666674613953,"width":122.9375,"height":78.9375,"bgColor":"#F0F0F0"},{"id":"Node-1759067579230","text":"Worker Node\nExecutor\n(cache)\n(task)","x":737,"y":274.6666667461395,"width":112.96875,"height":86.9375,"bgColor":"#F0F0F0"}],"x":167,"y":498.6666667461395,"width":893.90625,"height":388.90625,"bgColor":"#F0F0F0"},{"id":"Container-1759068506983","elements":[{"id":"Text-1759068521872","text":"Spark Components","x":2,"y":5,"width":175.90625,"height":22.96875,"bgColor":"#E0E0E0"},{"id":"Node-1759068548800","text":"Spark Core\n- Python\n- Scala/Java","x":284,"y":368,"width":306.96875,"height":72,"bgColor":"#F0F0F0"},{"id":"Node-1759068555698","text":"Spark Streaming\n- Python\n- Scala/Java","x":147,"y":23,"width":168.9375,"height":65.9375,"bgColor":"#F0F0F0"},{"id":"Node-1759068570505","text":"Spark SQL\n- SQL","x":208,"y":105,"width":124.90625,"height":60,"bgColor":"#F0F0F0"},{"id":"Node-1759068579108","text":"MLLib\n- Python\n-Scala/Java","x":488,"y":8,"width":127.96875,"height":61.90625,"bgColor":"#F0F0F0"},{"id":"Node-1759068596963","text":"GraphX\n- Scala/Java","x":541,"y":130,"width":135.9375,"height":58.90625,"bgColor":"#F0F0F0"},{"id":"Node-1761504385727","text":"Spark Dataframe / Dataset\n- Python\n- Scala/Java","x":46.3333740234375,"y":197.8888931274414,"width":784.96875,"height":93.96875,"bgColor":"#F0F0F0"},{"id":"Node-1761504539677","text":"SparkR\n- R","x":501.3333740234375,"y":85.8888931274414,"width":106.96875,"height":47.9375,"bgColor":"#F0F0F0"}],"x":1120,"y":497,"width":871,"height":461,"bgColor":"#F0F0F0"},{"id":"Container-1759070178404","elements":[{"id":"Text-1759070026954","text":"RDD","x":2,"y":6,"width":99.9375,"height":30,"bgColor":"#E0E0E0"},{"id":"Note-1759070035342","text":"<div>RDD (Resilient Distributed Dataset) is the fundamental data structure — an immutable, distributed collection of objects that can be processed in parallel.</div><div>You can create an RDD from various sources — such as collections in memory, external data sources (HDFS, S3, local files), and other RDDs.</div><div>Transforming RDDs</div><div>- map / flatmap</div><div>- filter</div><div>- distinct</div><div>- sample</div><div>- union, intersection, subtract, cartesian</div><div>Aggregate operations (RDD Actions)</div><div>- count, countByValue, collect, take, top, reduce, etc...</div><div>Nothing happends in driver program until an actions is called</div>","x":19,"y":61,"width":738.96875,"height":330.96875,"bgColor":"wheat"},{"id":"Note-1759124046611","text":"Key-Value RDD<div>Just map paris of data into tuples </div><div>Ex: val result = source.map(x =&gt; (x, 1))</div><div>There are operations for Key-Value RDD</div><div>- reduceByKey((v1, v2) =&gt; (v1 + v2))</div><div>- groupByKey</div><div>- sortByKey</div><div>- keys(), values()</div><div>- join, rightOuterJoin, cogroup, subtractByKey</div>","x":23,"y":404.1111145019531,"width":603,"height":270,"bgColor":"wheat"},{"id":"Code-1761720133358","text":"<div># from memory\nfrom pyspark import SparkContext\nsc = SparkContext(\"local\", \"RDD Example\")\ndata = [1, 2, 3, 4, 5]\nrdd = sc.parallelize(data)\n\n# from file\ntext_rdd = sc.textFile(\"hdfs://namenode:9000/user/data/file.txt\")\n# or local\ntext_rdd = sc.textFile(\"file:///home/user/data.txt\")\n# multiple files\nrdd = sc.textFile(\"hdfs://namenode:9000/user/data/*.txt\")        \n#(filename, content) from directory\nwhole_rdd = sc.wholeTextFiles(\"hdfs://namenode:9000/user/data/\") \n\n# from other RDDs\nnumbers = sc.parallelize([1, 2, 3, 4, 5])\nsquared = numbers.map(lambda x: x ** 2)\nfiltered = squared.filter(lambda x: x &gt; 10)\n\n# from dataframes and datasets\ndf = spark.read.csv(\"data.csv\", header=True, inferSchema=True)\nrdd = df.rdd  # convert to RDD\n\n# from external databases\njdbc_df = spark.read.format(\"jdbc\").options(\n    url=\"jdbc:mysql://localhost:3306/testdb\",\n    driver=\"com.mysql.jdbc.Driver\",\n    dbtable=\"employees\",\n    user=\"root\",\n    password=\"password\"\n).load()\n\nrdd = jdbc_df.rdd\n\n</div><div><br></div>","x":772,"y":53,"width":970.96875,"height":603,"bgColor":"#555"}],"x":162.22222137451172,"y":4238.66667175293,"width":1765.96875,"height":678,"bgColor":"#F0F0F0"},{"id":"Container-1759070226869","elements":[{"id":"Text-1759070232579","text":"Spark Driver","x":0.22222137451171875,"y":2.5555572509765625,"width":135,"height":30.9375,"bgColor":"#E0E0E0"},{"id":"Note-1759070247024","text":"<div>Creates the SparkSession which is the entry point of execution</div><div>Analyses the Spark application and constructs DAG</div><div>Schedules and distributes the task to executors</div><div>Monitors the progress of execution and handles failures</div><div>Return the result to the client</div>\n<div>Creates RDDs</div><div>It is responsible for RDDs resilient and distributed</div>","x":12.222221374511719,"y":40.55555725097656,"width":573.96875,"height":195.96875,"bgColor":"wheat"}],"x":194.22222137451172,"y":980.5555572509766,"width":613.96875,"height":262.96875,"bgColor":"#F0F0F0"},{"id":"Container-1761505391017","elements":[{"id":"Text-1761505394883","text":"Executors","x":5.6666717529296875,"y":5.22222900390625,"width":115.90625,"height":30.9375,"bgColor":"#E0E0E0"},{"id":"Note-1761505406611","text":"Run on worker nodes in Spark cluster and host tasks (IO, Data).<div>Each worker node can run multiple executors based on \n - available CPU cores\n - memory\n - configuration settings</div><div>Store intermediate results in memory or disk</div><div>Interact with driver for task execution or data transfer</div><div><br></div>","x":14.666671752929688,"y":49.22222900390625,"width":478.96875,"height":180,"bgColor":"wheat"}],"x":843.6666717529297,"y":984.2222290039062,"width":520.96875,"height":244.96875,"bgColor":"#F0F0F0"},{"id":"Container-1761505667291","elements":[{"id":"Text-1761505675954","text":"Spark DAG","x":3,"y":2.81817626953125,"width":117.9375,"height":27.9375,"bgColor":"#E0E0E0"},{"id":"Note-1761505689576","text":"Spark jobs are broken down to stages - group of tasks that can run in parallel<div>Computations flow in one direction through the stags (Directed)</div><div>Stages will never loop back, ensuring the job terminates (Acyclic)</div><div>Stages are organized into a dependency graph for execution flow (Graph)</div><div>Tasks within a stage can run in parallel, known as Shared Nothing mode</div><div>Job -&gt; Stage1 -&gt; Stage 2\n             (Task1)    (Task1)\n             (Taks 2)    (Task 2)</div><div><br></div>","x":10,"y":35.81817626953125,"width":563.9375,"height":195,"bgColor":"wheat"}],"x":1435.272705078125,"y":986.8181762695312,"width":590.9375,"height":241.96875,"bgColor":"#F0F0F0"},{"id":"Container-1761506124548","elements":[{"id":"Text-1761506130026","text":"Spark UI","x":5.818115234375,"y":4.6363525390625,"width":99.9375,"height":30,"bgColor":"#E0E0E0"},{"id":"Note-1761506139626","text":"Spark provides web UI for monitoring and management<div>- Application UI \n  Per application / SparkSession\n  Application progress and task execution\n  DAG visualization\n  Resource usage and performace metrics</div><div>- Master UI\n  Per Cluster \n  Worker node status and health\n  Cluster wide runing applications and resources</div>","x":13.818115234375,"y":43.6363525390625,"width":422.9375,"height":212.9375,"bgColor":"wheat"}],"x":2056.818115234375,"y":984.6363525390625,"width":452.9375,"height":267.9375,"bgColor":"#F0F0F0"},{"id":"Container-1761508780765","elements":[{"id":"Text-1761508785421","text":"Spark Dataframe","x":9.5,"y":5,"width":185.9375,"height":32.9375,"bgColor":"#E0E0E0"},{"id":"Note-1761508803091","text":"Dataframes are distributed collection of records, all with same schema<div>Dataframes track their schema, and provide native support for SQL functions and joins</div><div>Dataframes are evaluatd as DAGs, using lazy evaluation, and providing lineage and fault tolerance</div><div>Dataframes can be created \n- from various sources CSV, JSON, Parquet, text, and Binary files\n- from Delta late and other table storage format directories\n- from tables and views in Unity catalog, external databases\n- from an existing RDD</div><div>Tungsten is Spark's columnar in-memory execution engine, which is key for optimization</div><div>Tungsten offers\n- off-heap memory managemnt\n- cache-aware computation\n- Code generatino for faster execution<br></div><div>In memory - columnar - Tungsten</div><div>On Disk - rows based (CSV) - columnar (Parquet)</div><div>Dataframes schema can be inferred from data, or explicitly mentioned(more efficient)</div><div>Dataframes are immutable </div><div>Transformations create new Dataframes from exisitng ones and are lazy operations(logical plan)\n- select, filter, withcolumn, groupby, agg</div><div>Actions initiate the actual exeuction and return result/write to external storage\n- count, show, take, first, write</div><div><br></div><div><br></div>","x":24.5,"y":60,"width":817,"height":453,"bgColor":"wheat"},{"id":"Code-1761742111426","text":"from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"App 1\").getOrCreate()\n\n# from in-memory\ndata = [(\"Alice\", 34), (\"Bob\", 45), (\"Cathy\", 29)]\ncolumns = [\"Name\", \"Age\"]\ndf = spark.createDataFrame(data, columns)\n\n# from RDD\nrdd = spark.sparkContext.parallelize(data)\ndf_from_rdd = rdd.toDF(columns)\n\n# from CSV\ndf_csv = spark.read.option(\"delimiter\", \",\") \\\n                   .option(\"header\", True) \\\n                   .option(\"inferSchema\", True) \\\n                   .csv(\"path/to/file.csv\")  #file:/s3a:/hdfs:\n\n# from JSON\ndf_json = spark.read.json(\"path/to/file.json\")\n\n# from Parquet\ndf_parquet = spark.read.parquet(\"path/to/file.parquet\")\n\n# from avro\ndf_avro = spark.read.format(\"avro\").load(\"path/to/file.avro\")\n\n# from orc\ndf_orc = spark.read.orc(\"path/to/file.orc\")\n\n# from database\ndf_jdbc = spark.read.format(\"jdbc\").options(\n    url=\"jdbc:mysql://localhost:3306/testdb\",\n    driver=\"com.mysql.cj.jdbc.Driver\",\n    dbtable=\"employees\",\n    user=\"root\",\n    password=\"password\"\n).load()\n\n# from kafka\ndf_kafka = spark.read \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n    .option(\"subscribe\", \"my_topic\") \\\n    .load()\n\n# from pandas dataframe\nimport pandas as pd\npdf = pd.DataFrame({\"name\": [\"John\", \"Jane\"], \"age\": [30, 25]})\n\ndf_spark = spark.createDataFrame(pdf)\n\n# from delta lake table\ndf_delta = spark.read.format(\"delta\").load(\"/mnt/delta/events/\")\n\n\n","x":906,"y":55.25,"width":925,"height":885,"bgColor":"#555"},{"id":"Image-1761842799448","x":24,"y":533,"width":806,"height":399,"bgColor":"#F0F0F0","imagePath":"https://raw.githubusercontent.com/GaneshMaddipoti/ganeshmaddipoti.github.io/30569c6dae19b72cf449d9bcab6d392638708a9b/assets/img/spark/SparkDataFrameCreate.png"}],"x":166.5,"y":3124,"width":1889,"height":1085,"bgColor":"#F0F0F0"},{"id":"Container-1761509806889","elements":[{"id":"Text-1761509813845","text":"Photon Engine","x":0.6666717529296875,"y":4,"width":154.90625,"height":33.9375,"bgColor":"#E0E0E0"},{"id":"Note-1761509828692","text":"Databricks native vectorized query engine<div>Processes data in batches rather than row by row for performance</div><div>Photon is by default enabled in SQL warehouses and serverless compute</div><div>Can be enabled in all purpose and job clusters</div>","x":19.666671752929688,"y":51,"width":557.9375,"height":125.9375,"bgColor":"wheat"}],"x":845.6666717529297,"y":1282,"width":590.9375,"height":196.90625,"bgColor":"#F0F0F0"},{"id":"Divider-1761564386985","text":"Divider","x":160,"y":406,"width":2307.9375,"height":22.96875,"bgColor":"#F0F0F0"},{"id":"Divider-1761564644303","text":"Divider","x":164,"y":1551,"width":2310.9375,"height":22.96875,"bgColor":"#F0F0F0"},{"id":"Container-1761804224918","elements":[{"id":"Text-1761804232244","text":"Cluser Manager","x":3.333333373069763,"y":4.33331298828125,"width":163.96875,"height":33,"bgColor":"#E0E0E0"},{"id":"Note-1761804244110","text":"The cluster manager is responsible for managing and allocating resources for the cluster of nodes on which your Spark application runs. <div>Currently, Spark supports four cluster managers: the built-in standalone cluster manager, Apache Hadoop YARN, Apache Mesos, and Kubernetes.</div>","x":16.333333373069763,"y":50.33331298828125,"width":534.96875,"height":130.96875,"bgColor":"wheat"}],"x":211.33333337306976,"y":1286.3333129882812,"width":570.96875,"height":202.96875,"bgColor":"#F0F0F0"},{"id":"Container-1761804508428","elements":[{"id":"Text-1761804513291","text":"Deployement Modes","x":8.333333373069763,"y":10.6666259765625,"width":229.96875,"height":36.96875,"bgColor":"#E0E0E0"},{"id":"Note-1761804525821","text":"Mode                                Spark driver                                                                                    Spark executor                                                                                      Cluster manager\nLocal                        Runs on a single JVM, like a laptop or single node                 Runs on the same JVM as the driver                                                         Runs on the same host\nStandalone             Can run on any node in the cluster                                             Each node in the cluster will launch its own executor JVM                  Can be allocated arbitrarily to any host in the cluster\nYARN (client)          Runs on a client, not part of the cluster                                    YARN’s NodeManager’s container                                                              YARN’s Resource Manager works with YARN’s Application Master to allocate the containers on NodeManagers for executors\nYARN(cluster)        Runs with the YARN Application Master                                    Same as YARN client mode                                                                        Same as YARN client mode\nKubernetes             Runs in a Kubernetes pod                                                            Each worker runs within its own pod                                                          Kubernetes Master","x":31.333333373069763,"y":73.6666259765625,"width":1962.96875,"height":156.96875,"bgColor":"wheat"}],"x":173.33333337306976,"y":1614.3333129882812,"width":2016.96875,"height":258,"bgColor":"#F0F0F0"},{"id":"Image-1761805750988","x":2035.3333740234375,"y":499,"width":682.96875,"height":415.96875,"bgColor":"#F0F0F0","imagePath":"https://raw.githubusercontent.com/GaneshMaddipoti/ganeshmaddipoti.github.io/refs/heads/main/assets/img/spark/SparkDataPartitions.png"},{"id":"Container-1761807275112","elements":[{"id":"Text-1761807283471","text":"Application Concepts","x":5,"y":7,"width":214.96875,"height":39,"bgColor":"#E0E0E0"},{"id":"Note-1761807297770","text":"Application   -    A user program built on Spark using its APIs. It consists of a driver program and executors on the cluster.<div>SparkSession<div><div><div>An object that provides a point of entry to interact with underlying Spark func‐tionality and allows programming Spark with its APIs. In an interactive Spark shell, the Spark driver instantiates a SparkSession for you, while in a Spark application, you create a SparkSession object yourself</div><div>Job        -  A parallel computation consisting of multiple tasks that gets spawned in response to a Spark action (e.g., save(), collect()).</div><div>Stage    - Each job gets divided into smaller sets of tasks called stages that depend on each other.</div><div>Task      - A single unit of work or execution that will be sent to a Spark executor.</div></div></div></div><div><br></div><div>Transformations, as the name suggests, transform a Spark DataFrame into a new DataFrame without altering the original data, giving it the property of immutability. </div><div>All transformations are evaluated lazily.</div><div>Examples : map, filter, join, orderBy, groupBy, select, etc...</div><div><br></div><div>An action triggers the lazy evaluation of all the recorded transformations.</div><div>Examplse : count, show, take, collect, save</div><div><br></div><div>Narrow Transformations :</div><div>A <strong data-start=\"545\" data-end=\"570\">narrow transformation</strong> is one where <strong data-start=\"584\" data-end=\"649\">each input partition contributes to only one output partition</strong>. There is <strong data-start=\"662\" data-end=\"683\">no data shuffling</strong> across the cluster — the data stays on the same node.</div><div>Example : map, filter, sample, etc...</div><div><br></div><div>Wide Transformations :</div><div>A <strong data-start=\"1664\" data-end=\"1687\">wide transformation</strong> is one where <strong data-start=\"1701\" data-end=\"1761\">data from one partition can go to many output partitions</strong>. This requires a <strong data-start=\"1781\" data-end=\"1792\">shuffle</strong> — Spark redistributes data across the cluster network.</div><div>Example : join, district, groupByKey, reduceByKey, etc....</div><div><br></div><div><br></div>","x":22,"y":70,"width":1414.96875,"height":340.96875,"bgColor":"wheat"}],"x":182,"y":1895,"width":1554.96875,"height":430.96875,"bgColor":"#F0F0F0"},{"id":"Container-1761816376037","elements":[{"id":"Text-1761816417272","text":"Spark SQL","x":5,"y":3,"width":138,"height":30.96875,"bgColor":"#E0E0E0"},{"id":"Note-1761816430710","text":"Spark SQL engine:\n• Unifies Spark components and permits abstraction to DataFrames/Datasets in Java, Scala, Python, and R, which simplifies working with structured data sets.\n• Connects to the Apache Hive metastore and tables.\n• Reads and writes structured data with a specific schema from structured file formats (JSON, CSV, Text, Avro, Parquet, ORC, etc.) and converts data into temporary tables.\n• Offers an interactive Spark SQL shell for quick data exploration.\n• Provides a bridge to (and from) external tools via standard database JDBC/ODBC connectors.\n• Generates optimized query plans and compact code for the JVM, for final execution.","x":21,"y":50,"width":1043,"height":215,"bgColor":"wheat"},{"id":"Image-1761817007932","x":31,"y":294.5,"width":627,"height":373,"bgColor":"#F0F0F0","imagePath":"https://raw.githubusercontent.com/GaneshMaddipoti/ganeshmaddipoti.github.io/2a7afaed020b0ddbed827f00dab19a020f5218a1/assets/img/spark/SparkSQL.png"},{"id":"Image-1761817345138","x":693.75,"y":298.75,"width":384,"height":358,"bgColor":"#F0F0F0","imagePath":"https://raw.githubusercontent.com/GaneshMaddipoti/ganeshmaddipoti.github.io/a4ed1e4b5bb10d2f2bd0bcf206ca5962cff25b74/assets/img/spark/SparkCatalystOptimizer.png"},{"id":"Code-1761818778886","text":"<pre><code class=\"language-python hljs\" data-highlighted=\"yes\"><span class=\"hljs-comment\">#Read</span>\nSparkSession.read           <span class=\"hljs-comment\"># to get DataFrameReader</span>\nSparkSession.readStream.    <span class=\"hljs-comment\"># to get DataFrameReader with streaming source</span>\nspark.read.<span class=\"hljs-built_in\">format</span>(<span class=\"hljs-string\">\"parquet\"</span>)<span class=\"hljs-comment\"># csv, json, parquet, orc, text, jdbc, avro, delta, hive, kafka</span>\nspark.read.<span class=\"hljs-built_in\">format</span>(<span class=\"hljs-string\">\"csv\"</span>)\n                   .option() (<span class=\"hljs-string\">\"mode\"</span>, {PERMISSIVE | FAILFAST| DROPMALFORMED } )\n                        (<span class=\"hljs-string\">\"inferSchema\"</span>, {true | false})\n                        (<span class=\"hljs-string\">\"path\"</span>,<span class=\"hljs-string\">\"path_file_data_source\"</span>)\n                    .schema() DDL String <span class=\"hljs-keyword\">or</span> StructType, e.g., <span class=\"hljs-string\">'A INT, B STRING'</span> <span class=\"hljs-keyword\">or</span> StructType(...)\n                    .load() <span class=\"hljs-string\">\"/path/to/data/source\"</span>\n \n<span class=\"hljs-comment\">#Write</span>\nDataFrame.write                <span class=\"hljs-comment\"># to write or save</span>\nDataFrame.writeStream          <span class=\"hljs-comment\"># to write or save streaming data</span>\ndf.write.<span class=\"hljs-built_in\">format</span>(<span class=\"hljs-string\">\"csv\"</span>)         <span class=\"hljs-comment\"># csv, json, parquet, orc, text, jdbc, avro, delta, hive, kafka</span>\n             .option()         <span class=\"hljs-comment\"># (\"mode\", {append | overwrite| ignore | error or errorifexists} )</span>\n             (<span class=\"hljs-string\">\"mode\"</span>, {SaveMode.Overwrite| SaveMode.Append, Save\n             Mode.Ignore, SaveMode.ErrorIfExists})\n              (<span class=\"hljs-string\">\"path\"</span>, <span class=\"hljs-string\">\"path_to_write_to\"</span>)\n              .bucketBy(<span class=\"hljs-number\">4</span>, <span class=\"hljs-string\">\"age\"</span>) \n              .save() / saveAsTable() (with path unmanaged table)\n\ndf.createOrReplaceTempView(\"temp_dept\")               #views\ndf.createOrReplaceGlobalTempView(\"global_employees\")  #across sessions\n\n\n</code></pre>","x":1110,"y":51.666748046875,"width":835,"height":605,"bgColor":"#555"}],"x":183,"y":2373.3333740234375,"width":1991,"height":694,"bgColor":"#F0F0F0"},{"id":"Path-1759067737251","text":"*","sElement":"Node-1759067474223","eElement":"Node-1759067521911"},{"id":"Path-1759067739108","text":"*","sElement":"Node-1759067474223","eElement":"Node-1759067569427"},{"id":"Path-1759067740454","text":"*","sElement":"Node-1759067474223","eElement":"Node-1759067579230"},{"id":"Path-1759067741914","text":"*","sElement":"Node-1759067448094","eElement":"Node-1759067474223"},{"id":"Path-1759067826947","text":"*","sElement":"Node-1759067448094","eElement":"Node-1759067521911"},{"id":"Path-1759067885090","text":"*","sElement":"Node-1759067448094","eElement":"Node-1759067579230"},{"id":"Path-1761504418596","text":"*","sElement":"Node-1761504385727","eElement":"Node-1759068548800"},{"id":"Path-1761504435364","text":"*","sElement":"Node-1759068555698","eElement":"Node-1761504385727"},{"id":"Path-1761504454232","text":"*","sElement":"Node-1759068570505","eElement":"Node-1761504385727"},{"id":"Path-1761504461093","text":"*","sElement":"Node-1759068579108","eElement":"Node-1761504385727"},{"id":"Path-1761504505111","text":"*","sElement":"Node-1759068596963","eElement":"Node-1761504385727"},{"id":"Path-1761504566702","text":"*","sElement":"Node-1761504539677","eElement":"Node-1761504385727"},{"id":"Path-1761564502005","text":"*","sElement":"Node-1761504385727","eElement":"Node-1759068548800"}]