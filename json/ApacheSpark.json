[{"id":"Text-1759066374728","text":"Apache Spark","x":174.88888931274414,"y":114.33333349227905,"width":144.859375,"height":24.859375,"bgColor":"#E0E0E0"},{"id":"Note-1759067336405","text":"<div>Apache Spark is open source, distributed, unified analytics engine for fast, large-scale data processing.</div><div>Spark provides in-memory storage for intermediate computations, making it much faster than Hadoop MapReduce\n</div><div>Run programs upto 100* faster than Hadoop in memory, and 10* faster on disk.</div><div><div>Features native support for SQL, Streaming, ML, and Graph processing through consistent API</div></div><div>DAG(Dircted acyclic graph) engine which will optimize workflows.</div><div>Operates seamlessly with diverse data sources inluding cloud storage, local files, and databases.</div><div>Spark’s design philosophy centers around four key characteristics:\n• Speed\n• Ease of use\n• Modularity </div><div>• Extensibility</div>","x":175.88888931274414,"y":163.33333349227905,"width":845.9375,"height":225.9375,"bgColor":"wheat"},{"id":"Container-1759067422267","elements":[{"id":"Text-1759067429560","text":"Architecture","x":2,"y":2.6666667461395264,"width":118.828125,"height":24.859375,"bgColor":"#E0E0E0"},{"id":"Node-1759067448094","text":"Driver Program\n(SparkSession/\nSpakContext)","x":96,"y":135.66666674613953,"width":143.90625,"height":105,"bgColor":"#F0F0F0"},{"id":"Node-1759067474223","text":"Cluster Manager\n(Spark / YARN)","x":442,"y":160.66666674613953,"width":180,"height":54.859375,"bgColor":"#F0F0F0"},{"id":"Node-1759067521911","text":"Worker Node\nExecutor\n(cache)\n(task)","x":742,"y":22.666666746139526,"width":135,"height":75.9375,"bgColor":"#F0F0F0"},{"id":"Node-1759067569427","text":"Worker Node\nExecutor\n(cache)\n(task)","x":746,"y":141.66666674613953,"width":122.828125,"height":78.90625,"bgColor":"#F0F0F0"},{"id":"Node-1759067579230","text":"Worker Node\nExecutor\n(cache)\n(task)","x":737,"y":274.6666667461395,"width":112.96875,"height":86.875,"bgColor":"#F0F0F0"}],"x":167,"y":498.6666667461395,"width":893.90625,"height":388.828125,"bgColor":"#F0F0F0"},{"id":"Container-1759068506983","elements":[{"id":"Text-1759068521872","text":"Spark Components","x":2,"y":5,"width":175.796875,"height":22.96875,"bgColor":"#E0E0E0"},{"id":"Node-1759068548800","text":"Spark Core\n- Python\n- Scala/Java","x":284,"y":368,"width":306.875,"height":71.953125,"bgColor":"#F0F0F0"},{"id":"Node-1759068555698","text":"Spark Streaming\n- Python\n- Scala/Java","x":147,"y":23,"width":168.90625,"height":65.859375,"bgColor":"#F0F0F0"},{"id":"Node-1759068570505","text":"Spark SQL\n- SQL","x":208,"y":105,"width":124.84375,"height":60,"bgColor":"#F0F0F0"},{"id":"Node-1759068579108","text":"MLLib\n- Python\n-Scala/Java","x":488,"y":8,"width":127.96875,"height":61.875,"bgColor":"#F0F0F0"},{"id":"Node-1759068596963","text":"GraphX\n- Scala/Java","x":541,"y":130,"width":135.9375,"height":58.828125,"bgColor":"#F0F0F0"},{"id":"Node-1761504385727","text":"Spark Dataframe / Dataset\n- Python\n- Scala/Java","x":46.3333740234375,"y":197.8888931274414,"width":784.921875,"height":93.90625,"bgColor":"#F0F0F0"},{"id":"Node-1761504539677","text":"SparkR\n- R","x":501.3333740234375,"y":85.8888931274414,"width":106.890625,"height":47.828125,"bgColor":"#F0F0F0"}],"x":1120,"y":497,"width":870.9375,"height":460.9375,"bgColor":"#F0F0F0"},{"id":"Container-1759070178404","elements":[{"id":"Text-1759070026954","text":"RDD","x":2,"y":6,"width":99.859375,"height":30,"bgColor":"#E0E0E0"},{"id":"Note-1759070035342","text":"<div>RDD (Resilient Distributed Dataset) is the fundamental data structure — an immutable, distributed collection of objects that can be processed in parallel.</div><div>You can create an RDD from various sources — such as collections in memory, external data sources (HDFS, S3, local files), and other RDDs.</div><div>Transforming RDDs</div><div>- map / flatmap</div><div>- filter</div><div>- distinct</div><div>- sample</div><div>- union, intersection, subtract, cartesian</div><div>Aggregate operations (RDD Actions)</div><div>- count, countByValue, collect, take, top, reduce, etc...</div><div>Nothing happends in driver program until an actions is called</div>","x":19,"y":61,"width":738.90625,"height":330.9375,"bgColor":"wheat"},{"id":"Note-1759124046611","text":"Key-Value RDD<div>Just map paris of data into tuples </div><div>Ex: val result = source.map(x =&gt; (x, 1))</div><div>There are operations for Key-Value RDD</div><div>- reduceByKey((v1, v2) =&gt; (v1 + v2))</div><div>- groupByKey</div><div>- sortByKey</div><div>- keys(), values()</div><div>- join, rightOuterJoin, cogroup, subtractByKey</div>","x":23,"y":404.1111145019531,"width":602.96875,"height":270,"bgColor":"wheat"},{"id":"Code-1761720133358","text":"<div># from memory\nfrom pyspark import SparkContext\nsc = SparkContext(\"local\", \"RDD Example\")\ndata = [1, 2, 3, 4, 5]\nrdd = sc.parallelize(data)\n\n# from file\ntext_rdd = sc.textFile(\"hdfs://namenode:9000/user/data/file.txt\")\n# or local\ntext_rdd = sc.textFile(\"file:///home/user/data.txt\")\n# multiple files\nrdd = sc.textFile(\"hdfs://namenode:9000/user/data/*.txt\")        \n#(filename, content) from directory\nwhole_rdd = sc.wholeTextFiles(\"hdfs://namenode:9000/user/data/\") \n\n# from other RDDs\nnumbers = sc.parallelize([1, 2, 3, 4, 5])\nsquared = numbers.map(lambda x: x ** 2)\nfiltered = squared.filter(lambda x: x &gt; 10)\n\n# from dataframes and datasets\ndf = spark.read.csv(\"data.csv\", header=True, inferSchema=True)\nrdd = df.rdd  # convert to RDD\n\n# from external databases\njdbc_df = spark.read.format(\"jdbc\").options(\n    url=\"jdbc:mysql://localhost:3306/testdb\",\n    driver=\"com.mysql.jdbc.Driver\",\n    dbtable=\"employees\",\n    user=\"root\",\n    password=\"password\"\n).load()\n\nrdd = jdbc_df.rdd\n\n</div><div><br></div>","x":772,"y":53,"width":970.9375,"height":602.96875,"bgColor":"#555"}],"x":142.22222137451172,"y":4542.66667175293,"width":1765.9375,"height":677.96875,"bgColor":"#F0F0F0"},{"id":"Container-1759070226869","elements":[{"id":"Text-1759070232579","text":"Spark Driver","x":0.22222137451171875,"y":2.5555572509765625,"width":135,"height":30.9375,"bgColor":"#E0E0E0"},{"id":"Note-1759070247024","text":"<div>Creates the SparkSession which is the entry point of execution</div><div>Analyses the Spark application and constructs DAG</div><div>Schedules and distributes the task to executors</div><div>Monitors the progress of execution and handles failures</div><div>Return the result to the client</div>\n<div>Creates RDDs</div><div>It is responsible for RDDs resilient and distributed</div>","x":12.222221374511719,"y":40.55555725097656,"width":573.90625,"height":195.9375,"bgColor":"wheat"}],"x":194.22222137451172,"y":980.5555572509766,"width":613.90625,"height":262.96875,"bgColor":"#F0F0F0"},{"id":"Container-1761505391017","elements":[{"id":"Text-1761505394883","text":"Executors","x":5.6666717529296875,"y":5.22222900390625,"width":115.796875,"height":30.9375,"bgColor":"#E0E0E0"},{"id":"Note-1761505406611","text":"Run on worker nodes in Spark cluster and host tasks (IO, Data).<div>Each worker node can run multiple executors based on \n - available CPU cores\n - memory\n - configuration settings</div><div>Store intermediate results in memory or disk</div><div>Interact with driver for task execution or data transfer</div><div><br></div>","x":14.666671752929688,"y":49.22222900390625,"width":478.90625,"height":180,"bgColor":"wheat"}],"x":843.6666717529297,"y":984.2222290039062,"width":520.9375,"height":244.921875,"bgColor":"#F0F0F0"},{"id":"Container-1761505667291","elements":[{"id":"Text-1761505675954","text":"Spark DAG","x":3,"y":2.81817626953125,"width":117.890625,"height":27.890625,"bgColor":"#E0E0E0"},{"id":"Note-1761505689576","text":"Spark jobs are broken down to stages - group of tasks that can run in parallel<div>Computations flow in one direction through the stags (Directed)</div><div>Stages will never loop back, ensuring the job terminates (Acyclic)</div><div>Stages are organized into a dependency graph for execution flow (Graph)</div><div>Tasks within a stage can run in parallel, known as Shared Nothing mode</div><div>Job -&gt; Stage1 -&gt; Stage 2\n             (Task1)    (Task1)\n             (Taks 2)    (Task 2)</div><div><br></div>","x":10,"y":35.81817626953125,"width":563.90625,"height":195,"bgColor":"wheat"}],"x":1435.272705078125,"y":986.8181762695312,"width":590.859375,"height":241.890625,"bgColor":"#F0F0F0"},{"id":"Container-1761506124548","elements":[{"id":"Text-1761506130026","text":"Spark UI","x":5.818115234375,"y":4.6363525390625,"width":99.859375,"height":30,"bgColor":"#E0E0E0"},{"id":"Note-1761506139626","text":"Spark provides web UI for monitoring and management<div>- Application UI \n  Per application / SparkSession\n  Application progress and task execution\n  DAG visualization\n  Resource usage and performace metrics</div><div>- Master UI\n  Per Cluster \n  Worker node status and health\n  Cluster wide runing applications and resources</div>","x":13.818115234375,"y":43.6363525390625,"width":422.828125,"height":212.828125,"bgColor":"wheat"}],"x":2056.818115234375,"y":984.6363525390625,"width":452.828125,"height":267.890625,"bgColor":"#F0F0F0"},{"id":"Container-1761508780765","elements":[{"id":"Text-1761508785421","text":"Spark Dataframe","x":9.5,"y":5,"width":185.859375,"height":32.828125,"bgColor":"#E0E0E0"},{"id":"Note-1761508803091","text":"Dataframes are distributed collection of records, all with same schema<div>Dataframes track their schema, and provide native support for SQL functions and joins</div><div>Dataframes are evaluatd as DAGs, using lazy evaluation, and providing lineage and fault tolerance</div><div>Dataframes can be created \n- from various sources CSV, JSON, Parquet, text, and Binary files\n- from Delta late and other table storage format directories\n- from tables and views in Unity catalog, external databases\n- from an existing RDD</div><div>Tungsten is Spark's columnar in-memory execution engine, which is key for optimization</div><div>Tungsten offers\n- off-heap memory managemnt\n- cache-aware computation\n- Code generatino for faster execution<br></div><div>In memory - columnar - Tungsten</div><div>On Disk - rows based (CSV) - columnar (Parquet)</div><div>Dataframes schema can be inferred from data, or explicitly mentioned(more efficient)</div><div>Dataframes are immutable </div><div>Transformations create new Dataframes from exisitng ones and are lazy operations(logical plan)\n- select, filter, withcolumn, groupby, agg</div><div>Actions initiate the actual exeuction and return result/write to external storage\n- count, show, take, first, write\n<br></div><div>Transformations:\nColumn operations\twithColumn, drop, alias\tdf.withColumn(\"new\", col(\"age\")+1)\nFiltering\tfilter, where\t                                                df.filter(col(\"age\") &gt; 30)\nSelecting\tselect, selectExpr\t                                df.select(\"name\", \"age\")\nSorting\torderBy, sort\t                                                df.orderBy(col(\"age\").desc())\nGrouping\tgroupBy + agg\t                                df.groupBy(\"dept\").agg(avg(\"salary\"))\nJoins\tjoin\t                                                                df1.join(df2, \"id\", \"inner\")\nUnion\tunion, unionByName\t                                df1.union(df2)\n\nConditions: When otherwise\n\nActions:\nshow()\t        Display data\ncollect()\t        Return all rows\ncount()\t        Count rows\nfirst(), head()\tReturn one or few rows\ntake(n)\t        Return first n rows\nwrite\t        Write to file/table\n\nJoins:\nTypes: inner, left, right, outer, cross, semi, anti\nbroadcast(df2) for performance on small DataFrames\n</div><div>\nWindow functions :\nrank, dense_rank, row_number\nlead, lag\npartitionBy, orderBy</div><div>\nHandling nulls\ndropna, fillna, replace\nNull-safe equality: col1 &lt;=&gt; col2\n\nWriting modes : \"overwrite\", \"append\", \"ignore\", \"errorifexists\"\n\nUDF:\nRegistering UDFs\nPerformance impact (Python UDFs are slower than built-ins)\nPrefer pandas_udf or SQL functions when possible\n<br></div>","x":16.5,"y":71,"width":816.890625,"height":452.96875,"bgColor":"wheat"},{"id":"Code-1761742111426","text":"<pre><code class=\"language-python hljs\" data-highlighted=\"yes\"><span class=\"hljs-keyword\">from</span> pyspark.sql <span class=\"hljs-keyword\">import</span> SparkSession\n\nspark = SparkSession.builder.appName(<span class=\"hljs-string\">\"App 1\"</span>).getOrCreate()\n\n<span class=\"hljs-comment\"># from in-memory</span>\ndata = [(<span class=\"hljs-string\">\"Alice\"</span>, <span class=\"hljs-number\">34</span>), (<span class=\"hljs-string\">\"Bob\"</span>, <span class=\"hljs-number\">45</span>), (<span class=\"hljs-string\">\"Cathy\"</span>, <span class=\"hljs-number\">29</span>)]\ncolumns = [<span class=\"hljs-string\">\"Name\"</span>, <span class=\"hljs-string\">\"Age\"</span>]\ndf = spark.createDataFrame(data, columns)\n\n<span class=\"hljs-comment\"># from RDD</span>\nrdd = spark.sparkContext.parallelize(data)\ndf_from_rdd = rdd.toDF(columns)\n\n<span class=\"hljs-comment\"># from CSV</span>\ndf_csv = spark.read.option(<span class=\"hljs-string\">\"delimiter\"</span>, <span class=\"hljs-string\">\",\"</span>) \\\n                   .option(<span class=\"hljs-string\">\"header\"</span>, <span class=\"hljs-literal\">True</span>) \\\n                   .option(<span class=\"hljs-string\">\"inferSchema\"</span>, <span class=\"hljs-literal\">True</span>) \\\n                   .csv(<span class=\"hljs-string\">\"path/to/file.csv\"</span>)  <span class=\"hljs-comment\">#file:/s3a:/hdfs:</span>\n\n<span class=\"hljs-comment\"># from JSON</span>\ndf_json = spark.read.json(<span class=\"hljs-string\">\"path/to/file.json\"</span>)\n\n<span class=\"hljs-comment\"># from Parquet</span>\ndf_parquet = spark.read.parquet(<span class=\"hljs-string\">\"path/to/file.parquet\"</span>)\n\n<span class=\"hljs-comment\"># from avro</span>\ndf_avro = spark.read.<span class=\"hljs-built_in\">format</span>(<span class=\"hljs-string\">\"avro\"</span>).load(<span class=\"hljs-string\">\"path/to/file.avro\"</span>)\n\n<span class=\"hljs-comment\"># from orc</span>\ndf_orc = spark.read.orc(<span class=\"hljs-string\">\"path/to/file.orc\"</span>)\n\n<span class=\"hljs-comment\"># from database</span>\ndf_jdbc = spark.read.<span class=\"hljs-built_in\">format</span>(<span class=\"hljs-string\">\"jdbc\"</span>).options(\n    url=<span class=\"hljs-string\">\"jdbc:mysql://localhost:3306/testdb\"</span>,\n    driver=<span class=\"hljs-string\">\"com.mysql.cj.jdbc.Driver\"</span>,\n    dbtable=<span class=\"hljs-string\">\"employees\"</span>,\n    user=<span class=\"hljs-string\">\"root\"</span>,\n    password=<span class=\"hljs-string\">\"password\"</span>\n).load()\n\n<span class=\"hljs-comment\"># from kafka</span>\ndf_kafka = spark.read \\\n    .<span class=\"hljs-built_in\">format</span>(<span class=\"hljs-string\">\"kafka\"</span>) \\\n    .option(<span class=\"hljs-string\">\"kafka.bootstrap.servers\"</span>, <span class=\"hljs-string\">\"localhost:9092\"</span>) \\\n    .option(<span class=\"hljs-string\">\"subscribe\"</span>, <span class=\"hljs-string\">\"my_topic\"</span>) \\\n    .load()\n\n<span class=\"hljs-comment\"># from pandas dataframe</span>\n<span class=\"hljs-keyword\">import</span> pandas <span class=\"hljs-keyword\">as</span> pd\npdf = pd.DataFrame({<span class=\"hljs-string\">\"name\"</span>: [<span class=\"hljs-string\">\"John\"</span>, <span class=\"hljs-string\">\"Jane\"</span>], <span class=\"hljs-string\">\"age\"</span>: [<span class=\"hljs-number\">30</span>, <span class=\"hljs-number\">25</span>]})\n\ndf_spark = spark.createDataFrame(pdf)\n\n<span class=\"hljs-comment\"># from delta lake table</span>\ndf_delta = spark.read.<span class=\"hljs-built_in\">format</span>(<span class=\"hljs-string\">\"delta\"</span>).load(<span class=\"hljs-string\">\"/mnt/delta/events/\"</span>)\n\n<span class=\"hljs-comment\"># Condition logic:</span>\ndf = df.withColumn(\n    <span class=\"hljs-string\">\"category\"</span>,\n    when(col(<span class=\"hljs-string\">\"age\"</span>) &lt; <span class=\"hljs-number\">30</span>, <span class=\"hljs-string\">\"Young\"</span>)\n     .when(col(<span class=\"hljs-string\">\"age\"</span>) &lt; <span class=\"hljs-number\">50</span>, <span class=\"hljs-string\">\"Adult\"</span>)\n     .otherwise(<span class=\"hljs-string\">\"Senior\"</span>)\n)</code><code class=\"language-python hljs\" data-highlighted=\"yes\">df2.explain()  # shows logical plan\ndf.show()\n\n<span class=\"hljs-comment\"># Grouping and aggregating</span>\n<span class=\"hljs-keyword\">from</span> pyspark.sql <span class=\"hljs-keyword\">import</span> functions <span class=\"hljs-keyword\">as</span> F\ndf.groupBy(<span class=\"hljs-string\">\"dept\"</span>).agg(\n    F.avg(<span class=\"hljs-string\">\"salary\"</span>).alias(<span class=\"hljs-string\">\"avg_salary\"</span>),\n    F.<span class=\"hljs-built_in\">max</span>(<span class=\"hljs-string\">\"salary\"</span>).alias(<span class=\"hljs-string\">\"max_salary\"</span>)\n).show()\n\n<span class=\"hljs-comment\"># Joins</span>\ndf1.join(df2, df1.<span class=\"hljs-built_in\">id</span> == df2.emp_id, <span class=\"hljs-string\">\"inner\"</span>)\ndf1.join(df2, <span class=\"hljs-string\">\"id\"</span>, <span class=\"hljs-string\">\"left\"</span>)\n\n<span class=\"hljs-comment\"># Handling nulls</span>\ndf.na.drop(subset=[<span class=\"hljs-string\">\"age\"</span>])\ndf.na.fill({<span class=\"hljs-string\">\"salary\"</span>: <span class=\"hljs-number\">0</span>, <span class=\"hljs-string\">\"dept\"</span>: <span class=\"hljs-string\">\"Unknown\"</span>})\ndf.<span class=\"hljs-built_in\">filter</span>(col(<span class=\"hljs-string\">\"salary\"</span>).isNotNull())\n\n<span class=\"hljs-comment\"># Caching</span>\ndf.cache()\ndf.count()  <span class=\"hljs-comment\"># triggers caching</span>\n\n<span class=\"hljs-comment\"># Repartition and coalesce</span>\ndf.repartition(<span class=\"hljs-number\">4</span>, <span class=\"hljs-string\">\"dept\"</span>)\ndf.coalesce(<span class=\"hljs-number\">1</span>)\n\n<span class=\"hljs-comment\"># UDF</span>\n<span class=\"hljs-keyword\">from</span> pyspark.sql.functions <span class=\"hljs-keyword\">import</span> udf\n<span class=\"hljs-keyword\">from</span> pyspark.sql.types <span class=\"hljs-keyword\">import</span> StringType\n<span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">to_upper</span>(<span class=\"hljs-params\">name</span>):\n    <span class=\"hljs-keyword\">return</span> name.upper()\nto_upper_udf = udf(to_upper, StringType())\ndf.withColumn(<span class=\"hljs-string\">\"name_upper\"</span>, to_upper_udf(col(<span class=\"hljs-string\">\"name\"</span>))).show()\n\n\n</code></pre>","x":906,"y":55.25,"width":955.96875,"height":976.96875,"bgColor":"#555"},{"id":"Image-1761842799448","x":24,"y":533,"width":805.9375,"height":398.921875,"bgColor":"#F0F0F0","imagePath":"https://raw.githubusercontent.com/GaneshMaddipoti/ganeshmaddipoti.github.io/30569c6dae19b72cf449d9bcab6d392638708a9b/assets/img/spark/SparkDataFrameCreate.png"}],"x":173.5,"y":3362,"width":1888.90625,"height":1084.921875,"bgColor":"#F0F0F0"},{"id":"Container-1761509806889","elements":[{"id":"Text-1761509813845","text":"Photon Engine","x":0.6666717529296875,"y":4,"width":154.84375,"height":33.90625,"bgColor":"#E0E0E0"},{"id":"Note-1761509828692","text":"Databricks native vectorized query engine<div>Processes data in batches rather than row by row for performance</div><div>Photon is by default enabled in SQL warehouses and serverless compute</div><div>Can be enabled in all purpose and job clusters</div>","x":19.666671752929688,"y":51,"width":557.828125,"height":125.859375,"bgColor":"wheat"}],"x":845.6666717529297,"y":1282,"width":590.859375,"height":196.875,"bgColor":"#F0F0F0"},{"id":"Divider-1761564386985","text":"Divider","x":160,"y":406,"width":2307.890625,"height":22.96875,"bgColor":"#F0F0F0"},{"id":"Divider-1761564644303","text":"Divider","x":164,"y":1551,"width":2310.9375,"height":22.96875,"bgColor":"#F0F0F0"},{"id":"Container-1761804224918","elements":[{"id":"Text-1761804232244","text":"Cluser Manager","x":3.333333373069763,"y":4.33331298828125,"width":163.90625,"height":32.96875,"bgColor":"#E0E0E0"},{"id":"Note-1761804244110","text":"The cluster manager is responsible for managing and allocating resources for the cluster of nodes on which your Spark application runs. <div>Currently, Spark supports four cluster managers: the built-in standalone cluster manager, Apache Hadoop YARN, Apache Mesos, and Kubernetes.</div>","x":16.333333373069763,"y":50.33331298828125,"width":534.859375,"height":130.9375,"bgColor":"wheat"}],"x":211.33333337306976,"y":1286.3333129882812,"width":570.9375,"height":202.96875,"bgColor":"#F0F0F0"},{"id":"Container-1761804508428","elements":[{"id":"Text-1761804513291","text":"Deployement Modes","x":8.333333373069763,"y":10.6666259765625,"width":229.921875,"height":36.875,"bgColor":"#E0E0E0"},{"id":"Note-1761804525821","text":"Mode                                Spark driver                                                                                    Spark executor                                                                                      Cluster manager\nLocal                        Runs on a single JVM, like a laptop or single node                 Runs on the same JVM as the driver                                                         Runs on the same host\nStandalone             Can run on any node in the cluster                                             Each node in the cluster will launch its own executor JVM                  Can be allocated arbitrarily to any host in the cluster\nYARN (client)          Runs on a client, not part of the cluster                                    YARN’s NodeManager’s container                                                              YARN’s Resource Manager works with YARN’s Application Master to allocate the containers on NodeManagers for executors\nYARN(cluster)        Runs with the YARN Application Master                                    Same as YARN client mode                                                                        Same as YARN client mode\nKubernetes             Runs in a Kubernetes pod                                                            Each worker runs within its own pod                                                          Kubernetes Master","x":31.333333373069763,"y":73.6666259765625,"width":918.984375,"height":163.984375,"bgColor":"wheat"}],"x":173.33333337306976,"y":1614.3333129882812,"width":1000,"height":260.953125,"bgColor":"#F0F0F0"},{"id":"Image-1761805750988","x":2035.3333740234375,"y":499,"width":682.96875,"height":415.9375,"bgColor":"#F0F0F0","imagePath":"https://raw.githubusercontent.com/GaneshMaddipoti/ganeshmaddipoti.github.io/refs/heads/main/assets/img/spark/SparkDataPartitions.png"},{"id":"Container-1761807275112","elements":[{"id":"Text-1761807283471","text":"Application Concepts","x":5,"y":7,"width":214.921875,"height":38.921875,"bgColor":"#E0E0E0"},{"id":"Note-1761807297770","text":"Application   -    A user program built on Spark using its APIs. It consists of a driver program and executors on the cluster.<div>SparkSession<div><div><div>An object that provides a point of entry to interact with underlying Spark func‐tionality and allows programming Spark with its APIs. In an interactive Spark shell, the Spark driver instantiates a SparkSession for you, while in a Spark application, you create a SparkSession object yourself</div><div>Job        -  A parallel computation consisting of multiple tasks that gets spawned in response to a Spark action (e.g., save(), collect()).</div><div>Stage    - Each job gets divided into smaller sets of tasks called stages that depend on each other.</div><div>Task      - A single unit of work or execution that will be sent to a Spark executor.\n\nArrow - Efficient data transfer between JVM &amp; Python\nspark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n</div></div></div></div><div><br></div><div>Transformations, as the name suggests, transform a Spark DataFrame into a new DataFrame without altering the original data, giving it the property of immutability. </div><div>All transformations are evaluated lazily.</div><div>Examples : map, filter, join, orderBy, groupBy, select, etc...</div><div><br></div><div>An action triggers the lazy evaluation of all the recorded transformations.</div><div>Examplse : count, show, take, collect, save</div><div><br></div><div>Narrow Transformations :</div><div>A <strong data-start=\"545\" data-end=\"570\">narrow transformation</strong> is one where <strong data-start=\"584\" data-end=\"649\">each input partition contributes to only one output partition</strong>. There is <strong data-start=\"662\" data-end=\"683\">no data shuffling</strong> across the cluster — the data stays on the same node.</div><div>Example : map, filter, sample, etc...</div><div><br></div><div>Wide Transformations :</div><div>A <strong data-start=\"1664\" data-end=\"1687\">wide transformation</strong> is one where <strong data-start=\"1701\" data-end=\"1761\">data from one partition can go to many output partitions</strong>. This requires a <strong data-start=\"1781\" data-end=\"1792\">shuffle</strong> — Spark redistributes data across the cluster network.</div><div>Example : join, district, groupByKey, reduceByKey, etc....</div><div>\nCaching and Perstisting:\n.cache() = in-memory only (MEMORY_ONLY)\n.persist(StorageLevel.MEMORY_AND_DISK) = fallback to disk if memory full\n.unpersist() to free memory\n\nRepartition and Coalesce:\n| Function           | Use Case                                                         | Shuffle                | Example              |\n| ---------------- -------| ----------------------------------------------------------------  | ------------------------  |\n| `repartition(n)` | Increase or redistribute partitions              | Yes                      | `df.repartition(10)` |\n| `coalesce(n)`   | Decrease partitions (merge nearby ones) | No (best effort) | `df.coalesce(1)`     |\n\nBroadcast join:\nWhen joining a small lookup table with a large DataFrame, use broadcast to avoid a full shuffle.\nIf Spark knows small_df &lt; broadcast threshold (~10MB), it automatically chooses a broadcast join.\nYou can force it using broadcast().\n\nPartition Skey:\nData skew happens when some partitions contain more data than others, causing slow tasks.\nSymptoms\nSome tasks take much longer in Spark UI\n“Task not serializable” or “executor lost” errors in extreme cases\nFixes\nRepartition on a more uniform key\nAdd random “salt” column to even out joins\n\nPredicate pushdown:\nSpark can push filters down to the data source (Parquet, ORC, JDBC) — reducing data read from disk.\nThe physical plan should show \"PushedFilters: [GreaterThan(age,30)]\"\nPredicate pushdown works best with Parquet, ORC, and JDBC.\nIt does not help with CSV (no column-level pruning).\n\nCatalyst Optimizer vs Tungsten:\nCatalyst\tOptimizes logical plan (e.g., constant folding, predicate pushdown, reordering joins)\nTungsten\tOptimizes execution (memory management, code generation, binary processing)\n\nCommon Debugging tips:\nJob hangs at “shuffle read”\tData skew or large shuffle\t                                Repartition or broadcast small side\nJob out of memory\t                Too few partitions or large collect()\t                Increase partitions, avoid collect()\nStage retries repeatedly\t        Faulty UDF or network instability\t                Check UDF serialization and logs\n“Task not serializable”\t        Closure captured non-serializable object\tDefine variables outside transformations\nSmall files issue\t                        Too many small writes\t                                Coalesce before writing\n</div><div>\nUse built-in Spark SQL functions whenever possible — they’re executed in the JVM, not Python(UDF).\n\nAdjust shuffle partitions for medium-sized data:\nspark.conf.set(\"spark.sql.shuffle.partitions\", 50)\n\n</div>","x":22,"y":70,"width":903.984375,"height":331.953125,"bgColor":"wheat"}],"x":182,"y":1895,"width":970,"height":420,"bgColor":"#F0F0F0"},{"id":"Container-1761816376037","elements":[{"id":"Text-1761816417272","text":"Spark SQL","x":5,"y":3,"width":137.96875,"height":30.9375,"bgColor":"#E0E0E0"},{"id":"Note-1761816430710","text":"Spark SQL engine:\n• Unifies Spark components and permits abstraction to DataFrames/Datasets in Java, Scala, Python, and R, which simplifies working with structured data sets.\n• Connects to the Apache Hive metastore and tables.\n• Reads and writes structured data with a specific schema from structured file formats (JSON, CSV, Text, Avro, Parquet, ORC, etc.) and converts data into temporary tables.\n• Offers an interactive Spark SQL shell for quick data exploration.\n• Provides a bridge to (and from) external tools via standard database JDBC/ODBC connectors.\n• Generates optimized query plans and compact code for the JVM, for final execution.\n\nSpark SQL is fast because of:\nCatalyst Optimizer — analyzes and optimizes query plans\nTungsten Engine — efficient memory and CPU management\nColumnar storage — Parquet, ORC formats improve IO\n\nGlobal temp views are accsessible across multiple sessions\nGlobal views live under the global_temp database\nTemp view - cleaned_df.createOrReplaceTempView(\"data_table\")\n<div>Global temp view - cleaned_df.createOrReplaceGlobalTempView(\"data_table\")\n\nSave modes: overwrite, append, ignore, errorIfExists\n\nBuilt-in SQL functions (col, lit, when, upper, concat, etc.) and (<code data-start=\"4213\" data-end=\"4222\">groupBy</code>, <code data-start=\"4224\" data-end=\"4229\">agg</code>, <code data-start=\"4231\" data-end=\"4238\">count</code>, <code data-start=\"4240\" data-end=\"4245\">sum</code>, <code data-start=\"4247\" data-end=\"4252\">avg, alias)\n</code>String\tconcat, substr, length, lower, upper, trim\nMath\tround, sqrt, abs, pow\nDate/Time\tcurrent_date, year, month, datediff, to_date\nAggregation\tsum, avg, max, min, count\nConditional\tcase when, if, coalesce, nvl\n\nDifferent join types (inner, left, right, outer), and broadcast joins\n\nCACHE and UNCACHE operations\nPersisting intermediate results\nLazy vs eager caching\n\nHandling NULL values\nfillna, dropna, isNull, isNotNull\n\n\n</div>","x":24,"y":47,"width":887.96875,"height":306.953125,"bgColor":"wheat"},{"id":"Image-1761817007932","x":31,"y":390.5,"width":626.953125,"height":372.96875,"bgColor":"#F0F0F0","imagePath":"https://raw.githubusercontent.com/GaneshMaddipoti/ganeshmaddipoti.github.io/2a7afaed020b0ddbed827f00dab19a020f5218a1/assets/img/spark/SparkSQL.png"},{"id":"Image-1761817345138","x":695.75,"y":405.75,"width":383.921875,"height":357.96875,"bgColor":"#F0F0F0","imagePath":"https://raw.githubusercontent.com/GaneshMaddipoti/ganeshmaddipoti.github.io/a4ed1e4b5bb10d2f2bd0bcf206ca5962cff25b74/assets/img/spark/SparkCatalystOptimizer.png"},{"id":"Code-1761818778886","text":"<pre><code class=\"language-python hljs\" data-highlighted=\"yes\"><span class=\"hljs-comment\"># Pyspark SQL script to read data from a CSV file</span>\n<span class=\"hljs-keyword\">from</span> pyspark.sql <span class=\"hljs-keyword\">import</span> SparkSession\n\nspark = SparkSession.builder.appName(<span class=\"hljs-string\">\"SparkSQLScript\"</span>).getOrCreate()\n<span class=\"hljs-comment\"># Read data from a CSV file into a DataFrame</span>\ndf = spark.read.csv(<span class=\"hljs-string\">\"../data/input.csv\"</span>, header=<span class=\"hljs-literal\">True</span>, inferSchema=<span class=\"hljs-literal\">True</span>)\ncleaned_df = df.dropna()\n<span class=\"hljs-comment\"># Perform some transformations using SQL</span>\ncleaned_df.createOrReplaceTempView(<span class=\"hljs-string\">\"data_table\"</span>)\n# cleaned_df.createOrReplaceGlobalTempView(\"data_table\")\ntransformed_df = spark.sql(<span class=\"hljs-string\">\"\"\"\n    SELECT id, name, age, salary,\n           (age + salary) AS sum_column\n    FROM data_table\n    WHERE id IS NOT NULL\n\"\"\"</span>)\n<span class=\"hljs-comment\"># Show the results</span>\ntransformed_df.show()\n\n# Write\ntransformed_df.write.mode(\"overwrite\").parquet(\"output/people_parquet\")\nspark.stop()\n\n# manual schema definition\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType\nschema = StructType([\n    StructField(\"name\", StringType(), True),\n    StructField(\"age\", IntegerType(), True)\n])\ndf = spark.read.csv(\"data/people.csv\", schema=schema, header=True)\n\n# SQL functions\nfrom pyspark.sql.functions import col, upper, when\ndf = spark.read.csv(\"data/people.csv\", header=True, inferSchema=True)\ndf = df.withColumn(\"name_upper\", upper(col(\"name\"))) \\\n       .withColumn(\"age_group\", when(col(\"age\") &gt; 30, \"Adult\").otherwise(\"Youth\"))\ndf.show()\n\n# with Dataframe functios\nfrom pyspark.sql import functions as F\ndf.groupBy(\"department\").agg(\n    F.avg(\"salary\").alias(\"avg_salary\"),\n    F.count(\"*\").alias(\"count\")\n).show()\n\n\n# SQL Joins\nemp = spark.read.csv(\"data/employees.csv\", header=True, inferSchema=True)\ndept = spark.read.csv(\"data/departments.csv\", header=True, inferSchema=True)\nemp.join(dept, emp.dept_id == dept.id, \"inner\").select(\"name\", \"department_name\").show()\n\n# SQL Window/Rank\nfrom pyspark.sql.window import Window\nimport pyspark.sql.functions as F\nwindowSpec = Window.partitionBy(\"department\").orderBy(F.desc(\"salary\"))\ndf.withColumn(\"rank\", F.rank().over(windowSpec)).filter(\"rank = 1\").show()\n\n# SQL query optimization using caching\ndf.createOrReplaceTempView(\"sales\")\nspark.sql(\"CACHE TABLE sales\")\nspark.sql(\"SELECT COUNT(*) FROM sales\").show()\nspark.sql(\"SELECT * FROM sales WHERE region = 'EUROPE'\").show()\n\n# Creating permanent tables\nspark.sql(\"CREATE DATABASE IF NOT EXISTS company_db\")\ndf.write.saveAsTable(\"company_db.employees\")\nspark.sql(\"SELECT * FROM company_db.employees\").show()\n\n\n</code></pre>","x":1110,"y":51.666748046875,"width":834.921875,"height":604.921875,"bgColor":"#555"}],"x":183,"y":2373.3333740234375,"width":1990,"height":825,"bgColor":"#F0F0F0"},{"id":"Container-1761905415468","elements":[{"id":"Text-1761905422484","text":"Certification","x":4,"y":3.33349609375,"width":157.96875,"height":38.921875,"bgColor":"#E0E0E0"},{"id":"Note-1761905434235","text":"Apache Spark Architecture and Components - 20%\n\nUsing Spark SQL - 20%\n\nDeveloping Apache Spark™ DataFrame/DataSet API Applications - 30%\n\nTroubleshooting and Tuning Apache Spark DataFrame API Applications - 10%\n\nStructured Streaming - 10%\n\nUsing Spark Connect to deploy applications - 5%\n\nUsing Pandas API on Apache Spark - 5%","x":23,"y":55.33349609375,"width":573.984375,"height":262.96875,"bgColor":"wheat"}],"x":178,"y":5334.33349609375,"width":628.90625,"height":350.9375,"bgColor":"#F0F0F0"},{"id":"Container-1761905724445","elements":[{"id":"Text-1761905729968","text":"Using DStream","x":1.6666259765625,"y":5.3333740234375,"width":226.890625,"height":40.9375,"bgColor":"#E0E0E0"},{"id":"Code-1761906746473","text":"<pre><code class=\"language-python hljs\" data-highlighted=\"yes\"><span class=\"hljs-comment\"># DStreams are micro-batch–based, suitable for simpler, lower-latency streaming.</span>\n<span class=\"hljs-keyword\"># driver gets batches of data which will be converted to RDDs\nfrom</span> pyspark <span class=\"hljs-keyword\">import</span> SparkContext\n<span class=\"hljs-keyword\">from</span> pyspark.streaming <span class=\"hljs-keyword\">import</span> StreamingContext\n\n<span class=\"hljs-comment\"># Create a local StreamingContext with two working threads</span>\n<span class=\"hljs-comment\"># and a batch interval of 5 seconds</span>\nsc = SparkContext(<span class=\"hljs-string\">\"local[2]\"</span>, <span class=\"hljs-string\">\"NetworkWordCount\"</span>)\nssc = StreamingContext(sc, <span class=\"hljs-number\">5</span>)\n\n<span class=\"hljs-comment\"># Create a DStream that connects to hostname:port, e.g., localhost:9999</span>\nlines = ssc.socketTextStream(<span class=\"hljs-string\">\"localhost\"</span>, <span class=\"hljs-number\">9999</span>)\n\n<span class=\"hljs-comment\"># Split each line into words</span>\nwords = lines.flatMap(<span class=\"hljs-keyword\">lambda</span> line: line.split(<span class=\"hljs-string\">\" \"</span>))\n\n<span class=\"hljs-comment\"># Count each word in each batch</span>\npairs = words.<span class=\"hljs-built_in\">map</span>(<span class=\"hljs-keyword\">lambda</span> word: (word, <span class=\"hljs-number\">1</span>))\nwordCounts = pairs.reduceByKey(<span class=\"hljs-keyword\">lambda</span> x, y: x + y)\n\n<span class=\"hljs-comment\"># Print the first ten elements of each RDD generated in this DStream</span>\nwordCounts.pprint()\n\n<span class=\"hljs-comment\"># Start the computation</span>\nssc.start()\n\n<span class=\"hljs-comment\"># Wait for the streaming to finish (manually stop with Ctrl+C)</span>\nssc.awaitTermination()\n\n</code></pre>","x":1009,"y":67.5,"width":838.90625,"height":608.921875,"bgColor":"#555"},{"id":"Image-1761908890949","x":25.25,"y":70,"width":925.9375,"height":622.96875,"bgColor":"#F0F0F0","imagePath":"https://raw.githubusercontent.com/GaneshMaddipoti/ganeshmaddipoti.github.io/536f305dc76297880cbec88f8f4dcf19d1133f32/assets/img/spark/SparkDStream.png"}],"x":2124.6666259765625,"y":3365.3333740234375,"width":1871.953125,"height":724.921875,"bgColor":"#F0F0F0"},{"id":"Container-1761906968398","elements":[{"id":"Text-1761906971770","text":"Structured Streaming","x":8.25,"y":6.5,"width":214.921875,"height":46.890625,"bgColor":"#E0E0E0"},{"id":"Code-1761907572429","text":"<pre><code class=\"language-python hljs\" data-highlighted=\"yes\"><span class=\"hljs-comment\"># Dataset/Dataframe will expand ever, on which we can query window operations</span>\n<span class=\"hljs-comment\"># Structured Streaming supports 3 modes depending on the query type:</span>\n<span class=\"hljs-comment\">#append → Only new rows added to the result table.</span>\n<span class=\"hljs-comment\">#update → Updated rows in the result table.</span>\n<span class=\"hljs-comment\">#complete → Recompute entire result each time (used in the example).</span>\n\n<span class=\"hljs-keyword\">from</span> pyspark.sql <span class=\"hljs-keyword\">import</span> SparkSession\n<span class=\"hljs-keyword\">from</span> pyspark.sql.functions <span class=\"hljs-keyword\">import</span> explode, split\n\n<span class=\"hljs-comment\"># Create Spark session</span>\nspark = SparkSession.builder \\\n    .appName(<span class=\"hljs-string\">\"StructuredNetworkWordCount\"</span>) \\\n    .getOrCreate()\n\n<span class=\"hljs-comment\"># Set log level to reduce verbosity</span>\nspark.sparkContext.setLogLevel(<span class=\"hljs-string\">\"WARN\"</span>)\n\n<span class=\"hljs-comment\"># Create DataFrame representing the stream of input lines from connection to host:port</span>\nlines = spark.readStream \\\n    .<span class=\"hljs-built_in\">format</span>(<span class=\"hljs-string\">\"socket\"</span>) \\\n    .option(<span class=\"hljs-string\">\"host\"</span>, <span class=\"hljs-string\">\"localhost\"</span>) \\\n    .option(<span class=\"hljs-string\">\"port\"</span>, <span class=\"hljs-number\">9999</span>) \\\n    .load()\n\n<span class=\"hljs-comment\"># Split the lines into words and add a timestamp column</span>\n<span class=\"hljs-comment\"># (socket source does not have built-in event time, so we add one manually)</span>\nwords = lines.select(\n    explode(split(lines.value, <span class=\"hljs-string\">\" \"</span>)).alias(<span class=\"hljs-string\">\"word\"</span>),\n    current_timestamp().alias(<span class=\"hljs-string\">\"timestamp\"</span>)\n)\n\n<span class=\"hljs-comment\"># Group the data by window and word, then count occurrences</span>\nwindowedCounts = words.groupBy(\n    window(words.timestamp, <span class=\"hljs-string\">\"30 seconds\"</span>, <span class=\"hljs-string\">\"10 seconds\"</span>),\n    words.word\n).count()\n\n# watermarks\nfrom pyspark.sql.functions import window\nevents = df.withWatermark(\"timestamp\", \"10 minutes\")\nagg = events.groupBy(\n    window(events.timestamp, \"5 minutes\"),\n    events.user\n).count()\n# Spark will keep state for max 10 minutes of lateness.\n\n<span class=\"hljs-comment\"># Start running the query that prints the running counts to the console</span>\nquery = windowedCounts.writeStream \\\n    .outputMode(<span class=\"hljs-string\">\"complete\"</span>) \\\n    .<span class=\"hljs-built_in\">format</span>(<span class=\"hljs-string\">\"console\"</span>) \\\n    .option(<span class=\"hljs-string\">\"truncate\"</span>, <span class=\"hljs-string\">\"false\"</span>) \\\n    .start()\n\n# append\tNew rows only\t        e.g., new sensor readings\n# update\tUpdated + new rows\te.g., running averages\n# complete\tEntire table each time\te.g., word count\n\nquery.awaitTermination()\n</code></pre>","x":725.25,"y":55,"width":830.953125,"height":575.953125,"bgColor":"#555"},{"id":"Image-1761908973466","x":26.5,"y":366.5,"width":670,"height":260,"bgColor":"#F0F0F0","imagePath":"https://raw.githubusercontent.com/GaneshMaddipoti/ganeshmaddipoti.github.io/6d82d0d966080dcd60c369a0092b1145cdd3016f/assets/img/spark/SparkStructuredStreaming.png"},{"id":"Note-1761992481526","text":"<div>Trigger options:\nDefault (micro-batch)\t        \nquery.start()\t                      Continuous small batches(Default ~500ms batches)\nFixed interval\t                .trigger(processingTime=\"10 seconds\")\tRun every 10s\nOne-time\t                        .trigger(once=True)\t     Batch-like behavior\nContinuous (low latency)\t\n.trigger(continuous=\"5 seconds\")\tExperimental continuous processing\n\nWindowed aggregations:\nGroup streaming data into time windows (like every 10 seconds).\n\nWatermark:\nWhen streaming events can arrive late, watermarks allow Spark to drop old state safely.\n</div>","x":22.888916015625,"y":70.444580078125,"width":670,"height":280.984375,"bgColor":"wheat"}],"x":2340.25,"y":2389.5,"width":1581.984375,"height":651.984375,"bgColor":"#F0F0F0"},{"id":"Container-1761910682361","elements":[{"id":"Text-1761910687198","text":"Spark Connect","x":2.5,"y":4.5,"width":200,"height":40,"bgColor":"#E0E0E0"},{"id":"Note-1761910704647","text":"Spark Connect decouples the client (your PySpark, Scala, or Java application) from the Spark driver (running on a remote Spark cluster).\n\nIt’s built on gRPC, meaning you can:\nRun your Spark code remotely (e.g., from your laptop, notebook, or web app),\nWhile the actual computation runs on a remote Spark cluster (local, YARN, Kubernetes, or Databricks).\n\nThink of it like:\nTraditional Spark = Client and Driver in same process\nSpark Connect = Client → gRPC → Remote Driver\n\nSpark Connect properties:\nspark.connect.grpc.binding.port\tgRPC port (default 15002)\nspark.connect.grpc.binding.host\tHost address (default localhost)\nspark.connect.auth.enable\tEnable authentication\nspark.connect.auth.secret\tShared secret key\nspark.connect.log.level\tLogging verbosity\nspark.connect.extensions.classes\tLoad custom extensions\n<div><br></div>","x":25.5,"y":73.5,"width":596.984375,"height":421.984375,"bgColor":"wheat"},{"id":"Node-1761910795093","text":"Spark connect App\n(pyspark, scala app)","x":54.75,"y":522.375,"width":193.921875,"height":68.921875,"bgColor":"#F0F0F0"},{"id":"Node-1761910821434","text":"Spark Driver\nSpark Executors","x":408.75,"y":537.375,"width":197.984375,"height":63.984375,"bgColor":"#F0F0F0"},{"id":"Code-1761910956164","text":"<div># start the Spark Connect server\n$ spark-connect-server \\\n  --master local[*] \\\n  --conf spark.sql.warehouse.dir=/tmp/spark-warehouse \\\n  --packages org.apache.spark:spark-connect_2.12:3.5.1\n\n# Client App\nfrom pyspark.sql import SparkSession\n\n# Connect to remote Spark server via gRPC\nspark = SparkSession.builder.remote(\"sc://localhost:15002\").getOrCreate()\n\n# Example transformation\ndf = spark.createDataFrame([\n    (1, \"Alice\", 2000),\n    (2, \"Bob\", 1500),\n    (3, \"Charlie\", 4000)\n], [\"id\", \"name\", \"salary\"])\n\n# Run a simple transformation\nhigh_salary = df.filter(df.salary &gt; 1800)\nhigh_salary.show()\n\n# You can also use SQL\ndf.createOrReplaceTempView(\"employees\")\nspark.sql(\"SELECT name, salary FROM employees WHERE salary &gt; 1800\").show()\n\nspark.stop()\n</div><div><br></div>","x":672.75,"y":74.375,"width":675.9375,"height":425,"bgColor":"#555"}],"x":1207.5,"y":1695.5,"width":1448.984375,"height":640,"bgColor":"#F0F0F0"},{"id":"Container-1761911770501","elements":[{"id":"Text-1761911780622","text":"Pandas API","x":2,"y":4,"width":141.953125,"height":41.953125,"bgColor":"#E0E0E0"},{"id":"Note-1761911801288","text":"The Pandas API on Spark (pyspark.pandas) lets you write Pandas-like code that actually runs in parallel on Spark.\nIt provides nearly the same API as Pandas — so your existing Pandas code can often run with only one line of change.","x":19,"y":60,"width":432.96875,"height":130.953125,"bgColor":"wheat"},{"id":"Code-1761911833940","text":"<pre><code class=\"language-python hljs\" data-highlighted=\"yes\"><span class=\"hljs-keyword\">import</span> pyspark.pandas <span class=\"hljs-keyword\">as</span> ps\n\n<span class=\"hljs-comment\"># Create a Pandas-on-Spark DataFrame</span>\ndf = ps.DataFrame({\n    <span class=\"hljs-string\">\"id\"</span>: [<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">3</span>, <span class=\"hljs-number\">4</span>],\n    <span class=\"hljs-string\">\"name\"</span>: [<span class=\"hljs-string\">\"Alice\"</span>, <span class=\"hljs-string\">\"Bob\"</span>, <span class=\"hljs-string\">\"Charlie\"</span>, <span class=\"hljs-string\">\"David\"</span>],\n    <span class=\"hljs-string\">\"salary\"</span>: [<span class=\"hljs-number\">2000</span>, <span class=\"hljs-number\">1500</span>, <span class=\"hljs-number\">4000</span>, <span class=\"hljs-number\">3000</span>]\n})\n\n<span class=\"hljs-comment\"># Perform standard Pandas-like operations</span>\n<span class=\"hljs-built_in\">print</span>(df.head())\n\n<span class=\"hljs-comment\"># Filtering</span>\nfiltered = df[df.salary &gt; <span class=\"hljs-number\">1800</span>]\n<span class=\"hljs-built_in\">print</span>(filtered)\n\n<span class=\"hljs-comment\"># GroupBy + aggregation</span>\ngrouped = df.groupby(<span class=\"hljs-string\">\"salary\"</span>).count()\n<span class=\"hljs-built_in\">print</span>(grouped)\n\n<span class=\"hljs-comment\"># Adding new column</span>\ndf[<span class=\"hljs-string\">\"bonus\"</span>] = df.salary * <span class=\"hljs-number\">0.1</span>\n<span class=\"hljs-built_in\">print</span>(df)\n\n<span class=\"hljs-comment\"># Convert back to pandas if needed (collects to driver)</span>\npdf = df.to_pandas()\n\n\n\n\n\n<span class=\"hljs-comment\"># we can swith between padas API and pyspark dataframe</span>\n<span class=\"hljs-keyword\">from</span> pyspark.sql <span class=\"hljs-keyword\">import</span> SparkSession\n<span class=\"hljs-keyword\">import</span> pyspark.pandas <span class=\"hljs-keyword\">as</span> ps\n\nspark = SparkSession.builder.appName(<span class=\"hljs-string\">\"PandasOnSparkExample\"</span>).getOrCreate()\n\n<span class=\"hljs-comment\"># Create a normal Spark DataFrame</span>\nsdf = spark.createDataFrame([(<span class=\"hljs-number\">1</span>, <span class=\"hljs-string\">\"A\"</span>), (<span class=\"hljs-number\">2</span>, <span class=\"hljs-string\">\"B\"</span>), (<span class=\"hljs-number\">3</span>, <span class=\"hljs-string\">\"C\"</span>)], [<span class=\"hljs-string\">\"id\"</span>, <span class=\"hljs-string\">\"value\"</span>])\n\n<span class=\"hljs-comment\"># Convert to pandas-on-Spark DataFrame</span>\npsdf = sdf.pandas_api()\n\n<span class=\"hljs-comment\"># Perform Pandas-style operations</span>\npsdf[<span class=\"hljs-string\">\"new_col\"</span>] = psdf[<span class=\"hljs-string\">\"id\"</span>] * <span class=\"hljs-number\">10</span>\n<span class=\"hljs-built_in\">print</span>(psdf.head())\n\n<span class=\"hljs-comment\"># Convert back to Spark DataFrame</span>\nsdf2 = psdf.to_spark()\n\n</code></pre>","x":504,"y":63,"width":748.984375,"height":620,"bgColor":"#555"}],"x":2685,"y":1562,"width":1295,"height":720,"bgColor":"#F0F0F0"},{"id":"Path-1759067737251","text":"*","sElement":"Node-1759067474223","eElement":"Node-1759067521911"},{"id":"Path-1759067739108","text":"*","sElement":"Node-1759067474223","eElement":"Node-1759067569427"},{"id":"Path-1759067740454","text":"*","sElement":"Node-1759067474223","eElement":"Node-1759067579230"},{"id":"Path-1759067741914","text":"*","sElement":"Node-1759067448094","eElement":"Node-1759067474223"},{"id":"Path-1759067826947","text":"*","sElement":"Node-1759067448094","eElement":"Node-1759067521911"},{"id":"Path-1759067885090","text":"*","sElement":"Node-1759067448094","eElement":"Node-1759067579230"},{"id":"Path-1761504418596","text":"*","sElement":"Node-1761504385727","eElement":"Node-1759068548800"},{"id":"Path-1761504435364","text":"*","sElement":"Node-1759068555698","eElement":"Node-1761504385727"},{"id":"Path-1761504454232","text":"*","sElement":"Node-1759068570505","eElement":"Node-1761504385727"},{"id":"Path-1761504461093","text":"*","sElement":"Node-1759068579108","eElement":"Node-1761504385727"},{"id":"Path-1761504505111","text":"*","sElement":"Node-1759068596963","eElement":"Node-1761504385727"},{"id":"Path-1761504566702","text":"*","sElement":"Node-1761504539677","eElement":"Node-1761504385727"},{"id":"Path-1761564502005","text":"*","sElement":"Node-1761504385727","eElement":"Node-1759068548800"},{"id":"Path-1761910859333","text":"gRPC","sElement":"Node-1761910795093","eElement":"Node-1761910821434"}]