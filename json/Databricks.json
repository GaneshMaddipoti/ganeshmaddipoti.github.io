[{"id":"Text-1761299888497","text":"Databricks","x":122,"y":69,"width":146.96875,"height":30,"bgColor":"#E0E0E0"},{"id":"Container-1761299904495","elements":[{"id":"Text-1761299908838","text":"Data Warehouse","x":4,"y":3,"width":161.96875,"height":31.96875,"bgColor":"#E0E0E0"},{"id":"Node-1761299974296","text":"External Data","x":22,"y":83,"width":85.96875,"height":60,"bgColor":"#F0F0F0"},{"id":"Node-1761299985347","text":"Extract\nTransform\nLoad","x":212,"y":112,"width":108.96875,"height":77.96875,"bgColor":"#F0F0F0"},{"id":"Node-1761300013627","text":"Warehouse","x":378,"y":109,"width":120,"height":77.96875,"bgColor":"#F0F0F0"},{"id":"Node-1761300033018","text":"BI Tools","x":597,"y":93,"width":100,"height":30,"bgColor":"#F0F0F0"},{"id":"Node-1761300091970","text":"Operational Data","x":27,"y":171,"width":90,"height":60,"bgColor":"#F0F0F0"},{"id":"Node-1761300137476","text":"Reports","x":596,"y":155,"width":100,"height":30,"bgColor":"#F0F0F0"},{"id":"Note-1761300265452","text":"- Data is cleaned, structured before loading to warehouse<div>- Purpose built for BI tools and reporting</div><div>- Meant to unify different sources of data - single source of truth</div><div>- no support for semi and unstructured data</div><div>- Uses proprietary and closed formats</div><div>- expensive to scale</div><div>- poor support for streaming, data science, ML, and AI</div>","x":23,"y":265,"width":515.96875,"height":183.96875,"bgColor":"wheat"}],"x":135,"y":132,"width":725,"height":467.96875,"bgColor":"#F0F0F0"},{"id":"Container-1761300709622","elements":[{"id":"Text-1761300715790","text":"Data Lake","x":6,"y":3,"width":100,"height":30,"bgColor":"#E0E0E0"},{"id":"Node-1761300735049","text":"External Data\nImages\nVideos\nText","x":17,"y":74,"width":113.96875,"height":77.96875,"bgColor":"#F0F0F0"},{"id":"Node-1761300757939","text":"Operaional \nData","x":25,"y":185,"width":95,"height":70,"bgColor":"#F0F0F0"},{"id":"Node-1761300774641","text":"Data \nLake","x":201,"y":132,"width":100,"height":56.96875,"bgColor":"#F0F0F0"},{"id":"Node-1761300826586","text":"Transform","x":369,"y":141,"width":100.96875,"height":40,"bgColor":"#F0F0F0"},{"id":"Node-1761300848652","text":"BI Tools","x":548,"y":61,"width":100,"height":30,"bgColor":"#F0F0F0"},{"id":"Node-1761300865066","text":"Data Science  , ML, &amp; AI","x":555,"y":119,"width":100.96875,"height":55,"bgColor":"#F0F0F0"},{"id":"Node-1761300907875","text":"Reports","x":554,"y":211,"width":100,"height":30,"bgColor":"#F0F0F0"},{"id":"Note-1761300992245","text":"- Data is loaded from various sources in raw format (Structured, Semi, and Unstructured)<div>- Then transofrmed to up stream systesm like BI, DS, ML, AI, and Reports<br></div><div>- Supports streaming </div><div>- Complex to setup</div><div>- Warehouses still needed</div><div>- Poor BI performance</div><div>- Unreliable data swamps</div>","x":21,"y":268,"width":657.96875,"height":180.96875,"bgColor":"wheat"}],"x":879,"y":132,"width":705,"height":468.96875,"bgColor":"#F0F0F0"},{"id":"Container-1761301276539","elements":[{"id":"Text-1761301296060","text":"Data Lakehouse","x":3,"y":3.444446563720703,"width":172.96875,"height":28.96875,"bgColor":"#E0E0E0"},{"id":"Note-1761301311934","text":"- best of both worlds( Data warehouses and Data lakes)<div>- </div>","x":15,"y":45.4444465637207,"width":560,"height":110.96875,"bgColor":"wheat"}],"x":517,"y":638.4444465637207,"width":885,"height":235,"bgColor":"#F0F0F0"},{"id":"Container-1761311693164","elements":[{"id":"Text-1761311698670","text":"Data Engineering","x":4,"y":2.6666717529296875,"width":189,"height":37,"bgColor":"#E0E0E0"},{"id":"Note-1761311803020","text":"Key Responsibilities<div>- Design, build, automate and maintain data pipelines</div><div>- Transform raw data into clean reliable data and structured</div><div>- Ensuring data quality, accuracy, consistency and reliable</div>","x":27,"y":528.6666717529297,"width":463,"height":130,"bgColor":"wheat"},{"id":"Node-1761312023467","text":"Data sources\n- Databases\n- Files\n- Cloud Storage\n- Network Logs","x":22,"y":132.33334350585938,"width":148,"height":238,"bgColor":"#F0F0F0"},{"id":"Node-1761312113296","text":"Data Storage\n(DELTA LAKE)","x":484,"y":368.3333435058594,"width":183,"height":77,"bgColor":"#F0F0F0"},{"id":"Node-1761312142959","text":"Processed Data\n(LAKEFLOW DECLARTIVE PIPELINES)","x":470,"y":82.33334350585938,"width":189,"height":78,"bgColor":"#F0F0F0"},{"id":"Node-1761312169287","text":"Data warehousing, BI\n(DBSQL)","x":927,"y":112.33334350585938,"width":185,"height":102,"bgColor":"#F0F0F0"},{"id":"Node-1761312205709","text":"Data science,  ML\n(MOSAIC AI)","x":944,"y":246.33334350585938,"width":150,"height":73,"bgColor":"#F0F0F0"},{"id":"Node-1761312224898","text":"Data sharing (DELTA SHARING)","x":928,"y":359.3333435058594,"width":265,"height":56,"bgColor":"#F0F0F0"},{"id":"Node-1761312344047","text":"Orchestration (LAKEFLOW JOBS)","x":42,"y":461.3333435058594,"width":1151,"height":44,"bgColor":"#F0F0F0"},{"id":"Node-1761312371112","text":"Governance,  Access, \nSecurity (UNITY CATALOG)","x":460,"y":295.3333435058594,"width":248,"height":54,"bgColor":"#F0F0F0"},{"id":"Node-1761312911643","text":"LAKEFLOW CONNECT","x":219,"y":218,"width":176,"height":40,"bgColor":"#F0F0F0"}],"x":185,"y":979.6666717529297,"width":1208,"height":675,"bgColor":"#F0F0F0"},{"id":"Divider-1761378065451","text":"Divider","x":184,"y":909,"width":1940,"height":20,"bgColor":"#F0F0F0"},{"id":"Divider-1761378081896","text":"Divider","x":153,"y":1727.6666870117188,"width":2013,"height":20,"bgColor":"#F0F0F0"},{"id":"Container-1761378096056","elements":[{"id":"Text-1761378100241","text":"Lakeflow Connect","x":6,"y":9,"width":186,"height":41,"bgColor":"#E0E0E0"},{"id":"Note-1761378131433","text":"- Using connectors to ingest data from files, databases, cloud storage, and streaming.<div>3 types of ingestion methods</div><div>- Manual file uploads - uploading files to volumes or tables</div><div>- Standard connectors </div><div>        - cloud object storage, kafka, etc...</div><div>        - uses batch, incremental batch, and streaming </div><div>- Managed connectors</div><div>        - to ingest data from enterprise apps, SaaS, Database, etc...</div><div>        - incremental read/write patterns </div><div><br></div><div>Ingestion methods</div><div>1) Batch : all data is re-ingested each time pipeline runs</div><div>                  load data as batches of rows on a schedule</div><div>                  creates Delta table from files in cloud storage</div><div>    Ex: CREATE TALBE AS SELECT</div><div>           spark.read.load()</div><div>2) Incremental Batch: Only new data is ingested</div><div>     Ex: COPY INTO</div><div>            spark.readStream (Autoloader with timed trigger)</div><div>            DECLARATIVE PIPELINES (CREATE OR REFRESH STREAMING TABLE)</div><div>3) Streaming: data is continuously loaded as its is generated</div><div>      Ex: spark.readStream (Autoloader with continuous trigger)</div><div>             DECLARATIVE PIPELINES (continuous trigger mode)      </div><div><br></div><div>As part of ingestion, we can include 2 additional columns to creating Delta table</div><div>1) last modified time of the file where the row is originated from</div><div>2) raw source file name\n\nread_files(), spark.read, Auto loader provides rescued data column if the raw data does not match the schema.</div><div>                                             </div><div><br></div>","x":12,"y":59,"width":670,"height":555,"bgColor":"wheat"},{"id":"Code-1761383413414","text":"# CTAS\nCREATE TABLE bakehouse.new_table\nAS\nSELECT *\nFROM 's3://your-bucket/path/to/files'\nFILEFORMAT = CSV                                   # supports CSV, JSON, AVRO, etc....\n\n# COPY INTO - for incremental reads\nCOPY INTO bakehouse.target_table\nFROM 's3://your-bucket/path/to/files'\nFILEFORMAT = CSV\nFORMAT_OPTIONS('header' = 'true');\n\n# AUTO LOADER                                     # incremental batch and streaming\nCREATE OR REFRESH STREAMING TABLE bakehouse.sales_stream\nAS\nSELECT *\nFROM read_files(\n  's3://your-bucket/path/to/files',\n  'csv',\n  map('cloudFiles.format', 'csv')\n)\nOPTIONS (\n  trigger = 'availableNow',\n  interval = '10 minutes'\n);\n","x":25,"y":691.1112060546875,"width":641,"height":316,"bgColor":"#555"}],"x":150,"y":2043,"width":690,"height":1066,"bgColor":"#F0F0F0"},{"id":"Container-1761378288087","elements":[{"id":"Text-1761378293678","text":"Delta Lake","x":1,"y":1.888916015625,"width":110,"height":30,"bgColor":"#E0E0E0"},{"id":"Note-1761378311992","text":"- Open, reliable, and scalable data management<div>- Using medallion model to store data </div><div>      Bronge - raw and unprocessed data</div><div>       Silver   - cleaned, transformed, and enriched</div><div>       Gold     - curated, optimised, and aggregated</div><div>- ACID trasactions</div><div>- DML</div><div>- Schema enforcement</div><div>- Time travel</div><div>- Support both batch, and streaming data</div><div>- uses Delta Tables</div><div>- Delta Tables are stored as folder directory, and within directory contain data as parquet files, and delta logs in json format contain metadata of transactions and table versions.</div><div>Managed vs External tables</div><div>- managed tables contain and manage both data and metadata in databricks</div><div>- external tables contain and manage only metadata in databricks</div><div>- </div>","x":11,"y":43.888916015625,"width":879,"height":536,"bgColor":"wheat"}],"x":976,"y":2672.888916015625,"width":938,"height":841,"bgColor":"#F0F0F0"},{"id":"Container-1761379568423","elements":[{"id":"Text-1761379573167","text":"Unity Catalog","x":7,"y":4.2222900390625,"width":153,"height":32,"bgColor":"#E0E0E0"},{"id":"Note-1761379583149","text":"- Access Control<div>- Auditing</div><div>- Data lineage</div><div>- Quality monitoring</div><div>- Data discovery across workspaces</div>","x":18,"y":51.2222900390625,"width":471,"height":142,"bgColor":"wheat"}],"x":985,"y":2437.2222900390625,"width":581,"height":207,"bgColor":"#F0F0F0"},{"id":"Container-1761380870909","elements":[{"id":"Text-1761380875563","text":"Lakeflow Declarative Pipelines","x":6,"y":3.3333740234375,"width":295,"height":35,"bgColor":"#E0E0E0"},{"id":"Note-1761380905811","text":"- Building batch and streaming data pipelines<div>- </div>","x":13,"y":58.3333740234375,"width":495,"height":118,"bgColor":"wheat"}],"x":986,"y":2197.3333740234375,"width":579,"height":200,"bgColor":"#F0F0F0"},{"id":"Container-1761381040244","elements":[{"id":"Text-1761381082972","text":"Lakeflow Jobs","x":4,"y":3.3333740234375,"width":176,"height":31,"bgColor":"#E0E0E0"},{"id":"Note-1761381103110","text":"- A workflow automation tool<div>- Orchestrates and coordinates multiple tasks</div><div>- Scheduling, optimisation, and management of workflows </div>","x":16,"y":47.3333740234375,"width":487,"height":124,"bgColor":"wheat"}],"x":965,"y":1907,"width":593,"height":237,"bgColor":"#F0F0F0"},{"id":"Path-1761300052170","text":"*","sElement":"Node-1761299974296","eElement":"Node-1761299985347"},{"id":"Path-1761300053563","text":"*","sElement":"Node-1761299985347","eElement":"Node-1761300013627"},{"id":"Path-1761300054925","text":"*","sElement":"Node-1761300013627","eElement":"Node-1761300033018"},{"id":"Path-1761300108359","text":"*","sElement":"Node-1761300091970","eElement":"Node-1761299985347"},{"id":"Path-1761300148298","text":"*","sElement":"Node-1761300013627","eElement":"Node-1761300137476"},{"id":"Path-1761300791657","text":"*","sElement":"Node-1761300735049","eElement":"Node-1761300774641"},{"id":"Path-1761300793141","text":"*","sElement":"Node-1761300757939","eElement":"Node-1761300774641"},{"id":"Path-1761300837640","text":"*","sElement":"Node-1761300774641","eElement":"Node-1761300826586"},{"id":"Path-1761300916417","text":"*","sElement":"Node-1761300826586","eElement":"Node-1761300848652"},{"id":"Path-1761300917997","text":"*","sElement":"Node-1761300826586","eElement":"Node-1761300865066"},{"id":"Path-1761300936470","text":"*","sElement":"Node-1761300826586","eElement":"Node-1761300907875"},{"id":"Path-1761312134455","text":"Ingest","sElement":"Node-1761312023467","eElement":"Node-1761312113296"},{"id":"Path-1761312158412","text":"Transform","sElement":"Node-1761312113296","eElement":"Node-1761312142959"},{"id":"Path-1761312266583","text":"*","sElement":"Node-1761312142959","eElement":"Node-1761312169287"},{"id":"Path-1761312268487","text":"*","sElement":"Node-1761312142959","eElement":"Node-1761312205709"},{"id":"Path-1761312276336","text":"*","sElement":"Node-1761312142959","eElement":"Node-1761312224898"}]